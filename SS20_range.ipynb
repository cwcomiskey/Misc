{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "# import scipy\n",
    "# from scipy import spatial\n",
    "# from scipy.spatial import cKDTree\n",
    "\n",
    "# import sklearn as sk\n",
    "# from sklearn import svm\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import neighbors\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn import tree\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn import clone\n",
    "# from sklearn.externals.six.moves import xrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2018: SS18/FW18 ranges\n",
    "\n",
    "# SS18 = pd.read_excel('RangeSS18.xlsx', skiprows = 5) # **PROBLEM**\n",
    "SS18 = pd.read_excel('SS18_range_data.xlsx') # Solution: Artem file\n",
    "FW18 = pd.read_excel('RangeFW18.xlsx', skiprows = 5)\n",
    "\n",
    "SS18_range = SS18['Article Number'].unique()\n",
    "FW18_range = FW18['Article Number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SS20 = (pd.read_excel('SS20Range.xlsx', skiprows = 6).\n",
    "#        drop(['Unnamed: 0', 'vlookup'], axis = 1))\n",
    "\n",
    "# SS20_all_articles = SS20['Article Number'].unique()\n",
    "\n",
    "# SS20 = SS20[SS20['CO'] == 'Y']\n",
    "# dat['CO'].fillna('N', inplace = True)\n",
    "# dat['SS19 Ranged'].fillna('N', inplace = True)\n",
    "# dat['FW19 Ranged'].fillna('N', inplace = True)\n",
    "\n",
    "# dat.to_csv('dat_SS20_range.csv')\n",
    "\n",
    "dat_SS20_range = pd.read_csv('dat_SS20_range.csv') # *** DATA ***\n",
    "SS20_range = dat_SS20_range['Article Number'].unique() # 1340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "dat_SS20_range = dat_SS20_range[['Article Number', 'Model', 'Product Division', \n",
    "                                 'Category Marketing Line', 'Gender', 'Model Number', 'Product Group',\n",
    "                                 'Product Type', 'Sports Category', 'Key Category', 'WE eCom', 'SS19 Ranged', \n",
    "                                 'FW19 Ranged', 'CO']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_SS20_range.groupby(['SS19 Ranged', 'FW19 Ranged', 'CO'])['Article Number'].count()\n",
    "# dat_SS20_range['Key Category'].value_counts()[:10]\n",
    "# dat_SS20_range['Product Division'].value_counts()\n",
    "# dat_SS20_range['Product Group'].value_counts()[:10]\n",
    "# dat_SS20_range['Gender'].value_counts()\n",
    "\n",
    "# dat_SS20_range[dat_SS20_range['FW19 Ranged'] == 'Y'].groupby('Article Number')[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat0 = pd.read_csv('Transaction data.csv', low_memory=False) # *** DATA ***\n",
    "dat0['consumer_order_date'] = pd.to_datetime(dat0['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EDA: clearance transactions by season\n",
    "dat.groupby('season')['Clearance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transaction subset: SS19\n",
    "dat = dat[['consumer_order_date', 'article_number', 'gross_demand_quantity', 'Clearance', 'season']]\n",
    "dat = dat[dat['season'] == 'SS19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wall time: 38.7 s\n",
    "\n",
    "dat_SS20_range = pd.read_csv('dat_SS20_range.csv') # *** DATA ***\n",
    "SS20_range = dat_SS20_range['Article Number'].unique()\n",
    "dat = dat[[(a in SS20_range) for a in dat['article_number']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clearance = dat.copy() # FOR CLEARANCE CORRECTION IN NEXT SECTION\n",
    "\n",
    "# 'aggregate' to weekly sums by article for buy_availability merge and adjustment\n",
    "dat.set_index('consumer_order_date', inplace = True)\n",
    "\n",
    "dat = dat[['article_number', 'gross_demand_quantity']].groupby(['article_number']).resample('W').sum()\n",
    "dat.reset_index(inplace=True)\n",
    "\n",
    "# Add 'week' and 'year' for merging with stock (buy_availability) data (b/c min_date_of_week)\n",
    "dat['week'] = [t.week for t in dat['consumer_order_date']]\n",
    "dat['year'] = [t.year for t in dat['consumer_order_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Wall time: ~40 s\n",
    "\n",
    "# 2018 data --> SS19 in Dec. 2018 -- to merge with 2019 SS19\n",
    "dat_stock = pd.read_csv('Stock Data/Stock data 2018.csv', low_memory=False, index_col = 0) # *** DATA ***\n",
    "dat_stock['min_date_of_week'] = pd.to_datetime(dat_stock['min_date_of_week'])\n",
    "dat_stock = dat_stock[dat_stock['min_date_of_week'] > pd.to_datetime('2018-12-1')] # filter to > Dec1 (SS19)\n",
    "\n",
    "# 2019 data\n",
    "dat_stock2 = pd.read_csv('Stock Data/Stock data 2019.csv', low_memory=False, index_col = 0) # *** DATA ***\n",
    "\n",
    "# Build: Dec2018 + 2019\n",
    "dat_stock = pd.concat([dat_stock, dat_stock2])\n",
    "\n",
    "# Tidy\n",
    "dat_stock.reset_index(inplace = True)\n",
    "dat_stock.drop(['avg(ecom_available_stock)', 'avg(size_availability)'], axis = 1, inplace = True)\n",
    "dat_stock.rename(columns = {'min_date_of_week': 'date', 'avg(buy_availability)': 'buy_availability'}, inplace = True)\n",
    "\n",
    "# Filter to SS20_range carryovers\n",
    "dat_stock = dat_stock[[(a in SS20_range) for a in dat_stock['article_number']]]\n",
    "\n",
    "dat_stock['date'] = pd.to_datetime(dat_stock['date'])\n",
    "\n",
    "# For merging with transaction data\n",
    "dat_stock['week'] = [t.week for t in dat_stock['date']]\n",
    "dat_stock['year'] = [t.year for t in dat_stock['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# weekly demand + buy_availability -- for understock correction\n",
    "dat = pd.merge(dat, dat_stock, \n",
    "               left_on = ['article_number', 'year', 'week'], \n",
    "               right_on = ['article_number', 'year', 'week'],\n",
    "               how = 'outer')\n",
    "\n",
    "# dat['buy_availability'] = dat['buy_availability'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----- Calculate *observed* full season gross_qty per article -----\n",
    "dat_season = pd.DataFrame(dat.groupby(['article_number'])['gross_demand_quantity'].sum())\n",
    "\n",
    "dat_season.rename(columns = {'gross_demand_quantity':'season_gross_demand_quantity'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# WEEKLY averages for articles when fully stocked\n",
    "dat_stocked = pd.DataFrame(\n",
    "    dat[dat['buy_availability'] > 0.35].\n",
    "    groupby(['article_number'])['gross_demand_quantity'].\n",
    "    mean())\n",
    "\n",
    "dat_stocked.rename(columns = {'gross_demand_quantity':'stocked_weekly_avg_gross_demand_quantity'}, inplace= True)\n",
    "\n",
    "dat_stocked['understock_correction'] = 26*dat_stocked['stocked_weekly_avg_gross_demand_quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_season = pd.merge(dat_season, dat_stocked, left_index=True, right_index=True, how = 'outer')\n",
    "dat_season.drop('stocked_weekly_avg_gross_demand_quantity', inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------ OVERSTOCK CORRECTION ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clearance0 = clearance[clearance['Clearance'] == 0]\n",
    "\n",
    "clearance0 = pd.DataFrame(clearance0.groupby(['article_number'])['gross_demand_quantity'].sum())\n",
    "\n",
    "clearance0.rename(columns = {'gross_demand_quantity': 'overstock_correction'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EDA -- article intersection -- articles in (i) SS20 range AND (ii) in SS19 clearance transactions\n",
    "set(SS20_range).intersection(set(clearance[clearance['Clearance'] == 1].reset_index()['article_number'])) # 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_season = pd.merge(dat_season, clearance0, how = 'outer', left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = (pd.DataFrame(\n",
    "    pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0)) # *** DATA ***\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "        dropna())\n",
    "\n",
    "preds = preds[preds['season'] == 'SS19']\n",
    "\n",
    "dat_season = pd.merge(\n",
    "    dat_season, preds, \n",
    "    left_on = ['article_number'],\n",
    "    right_on=['article'], \n",
    "    how = 'left').round()\n",
    "\n",
    "dat_season = dat_season[['article', 'season', 'season_gross_demand_quantity', 'understock_correction',\n",
    "                        'overstock_correction', 'ecom_marketing_forecast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def projection(gross, forecast, under, over):\n",
    "    if forecast < gross:\n",
    "        return under\n",
    "    elif forecast >= gross:\n",
    "        return 1.2*over\n",
    "    else:\n",
    "        return gross\n",
    "\n",
    "dat_season.loc[:,'projected_gross_demand_quantity'] = (\n",
    "    dat_season.apply(lambda row: projection(row['season_gross_demand_quantity'], \n",
    "                                            row['ecom_marketing_forecast'], \n",
    "                                            row['understock_correction'],\n",
    "                                            row['overstock_correction']), \n",
    "                     axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Article subset: projected to have gross_demand_quantity > 100 in SS19\n",
    "dat_season = dat_season[dat_season['season_gross_demand_quantity'] > 83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season['SS20_prediction'] = dat_season['projected_gross_demand_quantity']*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season.shape\n",
    "dat_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season.to_csv('article_baselines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Projected Gross Demand Quantity summary')\n",
    "dat_season['projected_gross_demand_quantity'].describe()\n",
    "print()\n",
    "print('Summary: eCom forecast minus projected gross demand')\n",
    "(dat_season['ecom_marketing_forecast'] - dat_season['projected_gross_demand_quantity']).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: get for Artem over/understock correction for 'B41674', CE2441\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Overstock-SupplyChain-Understock (OSU) Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat0 = pd.read_csv('Transaction data.csv', low_memory=False) # *** DATA ***\n",
    "dat0['consumer_order_date'] = pd.to_datetime(dat0['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transaction subset: SS19\n",
    "dat = dat[['consumer_order_date', 'article_number', 'gross_demand_quantity', 'Clearance', 'season']]\n",
    "dat = dat[dat['season'] == 'SS19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wall time: 38.7 s\n",
    "\n",
    "dat_SS20_range = pd.read_csv('dat_SS20_range.csv') # *** DATA ***\n",
    "SS20_range = dat_SS20_range['Article Number'].unique()\n",
    "dat = dat[[(a in SS20_range) for a in dat['article_number']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Subset to non-clearance transactions only --- change here for David/Mike's request\n",
    "dat = dat[dat['Clearance'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'aggregate' to weekly sums by article for buy_availability merge and adjustment\n",
    "dat.set_index('consumer_order_date', inplace = True)\n",
    "\n",
    "dat = dat[['article_number', 'gross_demand_quantity']].groupby(['article_number']).resample('W').sum()\n",
    "dat.reset_index(inplace=True)\n",
    "\n",
    "# Add 'week' and 'year' for merging with stock (buy_availability) data (b/c min_date_of_week)\n",
    "dat['week'] = [t.week for t in dat['consumer_order_date']]\n",
    "dat['year'] = [t.year for t in dat['consumer_order_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stock data for buy_availability adjustment\n",
    "\n",
    "# Wall time: ~40 s\n",
    "\n",
    "# 2018 data \n",
    "dat_stock = pd.read_csv('Stock Data/Stock data 2018.csv', low_memory=False, index_col = 0) # *** DATA ***\n",
    "dat_stock['min_date_of_week'] = pd.to_datetime(dat_stock['min_date_of_week'])\n",
    "dat_stock = dat_stock[dat_stock['min_date_of_week'] > pd.to_datetime('2018-12-1')] # filter to > Dec1 (SS19)\n",
    "\n",
    "# 2019 data\n",
    "dat_stock2 = pd.read_csv('Stock Data/Stock data 2019.csv', low_memory=False, index_col = 0) # *** DATA ***\n",
    "\n",
    "# Build: Dec2018 + 2019\n",
    "dat_stock = pd.concat([dat_stock, dat_stock2])\n",
    "\n",
    "# Tidy\n",
    "dat_stock.reset_index(inplace = True)\n",
    "dat_stock.drop(['avg(ecom_available_stock)', 'avg(size_availability)'], axis = 1, inplace = True)\n",
    "dat_stock.rename(columns = {'min_date_of_week': 'date', 'avg(buy_availability)': 'buy_availability'}, inplace = True)\n",
    "\n",
    "# Filter to SS20_range carryovers\n",
    "dat_stock = dat_stock[[(a in SS20_range) for a in dat_stock['article_number']]]\n",
    "\n",
    "dat_stock['date'] = pd.to_datetime(dat_stock['date'])\n",
    "\n",
    "# For merging with transaction data\n",
    "dat_stock['week'] = [t.week for t in dat_stock['date']]\n",
    "dat_stock['year'] = [t.year for t in dat_stock['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge weekly demand df and buy_availability df -- for understock correction\n",
    "dat = pd.merge(dat, dat_stock, \n",
    "               left_on = ['article_number', 'year', 'week'], \n",
    "               right_on = ['article_number', 'year', 'week'],\n",
    "               how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----- Calculate *observed* full season gross_demand_quantity per article -----\n",
    "\n",
    "dat_season = pd.DataFrame(dat.groupby(['article_number'])['gross_demand_quantity'].sum())\n",
    "dat_season.rename(columns = {'gross_demand_quantity':'season_gross_demand_quantity'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WEEKLY averages for articles when buy_availability > 0.35\n",
    "dat_stocked = pd.DataFrame(\n",
    "    dat[dat['buy_availability'] > 0.35].\n",
    "    groupby(['article_number'])['gross_demand_quantity'].\n",
    "    mean())\n",
    "\n",
    "dat_stocked.rename(columns = {'gross_demand_quantity':'corrected_weekly_avg_gross_demand_quantity'}, inplace= True)\n",
    "\n",
    "# Extend to full season (26 weeks) to estimate full season demand\n",
    "dat_stocked['corrected_gross_demand_quantity'] = 26*dat_stocked['corrected_weekly_avg_gross_demand_quantity'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season = pd.merge(dat_season, dat_stocked, left_index=True, right_index=True, how = 'outer')\n",
    "dat_season.drop('corrected_weekly_avg_gross_demand_quantity', inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = (pd.DataFrame(\n",
    "    pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0)) # *** DATA ***\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "        dropna())\n",
    "\n",
    "preds = preds[preds['season'] == 'SS19']\n",
    "\n",
    "dat_season = pd.merge(\n",
    "    dat_season, preds, \n",
    "    left_on = ['article_number'],\n",
    "    right_on=['article'], \n",
    "    how = 'left').round()\n",
    "\n",
    "dat_season = dat_season[['article', 'season', 'season_gross_demand_quantity', \n",
    "                         'corrected_gross_demand_quantity', 'ecom_marketing_forecast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = dat_season[dat_season['season_gross_demand_quantity'] > 83]\n",
    "dat_season['DAA_SS20_prediction'] = dat_season['corrected_gross_demand_quantity']*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_season.to_csv('dat_season.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = 'B96578'\n",
    "# article = 'DB3258'\n",
    "\n",
    "p = dat[dat['article_number'] == article].dropna()\n",
    "\n",
    "pivoted = p.pivot(index = 'consumer_order_date', \n",
    "                  columns = 'article_number', \n",
    "                  values = 'buy_availability')\n",
    "\n",
    "\n",
    "pivoted2 = p.pivot(index = 'consumer_order_date', \n",
    "                  columns = 'article_number', \n",
    "                  values = 'gross_demand_quantity')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.xlabel('log(net_sales)')\n",
    "# plt.ylabel('log(sold_qty)')\n",
    "# plt.colorbar()\n",
    "# plt.title('DBSCAN Classification')\n",
    "# plt.rcParams[\"figure.figsize\"] = [16,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(2, 1, 1)\n",
    "# plt.scatter(dat_scaled['gross_demand_quantity']['mean'], \n",
    "#             dat_scaled['sales_price']['mean'], \n",
    "#             c = agglom,\n",
    "#             alpha = 0.25)\n",
    "# plt.title('Agglomerative Classes: Sale Price vs. Gross Demand Qty (mean/sum)')\n",
    "# plt.ylabel('Sale Price')\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.scatter(dat_scaled['gross_demand_quantity']['sum'], \n",
    "#             dat_scaled['sales_price']['mean'], \n",
    "#             c = agglom,\n",
    "#             alpha = 0.25)\n",
    "# plt.ylabel('Sale Price')\n",
    "# plt.xlabel('GDQ Sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [16,5]\n",
    "\n",
    "pivoted.plot(linewidth = 4)\n",
    "plt.title('Buy Availability Over Time')\n",
    "plt.ylabel('Buy Availability')\n",
    "\n",
    "pivoted2.plot(linewidth = 4)\n",
    "plt.title('Weekly Gross Demand Quantity')\n",
    "plt.ylabel('Gross Demand Quantity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SS20 buy numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall time: 55.8 s\n",
    "\n",
    "# SS20 buyer predictions\n",
    "# SS20_buys = pd.read_excel('SS20 RMA2_May7.xlsx', skiprows = 2, sheet_name= '03 May download')\n",
    "# SS20_buys = SS20_buys[['Article Number', 'Model', 'Market', 'WE eCom', \n",
    "#                        'Sub-Brand', 'Product Division', 'Marketing Segment']]\n",
    "\n",
    "\n",
    "# SS20_buys.rename(columns = {'WE eCom': 'eCom SS20 Forecast'}, inplace = True)\n",
    "# SS20_buys = SS20_buys[[(a in AOIs) for a in SS20_buys['Article Number']]]\n",
    "# SS20_buys.to_csv('SS20_buys_carryovers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = pd.read_csv('dat_season.csv', low_memory=False, index_col = 0)\n",
    "SS20_buys = pd.read_csv('SS20_buys_carryovers.csv', low_memory=False, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = (pd.merge(dat_season, SS20_buys[['Article Number', 'eCom SS20 Forecast']], \n",
    "                       left_on='article', right_on='Article Number', how = 'outer'))\n",
    "\n",
    "dat_season = dat_season[['article', 'Article Number', 'season_gross_demand_quantity', \n",
    "                         'corrected_gross_demand_quantity', 'ecom_marketing_forecast',\n",
    "                         'DAA_SS20_prediction', 'eCom SS20 Forecast']]\n",
    "\n",
    "dat_season['prediction_difference'] = dat_season['eCom SS20 Forecast'] - dat_season['DAA_SS20_prediction']\n",
    "dat_season['abs_prediction_difference'] = abs(dat_season['eCom SS20 Forecast'] - dat_season['DAA_SS20_prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buyer version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season.drop('article', inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season.rename(columns = {'season_gross_demand_quantity': 'SS19 Demand',\n",
    "                            'corrected_gross_demand_quantity': 'SS19 Corrected Demand',\n",
    "                            'ecom_marketing_forecast': 'SS19 eCom Forecast',\n",
    "                            'DAA_SS20_prediction': 'Analytics SS20 Forecast',\n",
    "                            }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = dat_season[['Article Number', 'SS19 eCom Forecast', 'SS19 Demand', 'SS19 Corrected Demand',\n",
    "       'eCom SS20 Forecast', 'Analytics SS20 Forecast', \n",
    "       'prediction_difference']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers = pd.read_csv('article_managers.csv', low_memory=False, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = (pd.merge(dat_season, buyers, left_on = 'Article Number', right_index = True, how = 'outer').\n",
    "             set_index('Article Number').\n",
    "            rename(columns = {'CM': 'Manager'}).\n",
    "            drop('prediction_difference', axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable           Type         Data/Info\n",
      "-----------------------------------------\n",
      "SS20_buys          DataFrame          Article Number     <...>ASKETBALL ACC HW ADIDAS  \n",
      "buyers             DataFrame                             <...>n[13883 rows x 1 columns]\n",
      "cost_price         DataFrame                     cost\\nar<...>n[76715 rows x 1 columns]\n",
      "dat0               DataFrame             consumer_order_d<...>544600 rows x 15 columns]\n",
      "dat_season         DataFrame        Article Number  SS19 <...>                   54.9  \n",
      "forecasts          DataFrame                    SS19 eCom<...>n[13890 rows x 6 columns]\n",
      "missing_managers   DataFrame                    SS19 eCom<...>          232.1     NaN  \n",
      "price              DataFrame             consumer_order_d<...>544600 rows x 15 columns]\n",
      "price490           DataFrame                    sale_pric<...>35859           21.087905\n",
      "test               DataFrame            SS19 eCom Forecas<...>aN      NaN         NaN  \n"
     ]
    }
   ],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumer_order_date</th>\n",
       "      <th>country</th>\n",
       "      <th>article_number</th>\n",
       "      <th>brand</th>\n",
       "      <th>gross_demand_quantity</th>\n",
       "      <th>net_qty</th>\n",
       "      <th>gross_sales_gross_disc_net_ret</th>\n",
       "      <th>net_sales</th>\n",
       "      <th>total_markdown</th>\n",
       "      <th>article_promotion_main_category_group</th>\n",
       "      <th>fraction_of_full_price</th>\n",
       "      <th>markdown</th>\n",
       "      <th>Clearance</th>\n",
       "      <th>FW_or_SS</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-09-02 22:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AB0659</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.26</td>\n",
       "      <td>15.18</td>\n",
       "      <td>6.92</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.544137</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FW</td>\n",
       "      <td>FW16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-20 23:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AB0661</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.26</td>\n",
       "      <td>14.17</td>\n",
       "      <td>5.91</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.582922</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FW</td>\n",
       "      <td>FW16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-31 22:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AJ8191</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.66</td>\n",
       "      <td>34.13</td>\n",
       "      <td>4.47</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.869030</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SS</td>\n",
       "      <td>SS17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-03-31 22:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AJ8194</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.83</td>\n",
       "      <td>15.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.977587</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SS</td>\n",
       "      <td>SS17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-13 22:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AJ8017</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.83</td>\n",
       "      <td>18.96</td>\n",
       "      <td>4.13</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.782173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SS</td>\n",
       "      <td>SS17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  consumer_order_date country article_number   brand  gross_demand_quantity  \\\n",
       "0 2016-09-02 22:00:00      ES         AB0659  REEBOK                    1.0   \n",
       "1 2016-11-20 23:00:00      ES         AB0661  REEBOK                    1.0   \n",
       "2 2017-03-31 22:00:00      ES         AJ8191  REEBOK                    2.0   \n",
       "3 2017-03-31 22:00:00      ES         AJ8194  REEBOK                    1.0   \n",
       "4 2017-04-13 22:00:00      ES         AJ8017  REEBOK                    1.0   \n",
       "\n",
       "   net_qty  gross_sales_gross_disc_net_ret  net_sales  total_markdown  \\\n",
       "0      1.0                            8.26      15.18            6.92   \n",
       "1      1.0                            8.26      14.17            5.91   \n",
       "2      2.0                           29.66      34.13            4.47   \n",
       "3      1.0                           14.83      15.17            0.34   \n",
       "4      1.0                           14.83      18.96            4.13   \n",
       "\n",
       "  article_promotion_main_category_group  fraction_of_full_price  markdown  \\\n",
       "0                                Outlet                0.544137         1   \n",
       "1                                Outlet                0.582922         1   \n",
       "2                                Outlet                0.869030         1   \n",
       "3                                Outlet                0.977587         1   \n",
       "4                                Outlet                0.782173         1   \n",
       "\n",
       "   Clearance FW_or_SS season  \n",
       "0          1       FW   FW16  \n",
       "1          1       FW   FW16  \n",
       "2          1       SS   SS17  \n",
       "3          1       SS   SS17  \n",
       "4          1       SS   SS17  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test19 = pd.DataFrame(test[test['season'] == 'SS19'].groupby('article_number')['gross_demand_quantity'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gross_demand_quantity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_number</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B37616</th>\n",
       "      <td>1486.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                gross_demand_quantity\n",
       "article_number                       \n",
       "B37616                         1486.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test19[test19['gross_demand_quantity'] == 1486]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_managers = forecasts[forecasts['Manager'].isna()]\n",
    "# missing_managers.to_csv('missing_managers.csv')\n",
    "\n",
    "# forecasts.to_csv('forecasts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add cost/price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Add cost to DF ----\n",
    "cost_price = pd.read_csv('Cost Price.csv', low_memory=False, index_col = 0)\n",
    "\n",
    "# cost_price = cost_price[[(a in SS20_range) for a in cost_price.index]]\n",
    "cost_price.rename(columns = {'avg(cost_of_sales)': 'cost'}, inplace = True)\n",
    "cost_price = pd.DataFrame(cost_price['cost'].groupby(cost_price.index).mean()).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = pd.merge(forecasts, cost_price, left_index=True, right_index = True, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 7)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SS19 eCom Forecast</th>\n",
       "      <th>SS19 Demand</th>\n",
       "      <th>SS19 Corrected Demand</th>\n",
       "      <th>eCom SS20 Forecast</th>\n",
       "      <th>Analytics SS20 Forecast</th>\n",
       "      <th>Manager</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>015110</th>\n",
       "      <td>800.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>421.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>019000</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>1887.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>019228</th>\n",
       "      <td>500.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>256.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>019310</th>\n",
       "      <td>500.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>179.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>033905</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>701.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SS19 eCom Forecast  SS19 Demand  SS19 Corrected Demand  \\\n",
       "015110               800.0        280.0                  383.0   \n",
       "019000              2000.0        693.0                  920.0   \n",
       "019228               500.0        174.0                  233.0   \n",
       "019310               500.0        129.0                  163.0   \n",
       "033905              1500.0        498.0                  638.0   \n",
       "\n",
       "        eCom SS20 Forecast  Analytics SS20 Forecast Manager  cost  \n",
       "015110              1110.0                    421.3     NaN  32.0  \n",
       "019000              1887.0                   1012.0     NaN  17.0  \n",
       "019228               777.0                    256.3     NaN  23.0  \n",
       "019310               555.0                    179.3     NaN  21.0  \n",
       "033905              1500.0                    701.8     NaN   6.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts.shape\n",
    "forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'avg_price' to table\n",
    "# (1) Filter to the 491 SS20 articles of interest\n",
    "# (2) sale price on THAT transaction: gross_sales_gross_disc_net_ret/net_qty\n",
    "# (3) find average of that per article\n",
    "# (4) add that to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable           Type         Data/Info\n",
      "-----------------------------------------\n",
      "SS20_buys          DataFrame          Article Number     <...>ASKETBALL ACC HW ADIDAS  \n",
      "buyers             DataFrame                             <...>n[13883 rows x 1 columns]\n",
      "cost_price         DataFrame                     cost\\nar<...>n[76715 rows x 1 columns]\n",
      "dat_season         DataFrame        Article Number  SS19 <...>                   54.9  \n",
      "forecasts          DataFrame            SS19 eCom Forecas<...>              NaN   NaN  \n",
      "missing_managers   DataFrame                    SS19 eCom<...>          232.1     NaN  \n",
      "test               DataFrame            SS19 eCom Forecas<...>              NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Add price to DF ----\n",
    "dat0 = pd.read_csv('Transaction data.csv', low_memory=False) # *** DATA ***\n",
    "dat0['consumer_order_date'] = pd.to_datetime(dat0['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall time: 2min 8s\n",
    "\n",
    "the490 = forecasts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "price490 = price[[(a in the490) for a in price['article_number']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "price490.loc[:,'sale_price'] = price490['net_sales']/price490['gross_demand_quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "price490 = pd.DataFrame(price490.groupby('article_number')['sale_price'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = pd.merge(forecasts, price490, left_index = True, right_index = True, how = 'left').round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts['prediction_difference'] = forecasts['eCom SS20 Forecast'] - forecasts['Analytics SS20 Forecast']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "    forecasts['loss'] = np.where(\n",
    "        forecasts['prediction_difference'] > 0, \n",
    "        forecasts['prediction_difference']*forecasts['cost'],\n",
    "        forecasts['prediction_difference']*(forecasts['sale_price'] - forecasts['cost'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SS19 eCom Forecast</th>\n",
       "      <th>SS19 Demand</th>\n",
       "      <th>SS19 Corrected Demand</th>\n",
       "      <th>eCom SS20 Forecast</th>\n",
       "      <th>Analytics SS20 Forecast</th>\n",
       "      <th>Manager</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1486.0</td>\n",
       "      <td>3815.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4196.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B37616</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thibault</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                SS19 eCom Forecast  SS19 Demand  SS19 Corrected Demand  \\\n",
       "Article Number                                                           \n",
       "NaN                            NaN       1486.0                 3815.0   \n",
       "B37616                         NaN          NaN                    NaN   \n",
       "\n",
       "                eCom SS20 Forecast  Analytics SS20 Forecast   Manager  \n",
       "Article Number                                                         \n",
       "NaN                            NaN                   4196.5       NaN  \n",
       "B37616                         NaN                      NaN  Thibault  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts[(forecasts.index == 'B37616') | (forecasts['SS19 Demand'] == 1486)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way under: everything\n",
    "# Way over: DB3258, DP2398, EF2803, DQ3319"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
