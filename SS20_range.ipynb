{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import clone\n",
    "from sklearn.externals.six.moves import xrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2018: SS18/FW18 ranges\n",
    "\n",
    "# SS18 = pd.read_excel('RangeSS18.xlsx', skiprows = 5) # **PROBLEM**\n",
    "SS18 = pd.read_excel('SS18_range_data.xlsx') # Solution: Artem file\n",
    "FW18 = pd.read_excel('RangeFW18.xlsx', skiprows = 5)\n",
    "\n",
    "SS18_range = SS18['Article Number'].unique()\n",
    "FW18_range = FW18['Article Number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SS20 = (pd.read_excel('SS20Range.xlsx', skiprows = 6).\n",
    "#        drop(['Unnamed: 0', 'vlookup'], axis = 1))\n",
    "\n",
    "# SS20_all_articles = SS20['Article Number'].unique()\n",
    "\n",
    "# SS20 = SS20[SS20['CO'] == 'Y']\n",
    "# dat['CO'].fillna('N', inplace = True)\n",
    "# dat['SS19 Ranged'].fillna('N', inplace = True)\n",
    "# dat['FW19 Ranged'].fillna('N', inplace = True)\n",
    "\n",
    "# dat.to_csv('dat_SS20_range.csv')\n",
    "\n",
    "dat_SS20_range = pd.read_csv('dat_SS20_range.csv') # *** DATA ***\n",
    "SS20_range = dat_SS20_range['Article Number'].unique() # 1340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "dat_SS20_range = dat_SS20_range[['Article Number', 'Model', 'Product Division', \n",
    "                                 'Category Marketing Line', 'Gender', 'Model Number', 'Product Group',\n",
    "                                 'Product Type', 'Sports Category', 'Key Category', 'WE eCom', 'SS19 Ranged', \n",
    "                                 'FW19 Ranged', 'CO']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_SS20_range.groupby(['SS19 Ranged', 'FW19 Ranged', 'CO'])['Article Number'].count()\n",
    "# dat_SS20_range['Key Category'].value_counts()[:10]\n",
    "# dat_SS20_range['Product Division'].value_counts()\n",
    "# dat_SS20_range['Product Group'].value_counts()[:10]\n",
    "# dat_SS20_range['Gender'].value_counts()\n",
    "\n",
    "# dat_SS20_range[dat_SS20_range['FW19 Ranged'] == 'Y'].groupby('Article Number')[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 6.02 s, total: 2min 10s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dat0 = pd.read_csv('Transaction data.csv', low_memory=False) # *** DATA ***\n",
    "dat0['consumer_order_date'] = pd.to_datetime(dat0['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EDA: clearance transactions by season\n",
    "dat.groupby('season')['Clearance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transaction subset: SS19\n",
    "dat = dat[['consumer_order_date', 'article_number', 'gross_demand_quantity', 'Clearance', 'season']]\n",
    "dat = dat[dat['season'] == 'SS19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wall time: 38.7 s\n",
    "\n",
    "dat_SS20_range = pd.read_csv('dat_SS20_range.csv') # *** DATA ***\n",
    "SS20_range = dat_SS20_range['Article Number'].unique()\n",
    "dat = dat[[(a in SS20_range) for a in dat['article_number']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clearance = dat.copy() # FOR CLEARANCE CORRECTION IN NEXT SECTION\n",
    "\n",
    "# 'aggregate' to weekly sums by article for buy_availability merge and adjustment\n",
    "dat.set_index('consumer_order_date', inplace = True)\n",
    "\n",
    "dat = dat[['article_number', 'gross_demand_quantity']].groupby(['article_number']).resample('W').sum()\n",
    "dat.reset_index(inplace=True)\n",
    "\n",
    "# Add 'week' and 'year' for merging with stock (buy_availability) data (b/c min_date_of_week)\n",
    "dat['week'] = [t.week for t in dat['consumer_order_date']]\n",
    "dat['year'] = [t.year for t in dat['consumer_order_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Wall time: ~40 s\n",
    "\n",
    "# 2018 data --> SS19 in Dec. 2018 -- to merge with 2019 SS19\n",
    "dat_stock = pd.read_csv('Stock Data/Stock data 2018.csv', low_memory=False, index_col = 0) # *** DATA ***\n",
    "dat_stock['min_date_of_week'] = pd.to_datetime(dat_stock['min_date_of_week'])\n",
    "dat_stock = dat_stock[dat_stock['min_date_of_week'] > pd.to_datetime('2018-12-1')] # filter to > Dec1 (SS19)\n",
    "\n",
    "# 2019 data\n",
    "dat_stock2 = pd.read_csv('Stock Data/Stock data 2019.csv', low_memory=False, index_col = 0) # *** DATA ***\n",
    "\n",
    "# Build: Dec2018 + 2019\n",
    "dat_stock = pd.concat([dat_stock, dat_stock2])\n",
    "\n",
    "# Tidy\n",
    "dat_stock.reset_index(inplace = True)\n",
    "dat_stock.drop(['avg(ecom_available_stock)', 'avg(size_availability)'], axis = 1, inplace = True)\n",
    "dat_stock.rename(columns = {'min_date_of_week': 'date', 'avg(buy_availability)': 'buy_availability'}, inplace = True)\n",
    "\n",
    "# Filter to SS20_range carryovers\n",
    "dat_stock = dat_stock[[(a in SS20_range) for a in dat_stock['article_number']]]\n",
    "\n",
    "dat_stock['date'] = pd.to_datetime(dat_stock['date'])\n",
    "\n",
    "# For merging with transaction data\n",
    "dat_stock['week'] = [t.week for t in dat_stock['date']]\n",
    "dat_stock['year'] = [t.year for t in dat_stock['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# weekly demand + buy_availability -- for understock correction\n",
    "dat = pd.merge(dat, dat_stock, \n",
    "               left_on = ['article_number', 'year', 'week'], \n",
    "               right_on = ['article_number', 'year', 'week'],\n",
    "               how = 'outer')\n",
    "\n",
    "# dat['buy_availability'] = dat['buy_availability'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----- Calculate *observed* full season gross_qty per article -----\n",
    "dat_season = pd.DataFrame(dat.groupby(['article_number'])['gross_demand_quantity'].sum())\n",
    "\n",
    "dat_season.rename(columns = {'gross_demand_quantity':'season_gross_demand_quantity'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# WEEKLY averages for articles when fully stocked\n",
    "dat_stocked = pd.DataFrame(\n",
    "    dat[dat['buy_availability'] > 0.35].\n",
    "    groupby(['article_number'])['gross_demand_quantity'].\n",
    "    mean())\n",
    "\n",
    "dat_stocked.rename(columns = {'gross_demand_quantity':'stocked_weekly_avg_gross_demand_quantity'}, inplace= True)\n",
    "\n",
    "dat_stocked['understock_correction'] = 26*dat_stocked['stocked_weekly_avg_gross_demand_quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_season = pd.merge(dat_season, dat_stocked, left_index=True, right_index=True, how = 'outer')\n",
    "dat_season.drop('stocked_weekly_avg_gross_demand_quantity', inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------ OVERSTOCK CORRECTION ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clearance0 = clearance[clearance['Clearance'] == 0]\n",
    "\n",
    "clearance0 = pd.DataFrame(clearance0.groupby(['article_number'])['gross_demand_quantity'].sum())\n",
    "\n",
    "clearance0.rename(columns = {'gross_demand_quantity': 'overstock_correction'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EDA -- article intersection -- articles in (i) SS20 range AND (ii) in SS19 clearance transactions\n",
    "set(SS20_range).intersection(set(clearance[clearance['Clearance'] == 1].reset_index()['article_number'])) # 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_season = pd.merge(dat_season, clearance0, how = 'outer', left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = (pd.DataFrame(\n",
    "    pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0)) # *** DATA ***\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "        dropna())\n",
    "\n",
    "preds = preds[preds['season'] == 'SS19']\n",
    "\n",
    "dat_season = pd.merge(\n",
    "    dat_season, preds, \n",
    "    left_on = ['article_number'],\n",
    "    right_on=['article'], \n",
    "    how = 'left').round()\n",
    "\n",
    "dat_season = dat_season[['article', 'season', 'season_gross_demand_quantity', 'understock_correction',\n",
    "                        'overstock_correction', 'ecom_marketing_forecast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def projection(gross, forecast, under, over):\n",
    "    if forecast < gross:\n",
    "        return under\n",
    "    elif forecast >= gross:\n",
    "        return 1.2*over\n",
    "    else:\n",
    "        return gross\n",
    "\n",
    "dat_season.loc[:,'projected_gross_demand_quantity'] = (\n",
    "    dat_season.apply(lambda row: projection(row['season_gross_demand_quantity'], \n",
    "                                            row['ecom_marketing_forecast'], \n",
    "                                            row['understock_correction'],\n",
    "                                            row['overstock_correction']), \n",
    "                     axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Article subset: projected to have gross_demand_quantity > 100 in SS19\n",
    "dat_season = dat_season[dat_season['season_gross_demand_quantity'] > 83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season['SS20_prediction'] = dat_season['projected_gross_demand_quantity']*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season.shape\n",
    "dat_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season.to_csv('article_baselines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Projected Gross Demand Quantity summary')\n",
    "dat_season['projected_gross_demand_quantity'].describe()\n",
    "print()\n",
    "print('Summary: eCom forecast minus projected gross demand')\n",
    "(dat_season['ecom_marketing_forecast'] - dat_season['projected_gross_demand_quantity']).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: get for Artem over/understock correction for 'B41674', CE2441\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Overstock-SupplyChain-Understock (OSU) Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat0 = pd.read_csv('Transaction data.csv', low_memory=False) # *** DATA ***\n",
    "dat0['consumer_order_date'] = pd.to_datetime(dat0['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transaction subset: SS19\n",
    "dat = dat[['consumer_order_date', 'article_number', 'gross_demand_quantity', 'Clearance', 'season']]\n",
    "dat = dat[dat['season'] == 'SS19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wall time: 38.7 s\n",
    "\n",
    "dat_SS20_range = pd.read_csv('dat_SS20_range.csv') # *** DATA ***\n",
    "SS20_range = dat_SS20_range['Article Number'].unique()\n",
    "dat = dat[[(a in SS20_range) for a in dat['article_number']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Subset to non-clearance transactions only --- change here for David/Mike's request\n",
    "dat = dat[dat['Clearance'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'aggregate' to weekly sums by article for buy_availability merge and adjustment\n",
    "dat.set_index('consumer_order_date', inplace = True)\n",
    "\n",
    "dat = dat[['article_number', 'gross_demand_quantity']].groupby(['article_number']).resample('W').sum()\n",
    "dat.reset_index(inplace=True)\n",
    "\n",
    "# Add 'week' and 'year' for merging with stock (buy_availability) data (b/c min_date_of_week)\n",
    "dat['week'] = [t.week for t in dat['consumer_order_date']]\n",
    "dat['year'] = [t.year for t in dat['consumer_order_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wall time: ~40 s\n",
    "\n",
    "# 2018 data --> SS19 in Dec. 2018 -- to merge with 2019 SS19\n",
    "dat_stock = pd.read_csv('Stock Data/Stock data 2018.csv', low_memory=False, index_col = 0) # *** DATA ***\n",
    "dat_stock['min_date_of_week'] = pd.to_datetime(dat_stock['min_date_of_week'])\n",
    "dat_stock = dat_stock[dat_stock['min_date_of_week'] > pd.to_datetime('2018-12-1')] # filter to > Dec1 (SS19)\n",
    "\n",
    "# 2019 data\n",
    "dat_stock2 = pd.read_csv('Stock Data/Stock data 2019.csv', low_memory=False, index_col = 0) # *** DATA ***\n",
    "\n",
    "# Build: Dec2018 + 2019\n",
    "dat_stock = pd.concat([dat_stock, dat_stock2])\n",
    "\n",
    "# Tidy\n",
    "dat_stock.reset_index(inplace = True)\n",
    "dat_stock.drop(['avg(ecom_available_stock)', 'avg(size_availability)'], axis = 1, inplace = True)\n",
    "dat_stock.rename(columns = {'min_date_of_week': 'date', 'avg(buy_availability)': 'buy_availability'}, inplace = True)\n",
    "\n",
    "# Filter to SS20_range carryovers\n",
    "dat_stock = dat_stock[[(a in SS20_range) for a in dat_stock['article_number']]]\n",
    "\n",
    "dat_stock['date'] = pd.to_datetime(dat_stock['date'])\n",
    "\n",
    "# For merging with transaction data\n",
    "dat_stock['week'] = [t.week for t in dat_stock['date']]\n",
    "dat_stock['year'] = [t.year for t in dat_stock['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge weekly demand df and buy_availability df -- for understock correction\n",
    "dat = pd.merge(dat, dat_stock, \n",
    "               left_on = ['article_number', 'year', 'week'], \n",
    "               right_on = ['article_number', 'year', 'week'],\n",
    "               how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----- Calculate *observed* full season gross_demand_quantity per article -----\n",
    "\n",
    "dat_season = pd.DataFrame(dat.groupby(['article_number'])['gross_demand_quantity'].sum())\n",
    "dat_season.rename(columns = {'gross_demand_quantity':'season_gross_demand_quantity'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos DataFrame ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'DU0367' in SS20_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_stock[dat_stock['article_number'] == 'DU0367']\n",
    "dat_season.loc['DU0367']\n",
    "dat[dat['article_number'] == 'DU0367']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WEEKLY averages for articles when buy_availability > 0.35\n",
    "dat_stocked = pd.DataFrame(\n",
    "    dat[dat['buy_availability'] > 0.35].\n",
    "    groupby(['article_number'])['gross_demand_quantity'].\n",
    "    mean())\n",
    "\n",
    "dat_stocked.rename(columns = {'gross_demand_quantity':'corrected_weekly_avg_gross_demand_quantity'}, inplace= True)\n",
    "\n",
    "# Extend to full season (26 weeks) to estimate full season demand\n",
    "dat_stocked['corrected_gross_demand_quantity'] = 26*dat_stocked['corrected_weekly_avg_gross_demand_quantity'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season = pd.merge(dat_season, dat_stocked, left_index=True, right_index=True, how = 'outer')\n",
    "dat_season.drop('corrected_weekly_avg_gross_demand_quantity', inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = (pd.DataFrame(\n",
    "    pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0)) # *** DATA ***\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "        dropna())\n",
    "\n",
    "preds = preds[preds['season'] == 'SS19']\n",
    "\n",
    "dat_season = pd.merge(\n",
    "    dat_season, preds, \n",
    "    left_on = ['article_number'],\n",
    "    right_on=['article'], \n",
    "    how = 'left').round()\n",
    "\n",
    "dat_season = dat_season[['article', 'season', 'season_gross_demand_quantity', \n",
    "                         'corrected_gross_demand_quantity', 'ecom_marketing_forecast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = dat_season[dat_season['season_gross_demand_quantity'] > 83]\n",
    "\n",
    "dat_season['DAA_SS20_prediction'] = dat_season['corrected_gross_demand_quantity']*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season.shape\n",
    "dat_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_season.to_csv('dat_season.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SS20 buy numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall time: 55.8 s\n",
    "\n",
    "# SS20 buyer predictions\n",
    "# SS20_buys = pd.read_excel('SS20 RMA2_May7.xlsx', skiprows = 2, sheet_name= '03 May download')\n",
    "# SS20_buys = SS20_buys[['Article Number', 'Model', 'Market', 'WE eCom', \n",
    "#                        'Sub-Brand', 'Product Division', 'Marketing Segment']]\n",
    "\n",
    "\n",
    "# SS20_buys.rename(columns = {'WE eCom': 'eCom SS20 Forecast'}, inplace = True)\n",
    "# SS20_buys = SS20_buys[[(a in AOIs) for a in SS20_buys['Article Number']]]\n",
    "# SS20_buys.to_csv('SS20_buys_carryovers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = pd.read_csv('dat_season.csv', low_memory=False, index_col = 0)\n",
    "SS20_buys = pd.read_csv('SS20_buys_carryovers.csv', low_memory=False, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = (pd.merge(dat_season, SS20_buys[['Article Number', 'eCom SS20 Forecast']], \n",
    "                       left_on='article', right_on='Article Number', how = 'outer'))\n",
    "\n",
    "dat_season = dat_season[['article', 'Article Number', 'season_gross_demand_quantity', \n",
    "                         'corrected_gross_demand_quantity', 'ecom_marketing_forecast',\n",
    "                        'DAA_SS20_prediction', 'eCom SS20 Forecast']]\n",
    "\n",
    "dat_season['prediction_disparity'] = dat_season['eCom SS20 Forecast'] - dat_season['DAA_SS20_prediction']\n",
    "dat_season['abs_prediction_disparity'] = abs(dat_season['eCom SS20 Forecast'] - dat_season['DAA_SS20_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cost to DF\n",
    "cost_price = pd.read_csv('Cost Price.csv', low_memory=False, index_col = 0)\n",
    "\n",
    "cost_price = cost_price[[(a in SS20_range) for a in cost_price.index]]\n",
    "cost_price.rename(columns = {'avg(cost_of_sales)': 'cost'}, inplace = True)\n",
    "cost_price = pd.DataFrame(cost_price['cost'].groupby(cost_price.index).mean()).round()\n",
    "dat_season = pd.merge(dat_season, cost_price, left_on = 'article', right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add price to DF\n",
    "dat0 = pd.read_csv('Transaction data.csv', low_memory=False) # *** DATA ***\n",
    "dat0['consumer_order_date'] = pd.to_datetime(dat0['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumer_order_date</th>\n",
       "      <th>country</th>\n",
       "      <th>article_number</th>\n",
       "      <th>brand</th>\n",
       "      <th>gross_demand_quantity</th>\n",
       "      <th>net_qty</th>\n",
       "      <th>gross_sales_gross_disc_net_ret</th>\n",
       "      <th>net_sales</th>\n",
       "      <th>total_markdown</th>\n",
       "      <th>article_promotion_main_category_group</th>\n",
       "      <th>fraction_of_full_price</th>\n",
       "      <th>markdown</th>\n",
       "      <th>Clearance</th>\n",
       "      <th>FW_or_SS</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-09-02 22:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AB0659</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.26</td>\n",
       "      <td>15.18</td>\n",
       "      <td>6.92</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.544137</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FW</td>\n",
       "      <td>FW16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-20 23:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AB0661</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.26</td>\n",
       "      <td>14.17</td>\n",
       "      <td>5.91</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.582922</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FW</td>\n",
       "      <td>FW16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-31 22:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AJ8191</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.66</td>\n",
       "      <td>34.13</td>\n",
       "      <td>4.47</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.869030</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SS</td>\n",
       "      <td>SS17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-03-31 22:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AJ8194</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.83</td>\n",
       "      <td>15.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.977587</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SS</td>\n",
       "      <td>SS17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-13 22:00:00</td>\n",
       "      <td>ES</td>\n",
       "      <td>AJ8017</td>\n",
       "      <td>REEBOK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.83</td>\n",
       "      <td>18.96</td>\n",
       "      <td>4.13</td>\n",
       "      <td>Outlet</td>\n",
       "      <td>0.782173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SS</td>\n",
       "      <td>SS17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  consumer_order_date country article_number   brand  gross_demand_quantity  \\\n",
       "0 2016-09-02 22:00:00      ES         AB0659  REEBOK                    1.0   \n",
       "1 2016-11-20 23:00:00      ES         AB0661  REEBOK                    1.0   \n",
       "2 2017-03-31 22:00:00      ES         AJ8191  REEBOK                    2.0   \n",
       "3 2017-03-31 22:00:00      ES         AJ8194  REEBOK                    1.0   \n",
       "4 2017-04-13 22:00:00      ES         AJ8017  REEBOK                    1.0   \n",
       "\n",
       "   net_qty  gross_sales_gross_disc_net_ret  net_sales  total_markdown  \\\n",
       "0      1.0                            8.26      15.18            6.92   \n",
       "1      1.0                            8.26      14.17            5.91   \n",
       "2      2.0                           29.66      34.13            4.47   \n",
       "3      1.0                           14.83      15.17            0.34   \n",
       "4      1.0                           14.83      18.96            4.13   \n",
       "\n",
       "  article_promotion_main_category_group  fraction_of_full_price  markdown  \\\n",
       "0                                Outlet                0.544137         1   \n",
       "1                                Outlet                0.582922         1   \n",
       "2                                Outlet                0.869030         1   \n",
       "3                                Outlet                0.977587         1   \n",
       "4                                Outlet                0.782173         1   \n",
       "\n",
       "   Clearance FW_or_SS season  \n",
       "0          1       FW   FW16  \n",
       "1          1       FW   FW16  \n",
       "2          1       SS   SS17  \n",
       "3          1       SS   SS17  \n",
       "4          1       SS   SS17  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'avg_price' to table\n",
    "# (1) Filter to the 491 SS20 articles of interest\n",
    "# (2) sale price on THAT transaction: gross_sales_gross_disc_net_ret/net_qty\n",
    "# (3) find average of that per article\n",
    "# (4) add that to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems:\n",
    "# (1) DU0367 has 'corrected = NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way under: everything\n",
    "# Way over: DB3258, DP2398, EF2803, DQ3319"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
