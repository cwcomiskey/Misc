{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import clone\n",
    "from sklearn.externals.six.moves import xrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2018: SS18/FW18 ranges\n",
    "# SS18 = pd.read_excel('RangeSS18.xlsx', skiprows = 5) # **PROBLEM**\n",
    "SS18 = pd.read_excel('SS18_range_data.xlsx') \n",
    "FW18 = pd.read_excel('RangeFW18.xlsx', skiprows = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SS18 = SS18[['Article Number', 'Article format (all 6 digits)']]\n",
    "FW18 = FW18[['Article Number', 'Article Number (6 digits)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SS18_range = SS18['Article Number'].unique()\n",
    "FW18_range = FW18['Article Number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(SS18_range)\n",
    "len(FW18_range)\n",
    "\n",
    "len(set(SS18_range).intersection(set(FW18_range)))\n",
    "\n",
    "len(set(FW18_range).intersection(set(SS20_range)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SS20 = (pd.read_excel('SS20Range.xlsx', skiprows = 6).\n",
    "#        drop(['Unnamed: 0', 'vlookup'], axis = 1))\n",
    "# SS20 = SS20[SS20['CO'] == 'Y']\n",
    "# dat['CO'].fillna('N', inplace = True)\n",
    "# dat['SS19 Ranged'].fillna('N', inplace = True)\n",
    "# dat['FW19 Ranged'].fillna('N', inplace = True)\n",
    "\n",
    "# dat.to_csv('dat_SS20_range.csv')\n",
    "dat_SS20_range = pd.read_csv('dat_SS20_range.csv')\n",
    "SS20_range = dat_SS20_range['Article Number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "dat_SS20_range = dat_SS20_range[['Article Number', 'Model', 'Product Division', \n",
    "                                 'Category Marketing Line', 'Gender', 'Model Number', 'Product Group',\n",
    "                                 'Product Type', 'Sports Category', 'Key Category', 'WE eCom', 'SS19 Ranged', \n",
    "                                 'FW19 Ranged', 'CO']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_SS20_range.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_SS20_range.groupby(['SS19 Ranged', 'FW19 Ranged', 'CO'])['Article Number'].count()\n",
    "# dat_SS20_range['Key Category'].value_counts()[:10]\n",
    "# dat_SS20_range['Product Division'].value_counts()\n",
    "# dat_SS20_range['Product Group'].value_counts()[:10]\n",
    "# dat_SS20_range['Gender'].value_counts()\n",
    "\n",
    "# dat_SS20_range[dat_SS20_range['FW19 Ranged'] == 'Y'].groupby('Article Number')[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# *** Transactions: 2019 + 2018_Q4 ***** ---- ISSUES ---- ****\n",
    "\n",
    "# dat = pd.read_csv('Transaction data full 2019.csv', low_memory=False, index_col = 0) \n",
    "# # Incomplete: dat2 = pd.read_csv('Transaction Data/Transaction Data Q4 2018.csv', low_memory=False, index_col = 0)\n",
    "\n",
    "# # From David w/ Joerian's query\n",
    "# dat2 = pd.read_csv('Transaction data Q42018.csv', low_memory=False, index_col = 0)\n",
    "\n",
    "# dat = pd.concat([dat, dat2])\n",
    "# del dat2\n",
    "# dat = dat[dat['season'] == 'SS19']\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# David exasol\n",
    "# dat = pd.read_csv('SS2019.csv', low_memory=False, \n",
    "#                   names = ['order_date', 'articleno', 'quantity', 'order_item_paidprice_gross', \n",
    "#                            'order_item_listprice_gross', 'net_discount_value', 'storecountry_code', \n",
    "#                            'DDISCOUNTPERCENTAGE', 'promotioncodedescription', 'ssalesbrand'])\n",
    "\n",
    "\n",
    "\n",
    "# # ----- Benoit's magic ----\n",
    "# dat.loc[dat.order_date.apply(lambda x: \"mkt-Raiffeisen Bank\" in x),:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv('SS2019.csv', \n",
    "            low_memory=False, \n",
    "            skiprows= [109853, 109854, 109856, 119470, 212394, 212395, 212396, 212397,\n",
    "            420307, 420308, 568303, 755985, 758593, 758598, 984903, 984906,\n",
    "            984907, 984909, 984910],\n",
    "            names = ['order_date', 'articleno', 'quantity', 'order_item_paidprice_gross', \n",
    "                     'order_item_listprice_gross', 'net_discount_value', 'storecountry_code', \n",
    "                     'DDISCOUNTPERCENTAGE', 'promotioncodedescription', 'ssalesbrand'])\n",
    "\n",
    "dat.drop(['order_item_paidprice_gross', 'order_item_listprice_gross', 'order_item_paidprice_gross', \n",
    "          'net_discount_value', 'storecountry_code', 'DDISCOUNTPERCENTAGE', 'promotioncodedescription', \n",
    "          'ssalesbrand'], inplace = True, axis = 1)\n",
    "dat.rename(columns = {'articleno': 'article_number', \n",
    "                      'quantity': 'gross_qty',\n",
    "                      'order_date': 'consumer_order_date'}, \n",
    "           inplace = True)\n",
    "\n",
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat.groupby('article_number')['gross_qty'].sum().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wall time: 26.2 s\n",
    "\n",
    "# dat.reset_index(inplace = True)\n",
    "# dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "# dat.drop(['country', 'brand', 'gross_sales_gross_disc_net_ret', 'net_sales', 'total_markdown', 'article_promotion_main_category_group', 'fraction_of_full_price', 'markdown', 'fw_or_ss'], inplace = True, axis = 1)\n",
    "\n",
    "SS20_range = dat_SS20_range['Article Number'].unique()\n",
    "dat = dat[[(a in SS20_range) for a in dat['article_number']]]\n",
    "\n",
    "# 'aggregate' to weekly sums by article for buy_availability adjustment\n",
    "dat.set_index('consumer_order_date', inplace = True)\n",
    "dat = dat[['article_number', 'gross_qty']].groupby(['article_number']).resample('W').sum()\n",
    "dat.reset_index(inplace=True)\n",
    "\n",
    "# Add 'week' and 'year' for merging with stock (buy_availability) data (b/c min_date_of_week)\n",
    "dat['week'] = [t.week for t in dat['consumer_order_date']]\n",
    "dat['year'] = [t.year for t in dat['consumer_order_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat.groupby('article_number')['gross_qty'].sum().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wall time: ~40 s\n",
    "\n",
    "# Dec 2018 data\n",
    "dat_stock = pd.read_csv('Stock Data/Stock data 2018.csv', low_memory=False, index_col = 0)\n",
    "dat_stock['min_date_of_week'] = pd.to_datetime(dat_stock['min_date_of_week'])\n",
    "dat_stock = dat_stock[dat_stock['min_date_of_week'] > pd.to_datetime('2018-12-1')] # filter to > Dec1 (SS19)\n",
    "\n",
    "# 2019 data\n",
    "dat_stock2 = pd.read_csv('Stock Data/Stock data 2019.csv', low_memory=False, index_col = 0)\n",
    "\n",
    "# Build: Dec2018 + 2019\n",
    "dat_stock = pd.concat([dat_stock, dat_stock2])\n",
    "del dat_stock2\n",
    "\n",
    "# Tidy\n",
    "dat_stock.reset_index(inplace = True)\n",
    "dat_stock.drop(['avg(ecom_available_stock)', 'avg(size_availability)'], axis = 1, inplace = True)\n",
    "dat_stock.rename(columns = {'min_date_of_week': 'date', 'avg(buy_availability)': 'buy_availability'}, inplace = True)\n",
    "\n",
    "# Filter to SS20_range carryovers\n",
    "dat_stock = dat_stock[[(a in SS20_range) for a in dat_stock['article_number']]]\n",
    "dat_stock['date'] = pd.to_datetime(dat_stock['date'])\n",
    "\n",
    "# For merging with transaction data\n",
    "dat_stock['week'] = [t.week for t in dat_stock['date']]\n",
    "dat_stock['year'] = [t.year for t in dat_stock['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transactions + buy_availability\n",
    "dat = pd.merge(dat, dat_stock, \n",
    "               left_on = ['article_number', 'year', 'week'], \n",
    "               right_on = ['article_number', 'year', 'week'],\n",
    "               how = 'outer')\n",
    "\n",
    "dat['buy_availability'] = dat['buy_availability'].fillna(1)\n",
    "dat.dropna(inplace = True) # NA rows are 'gross_qty = 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----- Calculate observed gross_qty per season -----\n",
    "dat_season = pd.DataFrame(dat.groupby(['article_number'])['gross_qty'].sum())\n",
    "dat_season.rename(columns = {'gross_qty':'gross_qty_season'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WEEKLY averages for articles when fully stocked\n",
    "dat_stocked = pd.DataFrame(\n",
    "    dat[dat['buy_availability'] > 0.35].\n",
    "    groupby(['article_number'])['gross_qty'].\n",
    "    mean())\n",
    "\n",
    "dat_stocked.rename(columns = {'gross_qty':'gross_qty_stocked_weekly_avg'}, inplace= True)\n",
    "\n",
    "dat_stocked['stocked_season_projection'] = 26*dat_stocked['gross_qty_stocked_weekly_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season = (pd.merge(dat_season, dat_stocked, left_index=True, right_index=True, how = 'outer').round())\n",
    "\n",
    "dat_season.drop('gross_qty_stocked_weekly_avg', inplace=True, axis = 1)\n",
    "dat_season.rename(columns = {'stocked_season_projection':'understock_correction'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Clearance correction ----\n",
    "\n",
    "# --- *** NEED TO USE THE FULL SS19 DATA HERE TOO *** ---\n",
    "\n",
    "dat_wo_clearance = pd.read_csv('Transaction data full 2019.csv', low_memory=False, index_col = 0) \n",
    "dat_wo_clearance2 = pd.read_csv('Transaction Data/Transaction Data Q4 2018.csv', low_memory=False, index_col = 0)\n",
    "dat_wo_clearance = pd.concat([dat_wo_clearance, dat_wo_clearance2])\n",
    "del dat_wo_clearance2\n",
    "\n",
    "dat_wo_clearance = dat_wo_clearance[dat_wo_clearance['season'] == 'SS19']\n",
    "\n",
    "dat_wo_clearance.reset_index(inplace = True)\n",
    "dat_wo_clearance['consumer_order_date'] = pd.to_datetime(dat_wo_clearance['consumer_order_date'])\n",
    "dat_wo_clearance.drop(\n",
    "    ['country', 'brand', 'gross_sales_gross_disc_net_ret', 'net_sales', 'total_markdown',\n",
    "     'article_promotion_main_category_group', 'fraction_of_full_price', 'markdown', 'fw_or_ss'],\n",
    "    inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_wo_clearance = dat_wo_clearance[[(a in SS20_range) for a in dat_wo_clearance['article_number']]]\n",
    "\n",
    "dat_wo_clearance = dat_wo_clearance[dat_wo_clearance['clearance'] == 0]\n",
    "\n",
    "dat_wo_clearance = pd.DataFrame(dat_wo_clearance.groupby(['article_number', 'season'])['gross_qty'].sum())\n",
    "dat_wo_clearance.rename(columns = {'gross_qty': 'overstock_correction'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season = pd.merge(dat_season, dat_wo_clearance, how = 'outer', left_index = True, right_index = True)\n",
    "del dat_wo_clearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = (pd.DataFrame(\n",
    "    pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0))\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "        dropna())\n",
    "\n",
    "preds = preds[preds['season'] == 'SS19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season = pd.merge(\n",
    "    dat_season, preds, \n",
    "    left_on = ['article_number'],\n",
    "    right_on=['article'], \n",
    "    how = 'left').round()\n",
    "\n",
    "# dat_season.drop('article', axis = 1, inplace = True)\n",
    "del preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def projection(net, forecast, under, over):\n",
    "    if net > forecast:\n",
    "        return under\n",
    "    elif net < forecast:\n",
    "        return over\n",
    "    else:\n",
    "        return under\n",
    "\n",
    "dat_season['gross_qty*'] = (dat_season.\n",
    "                          apply(lambda row: projection(row['gross_qty_season'], \n",
    "                                                       row['ecom_marketing_forecast'], \n",
    "                                                       row['understock_correction'], \n",
    "                                                       row['overstock_correction']), axis = 1))\n",
    "dat_season = dat_season[dat_season['gross_qty*'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_season['gross_qty_season'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_season[['article', 'gross_qty_season', 'ecom_marketing_forecast']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
