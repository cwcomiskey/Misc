{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "source": [
    "# 0.0 Modules, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import clone\n",
    "from sklearn.externals.six.moves import xrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# plotly.tools.set_credentials_file(username='duplinskiy', api_key='RsZHhxIiAGGu7FN9P4bu')\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Useful example article -- C77124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat[dat['article_number'] == 'C77124']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "\n",
    "    - Small: < 100 units\n",
    "    - Large: > 30000 units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### MAPE: FW17, SS18, FW18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (pd.DataFrame(\n",
    "    pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0))\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "        dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat = pd.read_csv('dat.csv', low_memory=False, index_col = 0) # Wall time: 1min 47s\n",
    "# dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "# dat = dat[dat['gross_demand_quantity'] != 0] \n",
    "# dat.drop(['sales_organization', 'country', 'brand', 'sold_qty', 'std_margin', \n",
    "#           'return_qty', 'article_promotion_main_category_group', 'fw_or_ss'], \n",
    "#          inplace=True, axis = 1)\n",
    "# dat = dat.groupby(by = ['article_number', 'season']).agg('sum')[['net_qty']] # aggregate by year\n",
    "# dat.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat.to_csv('dat_raw_MAPE.csv')\n",
    "dat = pd.read_csv('dat_raw_MAPE.csv', low_memory=False, index_col = 0,) # to calculate raw MAPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge predictions\n",
    "dat1 = pd.merge(dat, preds, left_on=['article_number', 'season'], right_on=['article', 'season'])\n",
    "dat1 = dat1[dat1['season'] != 'SS19'].drop('article', axis = 1)\n",
    "dat1 = dat1[(dat1['net_qty'] != 0) & (dat1['ecom_marketing_forecast'] != 0)]\n",
    "del dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos DataFrame\n",
    "\n",
    "# dat.head() # 47756 \n",
    "# preds.head()\n",
    "\n",
    "dat1.shape\n",
    "dat1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pct error = [forecast - actual]/actual = (net_qty + stock - net_qty)/net_qty\n",
    "dat1['APE'] = abs(dat1['ecom_marketing_forecast'] - dat1['net_qty'])/dat1['net_qty']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = [0, 100, 10000, 30000]\n",
    "dat1['bin'] = pd.cut(np.array(dat1['net_qty']), bins)\n",
    "\n",
    "d = dat1.groupby(['season', 'bin'])['APE'].describe()[['count', 'mean', '50%']]\n",
    "\n",
    "order = {'SS17': 0, 'FW17': 1, 'SS18': 2, 'FW18': 3, 'SS19': 4}\n",
    "d['order_id'] = [order[i] for i in d.reset_index()['season']]\n",
    "\n",
    "d.sort_values(by = ['order_id', 'bin'], inplace=True)\n",
    "d.drop('order_id', axis = 1, inplace=True)\n",
    "\n",
    "d.round().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Corrected-MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": false
   },
   "source": [
    "### Understock EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "carryover = pd.read_csv('Stock left and of season.csv', low_memory=False, index_col = 0, usecols=['season', 'article_number', 'ecom_available_stock', 'buy_availability']) \n",
    "carryover = (pd.DataFrame(carryover.groupby(['article_number', 'season'])['ecom_available_stock'].min()).reset_index())\n",
    "\n",
    "dat0 = pd.read_csv('dat_raw_MAPE.csv', low_memory=False, index_col = 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = pd.merge(dat, carryover, left_on=['article_number', 'season'], right_on=['article_number', 'season']) # Add end of season stock information\n",
    "dat_understock = dat[dat['ecom_available_stock'] == 0] # leftover was 0, so understocked \n",
    "dat_understock = dat_understock[dat_understock['season'] != 'SS19'] # remove current season\n",
    "dat_understock = dat_understock[dat_understock['net_qty'] > 100] # remove small potatoes items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Sort & order by season ---\n",
    "bins = [0, 100, 1000, 30000]\n",
    "dat_understock['bin'] = pd.cut(np.array(dat_understock['net_qty']), bins)\n",
    "\n",
    "d = pd.DataFrame(dat_understock.groupby(['season', 'bin'])['article_number'].describe()['count'])\n",
    "\n",
    "order = {'SS17': 0, 'FW17': 1, 'SS18': 2, 'FW18': 3, 'SS19': 4}\n",
    "d['order_id'] = [order[i] for i in d.reset_index()['season']]\n",
    "\n",
    "d.sort_values(by = ['order_id', 'bin'], inplace=True)\n",
    "d.drop('order_id', axis = 1, inplace=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understock Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# dat = pd.read_csv('dat.csv', low_memory=False, index_col = 0) \n",
    "# dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "# dat = dat[dat['net_qty'] != 0] \n",
    "# dat = dat[dat['season'] != 'SS19']\n",
    "# dat.drop(['sales_organization', 'country', 'brand', 'sold_qty', 'gross_demand_quantity', 'net_sales', 'std_margin', 'return_qty', 'article_promotion_main_category_group', 'fw_or_ss'], inplace=True, axis = 1)\n",
    "\n",
    "# dat.set_index('consumer_order_date', inplace = True)\n",
    "# dat = dat.groupby(['article_number', 'season']).resample('W').sum() # 'aggregate' to weekly sums by article\n",
    "# dat.reset_index(inplace=True)\n",
    "# dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "# dat['week'] = [t.week for t in dat['consumer_order_date']]\n",
    "# dat['year'] = [t.year for t in dat['consumer_order_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# dat.to_csv('weekly_dat.csv')\n",
    "dat = pd.read_csv('weekly_dat.csv', low_memory=False, index_col = 0)\n",
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "dat = dat[dat['season'] != 'SS19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* THIS IS AN ISSUE ******* MISSING ~20K ARTICLES *********\n",
    "buy_avail = pd.read_csv('stock data per week and per article.csv', low_memory=False) # \n",
    "\n",
    "# buy_avail['min_date_of_week'] = pd.to_datetime(buy_avail['min_date_of_week'])\n",
    "# buy_avail = buy_avail[['article_number', 'min_date_of_week', 'avg(buy_availability)']]\n",
    "# buy_avail['week'] = [t.week for t in buy_avail['min_date_of_week']]\n",
    "# buy_avail['year'] = [t.year for t in buy_avail['min_date_of_week']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(buy_avail['article_number'].unique()) # ******* THIS IS AN ISSUE *******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buy_avail.to_csv('buy_availability.csv')\n",
    "buy_avail = pd.read_csv('buy_availability.csv', low_memory=False, index_col = 0)\n",
    "buy_avail = buy_avail[(buy_avail['year'] != 2019) & (buy_avail['year'] != 2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datA = pd.merge(dat, buy_avail, # THIS IS AN INNER JOIN. PROBLEM?\n",
    "                left_on = ['article_number', 'year', 'week'], \n",
    "                right_on = ['article_number', 'year', 'week'],\n",
    "               )\n",
    "\n",
    "# datB = pd.merge(dat, buy_avail, \n",
    "#                 left_on = ['article_number', 'year', 'week'], \n",
    "#                 right_on = ['article_number', 'year', 'week'],\n",
    "#                 how = 'outer')\n",
    "\n",
    "datA.shape # 719 653\n",
    "datB.shape # 2 290 052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datA[datA['article_number'] == 'CF1324']\n",
    "datB[datB['article_number'] == 'CF1324']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add indicator for buy availability > 0.35\n",
    "dat['stockout_indicator'] = dat['avg(buy_availability)'] < 0.35\n",
    "# dat['stockout_indicator'].mean() # 0.1706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_dat = pd.DataFrame(\n",
    "    dat.\n",
    "    groupby(['article_number', 'season'])['net_qty'].\n",
    "    sum()\n",
    ")\n",
    "\n",
    "# Season averages for articles when fully stocked\n",
    "stocked = pd.DataFrame(\n",
    "    dat[dat['stockout_indicator'] == False].\n",
    "    groupby(['article_number', 'season'])['net_qty'].\n",
    "    mean()\n",
    ")\n",
    "\n",
    "stocked['season_projected'] = 26*stocked['net_qty']\n",
    "\n",
    "season_dat = (\n",
    "    pd.merge(season_dat, stocked, left_index=True, right_index=True, how = 'outer').\n",
    "    rename(columns = {'net_qty_x':'season_actual', 'net_qty_y':'daily_available_avg'})\n",
    ")\n",
    "\n",
    "del stocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (pd.DataFrame(\n",
    "    pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0))\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "        dropna())\n",
    "\n",
    "season_dat = pd.merge(\n",
    "    season_dat, \n",
    "    preds, \n",
    "    left_index=True, \n",
    "    right_on=['article', 'season'], \n",
    "    how = 'outer').round().set_index(['article', 'season'])\n",
    "\n",
    "season_dat['season*'] = season_dat[['season_projected', 'season_actual']].max(axis = 1)\n",
    "season_dat['APE'] = abs(season_dat['season*'] - season_dat['ecom_marketing_forecast'])/season_dat['season*']*100\n",
    "\n",
    "season_dat.reset_index(inplace = True)\n",
    "season_dat.dropna(inplace = True)\n",
    "\n",
    "del preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = [0, 100, 1000, 30000]\n",
    "season_dat['bin'] = pd.cut(np.array(season_dat['season_actual']), bins) # net_qty\n",
    "\n",
    "d = season_dat.groupby(['season', 'bin'])['APE'].describe()[['count', 'mean', '50%']]\n",
    "\n",
    "# --- Sort by season ---\n",
    "\n",
    "order = {'SS17': 0, 'FW17': 1, 'SS18': 2, 'FW18': 3, 'SS19': 4}\n",
    "d['order_id'] = [order[i] for i in d.reset_index()['season']]\n",
    "\n",
    "d.sort_values(by = ['order_id', 'bin'], inplace=True)\n",
    "d.drop('order_id', axis = 1, inplace=True)\n",
    "\n",
    "d.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-MAPE: Overstock EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearance = pd.read_csv('Sale Data/Sale data FW17.csv', low_memory=False, index_col = 0) \n",
    "\n",
    "# clearance = clearance.reset_index() # groupby('article_number').resample('W')\n",
    "# clearance['consumer_order_date'] = pd.to_datetime(clearance['consumer_order_date'])\n",
    "# clearance.set_index('consumer_order_date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat0 = pd.read_csv('dat.csv', low_memory=False, index_col = 0) # Wall time: 1min 47s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Johannes said 'SALE' is Clearance, and outlet won't contribute much\n",
    "SALE = dat0[dat0['article_promotion_main_category_group'] == 'SALE']\n",
    "\n",
    "SALE['consumer_order_date'] = pd.to_datetime(SALE['consumer_order_date'])\n",
    "SALE.set_index('consumer_order_date', inplace = True)\n",
    "\n",
    "SALE2 = SALE[['article_number', 'season', 'net_qty']].groupby(['article_number', 'season']).resample('W').sum() # 'aggregate' to weekly sums by article\n",
    "SALE2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTOGRAM\n",
    "plt.rcParams[\"figure.figsize\"] = [18,4]\n",
    "SALE2['consumer_order_date'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SALE2.groupby(['consumer_order_date'])['net_qty'].sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Outlet ---\n",
    "outlet = dat0[dat0['article_promotion_main_category_group'] == 'Outlet']\n",
    "\n",
    "outlet['consumer_order_date'] = pd.to_datetime(outlet['consumer_order_date'])\n",
    "outlet.set_index('consumer_order_date', inplace = True)\n",
    "\n",
    "outlet2 = outlet[['article_number', 'season', 'net_qty']].groupby(['article_number', 'season']).resample('W').sum()\n",
    "outlet2.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [18,4]\n",
    "outlet2['consumer_order_date'].hist(bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-MAPE over/stock plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "- Spring Summer: (SS) 1 December -- 31 May\n",
    "- Fall Winter: (FW) 1 June -- 30 November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat0 = pd.read_csv('dat.csv', low_memory=False, index_col = 0) # Wall time: 1min 47s\n",
    "\n",
    "# dat = dat0.copy()\n",
    "# dat = dat[['consumer_order_date', 'article_number', \n",
    "#             'net_qty', 'article_promotion_main_category_group',\n",
    "#            'season']]\n",
    "# dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "\n",
    "# dat['clearance'] = (dat['article_promotion_main_category_group'] == 'SALE')\n",
    "# dat.drop('article_promotion_main_category_group', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat.to_csv('dat_clearance.csv')\n",
    "dat = pd.read_csv('dat_clearance.csv', low_memory=False, index_col = 0) \n",
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_wo_clearance = dat.copy()\n",
    "dat_wo_clearance = dat_wo_clearance[dat_wo_clearance['clearance'] != True] # **************\n",
    "dat_wo_clearance = pd.DataFrame(dat_wo_clearance.groupby(['article_number', 'consumer_order_date'])['net_qty'].sum())\n",
    "dat_wo_clearance = dat_wo_clearance.reset_index('article_number').groupby('article_number').resample('W').sum()\n",
    "dat_wo_clearance = pd.DataFrame(dat_wo_clearance.reset_index().groupby('consumer_order_date')['net_qty'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.head()\n",
    "dat_wo_clearance_clearance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Clearance and non-clearance -------\n",
    "dat_both = (pd.merge(dat, dat_wo_clearance, left_on='consumer_order_date', right_index=True).\n",
    "            rename(columns = {'net_qty_y':'Without Clearance', 'net_qty_x':'With Clearance'})\n",
    "           )\n",
    "\n",
    "# With/without Clearance weekly average net_qty \n",
    "plt.rcParams[\"figure.figsize\"] = [18,4]\n",
    "dat_both.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w/o Clearance Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# dat.to_csv('dat_clearance.csv')\n",
    "dat = pd.read_csv('dat_clearance.csv', low_memory=False, index_col = 0) \n",
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_wo_clearance = dat.copy()\n",
    "dat_wo_clearance = dat_wo_clearance[dat_wo_clearance['clearance'] != True]\n",
    "dat_wo_clearance = pd.DataFrame(dat_wo_clearance.groupby(['article_number', 'season'])['net_qty'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (pd.DataFrame(\n",
    "    pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0))\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "        dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_dat = (pd.merge(\n",
    "    dat_wo_clearance, preds, \n",
    "    left_index=True, right_on=['article', 'season'], \n",
    "    how = 'outer').\n",
    "              round().\n",
    "              # set_index(['article', 'season']).\n",
    "              dropna()\n",
    "             )\n",
    "season_dat = season_dat[season_dat['season'] != 'SS19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_dat['APE'] = abs(season_dat['net_qty'] - season_dat['ecom_marketing_forecast'])/season_dat['net_qty']*100\n",
    "\n",
    "# season_dat.reset_index(inplace = True)\n",
    "# season_dat.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = [0, 100, 10000, 30000]\n",
    "season_dat['bin'] = pd.cut(np.array(season_dat['net_qty']), bins)\n",
    "\n",
    "d = season_dat.groupby(['season', 'bin'])['APE'].describe()[['count', 'mean', '50%']]\n",
    "\n",
    "# --- Sort by season ---\n",
    "\n",
    "order = {'SS17': 0, 'FW17': 1, 'SS18': 2, 'FW18': 3, 'SS19': 4}\n",
    "d['order_id'] = [order[i] for i in d.reset_index()['season']]\n",
    "\n",
    "d.sort_values(by = ['order_id', 'bin'], inplace=True)\n",
    "d.drop('order_id', axis = 1, inplace=True)\n",
    "\n",
    "d.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-MAPE: both corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) buy_availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 617 ms, total: 14.6 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dat = pd.read_csv('weekly_dat.csv', low_memory=False, index_col = 0) # Wall time: 10.9 s\n",
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "\n",
    "dat['week'] = [t.week for t in dat['consumer_order_date']]\n",
    "dat['year'] = [t.year for t in dat['consumer_order_date']]\n",
    "\n",
    "buy_avail = pd.read_csv('buy_availability.csv', low_memory=False, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.merge(dat, buy_avail, \n",
    "                left_on = ['article_number', 'year', 'week'], \n",
    "                right_on = ['article_number', 'year', 'week'])\n",
    "\n",
    "# net_qty per season\n",
    "dat_season = pd.DataFrame(\n",
    "    dat.\n",
    "    groupby(['article_number', 'season'])['net_qty'].\n",
    "    sum()\n",
    ")\n",
    "dat_season.rename(columns = {'net_qty':'net_qty_season'}, inplace = True)\n",
    "\n",
    "# WEEKLY averages for articles when fully stocked\n",
    "dat_stocked = pd.DataFrame(\n",
    "    dat[dat['avg(buy_availability)'] > 0.35].\n",
    "    groupby(['article_number', 'season'])['net_qty'].\n",
    "    mean()\n",
    ")\n",
    "dat_stocked.rename(columns = {'net_qty':'net_qty_stocked_weekly_avg'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_stocked['stocked_season_projection'] = 26*dat_stocked['net_qty_stocked_weekly_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = (\n",
    "    pd.merge(dat_season, dat_stocked, left_index=True, right_index=True, how = 'outer').\n",
    "    round()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (pd.DataFrame(\n",
    "    pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0))\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "        dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = pd.merge(\n",
    "    dat_season, \n",
    "    preds, \n",
    "    left_index=True, \n",
    "    right_on=['article', 'season'], \n",
    "    how = 'outer').round().set_index(['article', 'season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season.drop('net_qty_stocked_weekly_avg', inplace=True, axis = 1)\n",
    "\n",
    "dat_season.rename(\n",
    "    columns = {'stocked_season_projection':'understock_correction', 'net_qty_season': 'net_qty'}, \n",
    "    inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del buy_avail, dat, dat_stocked, stocked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clearance correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall time: 42.4 s\n",
    "\n",
    "# dat.to_csv('dat_clearance.csv')\n",
    "dat_wo_clearance = pd.read_csv('dat_clearance.csv', low_memory=False, index_col = 0) \n",
    "dat_wo_clearance['consumer_order_date'] = pd.to_datetime(dat_wo_clearance['consumer_order_date'])\n",
    "\n",
    "dat_wo_clearance = dat_wo_clearance[dat_wo_clearance['clearance'] != True]\n",
    "dat_wo_clearance = pd.DataFrame(dat_wo_clearance.groupby(['article_number', 'season'])['net_qty'].sum())\n",
    "\n",
    "dat_wo_clearance.reset_index(inplace=True)\n",
    "dat_wo_clearance.rename(columns = {'article_number': 'article', 'net_qty': 'overstock_correction'}, inplace = True)\n",
    "dat_wo_clearance.set_index(['article', 'season'], inplace = True)\n",
    "\n",
    "del dat_wo_clearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = pd.merge(dat_season, dat_wo_clearance, \n",
    "                             how = 'outer', \n",
    "                             left_index = True, \n",
    "                             right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season = dat_season[\n",
    "    (dat_season['ecom_marketing_forecast'] != 0)\n",
    "    &\n",
    "    (dat_season['net_qty'] != 0)\n",
    "    &\n",
    "    (dat_season['season'] != 'SS19')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(net, forecast, under, over):\n",
    "    if net > forecast:\n",
    "        return under\n",
    "    elif net < forecast:\n",
    "        return over\n",
    "    else:\n",
    "        return under\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>season</th>\n",
       "      <th>net_qty</th>\n",
       "      <th>understock_correction</th>\n",
       "      <th>ecom_marketing_forecast</th>\n",
       "      <th>overstock_correction</th>\n",
       "      <th>net_qty*</th>\n",
       "      <th>APE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001354</td>\n",
       "      <td>FW17</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>11.533243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001354</td>\n",
       "      <td>FW18</td>\n",
       "      <td>1805.0</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1805.0</td>\n",
       "      <td>1805.0</td>\n",
       "      <td>21.883657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001354</td>\n",
       "      <td>SS17</td>\n",
       "      <td>276.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>276.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001354</td>\n",
       "      <td>SS18</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1254.0</td>\n",
       "      <td>1254.0</td>\n",
       "      <td>19.617225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>002214</td>\n",
       "      <td>FW17</td>\n",
       "      <td>2662.0</td>\n",
       "      <td>2662.0</td>\n",
       "      <td>2340.0</td>\n",
       "      <td>2676.0</td>\n",
       "      <td>2662.0</td>\n",
       "      <td>12.096168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article season  net_qty  understock_correction  ecom_marketing_forecast  \\\n",
       "0  001354   FW17   1474.0                 1474.0                   1304.0   \n",
       "1  001354   FW18   1805.0                 1738.0                   2200.0   \n",
       "2  001354   SS17    276.0                  598.0                      NaN   \n",
       "3  001354   SS18   1271.0                 1180.0                   1500.0   \n",
       "5  002214   FW17   2662.0                 2662.0                   2340.0   \n",
       "\n",
       "   overstock_correction  net_qty*        APE  \n",
       "0                1434.0    1474.0  11.533243  \n",
       "1                1805.0    1805.0  21.883657  \n",
       "2                 276.0     598.0        NaN  \n",
       "3                1254.0    1254.0  19.617225  \n",
       "5                2676.0    2662.0  12.096168  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_season.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_season['net_qty*'] = dat_season.apply(lambda row: projection(row[2], row[4], row[3], row[5]), axis = 1)\n",
    "dat_season = dat_season[dat_season['net_qty*'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "dat_season.loc[:,'APE'] = pd.to_numeric(\n",
    "    abs(dat_season['net_qty*'] - dat_season['ecom_marketing_forecast'])/dat_season['net_qty*']*100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <th>bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FW17</th>\n",
       "      <th>(0, 100]</th>\n",
       "      <td>799</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(100, 10000]</th>\n",
       "      <td>292</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10000, 30000]</th>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">SS18</th>\n",
       "      <th>(0, 100]</th>\n",
       "      <td>1180</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(100, 10000]</th>\n",
       "      <td>184</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10000, 30000]</th>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FW18</th>\n",
       "      <th>(0, 100]</th>\n",
       "      <td>901</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(100, 10000]</th>\n",
       "      <td>236</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10000, 30000]</th>\n",
       "      <td>106</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean  50%\n",
       "season bin                      \n",
       "FW17   (0, 100]         799  327\n",
       "       (100, 10000]     292   87\n",
       "       (10000, 30000]    24   22\n",
       "SS18   (0, 100]        1180  541\n",
       "       (100, 10000]     184   90\n",
       "       (10000, 30000]    23   21\n",
       "FW18   (0, 100]         901  344\n",
       "       (100, 10000]     236   65\n",
       "       (10000, 30000]   106   48"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [0, 100, 10000, 30000]\n",
    "dat_season.loc[:,'bin'] = pd.cut(np.array(dat_season['net_qty']), bins)\n",
    "\n",
    "d = dat_season.dropna().groupby(['season', 'bin'])['APE'].describe()[['mean', '50%']]\n",
    "\n",
    "order = {'SS17': 0, 'FW17': 1, 'SS18': 2, 'FW18': 3, 'SS19': 4}\n",
    "d['order_id'] = [order[i] for i in d.reset_index()['season']]\n",
    "\n",
    "d.sort_values(by = ['order_id', 'bin'], inplace=True)\n",
    "d.drop('order_id', axis = 1, inplace=True)\n",
    "\n",
    "d.round().astype(int)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
