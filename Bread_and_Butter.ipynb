{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Modules, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import clone\n",
    "from sklearn.externals.six.moves import xrange\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# plotly.tools.set_credentials_file(username='duplinskiy', api_key='RsZHhxIiAGGu7FN9P4bu')\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dat0 = pd.read_csv('seasonal_sales_indicators.csv',\n",
    "                 delimiter = '~')\n",
    "\n",
    "# len(dat0['article_number'].unique()) # 46573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns = [x.replace('t_eu_ecom_dit_dsf_transaction_t.', '') for x in dat.columns] # tidy column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat[dat['gross_demand_quantity'] != 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat = dat[['article_number', 'gross_demand_quantity', 'sold_qty',\n",
    "#            'net_qty', 'gross_sales_gross_disc_net_ret',\n",
    "#            'gross_sales_net_disc_gross_ret', 'net_sales', 'total_markdown',\n",
    "#            'temporary_markdown', 'permanent_markdown', 'employee_markdown',\n",
    "#            'fraction_of_full_price', 'markdown', 'sale']]\n",
    "\n",
    "\n",
    "dat = dat[['article_number', \n",
    "           'gross_demand_quantity', # mean, sum, std\n",
    "           'fraction_of_full_price', # mean, std\n",
    "           'markdown', # mean (INDICATOR) \n",
    "           'sale', # mean (INDICATOR no/sale)\n",
    "           'gross_sales_net_disc_gross_ret']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['sales_price'] = dat['gross_sales_net_disc_gross_ret'].divide(dat['gross_demand_quantity'])\n",
    "dat.drop('gross_sales_net_disc_gross_ret', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat_grouped = dat.groupby(by = 'article_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat_grouped.agg(['mean', 'std']) # Need count\n",
    "\n",
    "dat.drop([('markdown', 'std'),('sale', 'std')], axis = 1, inplace=True)\n",
    "\n",
    "dat[('gross_demand_quantity','sum')] = dat_grouped['gross_demand_quantity'].sum()\n",
    "\n",
    "dat.fillna(value=0, inplace = True) # Define single transaction article std to be 0\n",
    "dat = dat.replace([np.inf, -np.inf, np.nan], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Save/Load curated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/load tidied version\n",
    "\n",
    "# dat.to_csv('dat.csv')\n",
    "\n",
    "dat = pd.read_csv('dat.csv', low_memory=False, index_col = 0, header = [0,1]) # gotta encode multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Principal Component Analysis (for 2-D visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# SCALING: zero mean and unit variance \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(dat)\n",
    "# dat_scaled = scaler.transform(dat)\n",
    "\n",
    "# PRINCIPAL COMPONENT ANALYSIS\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2) # keep the first two principal components of the data\n",
    "pca.fit(dat)\n",
    "\n",
    "# transform data onto the first two principal components\n",
    "dat_pca = pca.transform(dat)\n",
    "\n",
    "# EDA \n",
    "np.isnan(np.log(dat_pca[:,0])).sum()\n",
    "(dat_pca[:,1] < 0).sum()\n",
    "(pd.DataFrame(dat_pca)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Clustering (aka classification, segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Feature-based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1 Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'sales_price'\n",
    "stat = 'mean'\n",
    "\n",
    "dat_feature_sorted = dat.sort_values([(feature, stat)], ascending=False) # Sort\n",
    "\n",
    "dat_feature_sorted['cumulative_feature'] = dat_feature_sorted[feature][stat].cumsum()\n",
    "\n",
    "total = dat_feature_sorted[feature][stat].sum() # standardized, so...???\n",
    "dat_feature_sorted['cumulative_pct_feature'] = dat_feature_sorted[feature][stat].cumsum()/total\n",
    "\n",
    "t = pd.Series(range(1, 46573))/46572\n",
    "dat_feature_sorted['cumulative_pct_articles'] = t.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_feature_sorted['feat_based'] = np.concatenate([np.ones(38089), np.zeros(8483)]) # match ____ counts below\n",
    "\n",
    "dat_feature_sorted['feat_based'].mean() \n",
    "dat_feature_sorted['feat_based'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2 Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dat_feature_sorted['cumulative_pct_articles'], \n",
    "         dat_feature_sorted['cumulative_pct_feature'],\n",
    "        linewidth = 3)\n",
    "\n",
    "# plt.xlabel('Percent of Articles')\n",
    "# plt.ylabel('Percent of Feature of Interst')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "plt.title('Majority, from a Minority of Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = dat_feature_sorted['cumulative_pct_articles'],\n",
    "    y = dat_feature_sorted['cumulative_pct_feature'],\n",
    "    mode = 'lines',\n",
    "    name = '45 degree line'\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "iplot(data, filename='scatter-mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.01 Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING: zero mean and unit variance \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dat)\n",
    "dat_scaled = pd.DataFrame(scaler.transform(dat), columns = dat.columns, index = dat.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now two datasets:\n",
    "\n",
    "    (1) dat\n",
    "    (2) dat_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# K-means clustering  --------------------  --------------------\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(dat_scaled)\n",
    "\n",
    "Kmeans = pd.Series(kmeans.predict(dat_scaled)) # Cluster assignments\n",
    "\n",
    "# Cluster counts\n",
    "print(Kmeans.value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmeans = (Kmeans != 0)*1 # lone vector\n",
    "Kmeans.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_scaled['Kmeans'] = Kmeans.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(\n",
    "    dat[[\n",
    "        ('gross_demand_quantity', 'mean'), \n",
    "        ('gross_demand_quantity', 'std'),\n",
    "        ('sales_price', 'mean'), \n",
    "        ('gross_demand_quantity', 'sum')\n",
    "        ]], \n",
    "    figsize=(10,10),\n",
    "    diagonal='kde',\n",
    "    c = Kmeans, \n",
    "    alpha = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(dat['gross_demand_quantity']['mean'], \n",
    "            dat['sales_price']['mean'], \n",
    "            c = Kmeans,\n",
    "            alpha = 0.5)\n",
    "plt.title('Sale Price vs. Gross Demand Quantity')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(dat['gross_demand_quantity']['sum'], \n",
    "            dat['sales_price']['mean'], \n",
    "            c = Kmeans,\n",
    "            alpha = 0.5)\n",
    "# plt.title('Sale Price vs. Gross Demand Qty (mean/sum)')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('GDQ Sum')\n",
    "\n",
    "# plt.xlabel('Sales Price')\n",
    "# plt.ylabel('log(sold_qty)')\n",
    "\n",
    "# plt.colorbar()\n",
    "# plt.rcParams[\"figure.figsize\"] = [5,5]\n",
    "# plt.title('K-means Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) gross_demand_quantity - basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram: total gross demand quantity, basic items\n",
    "dat[(dat['Kmeans'] == 0) & (dat['gross_demand_quantity']['sum'] < 10000)]['gross_demand_quantity']['sum'].hist(\n",
    "    bins = 100, \n",
    ")\n",
    "\n",
    "plt.xlabel('Total Gross Demand Quantity')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Basic Articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) gross_demand_quantity - non-basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram: total gross_demand_quantity, non-basic items\n",
    "dat[(dat['Kmeans'] != 0) & (dat['gross_demand_quantity']['sum'] < 5000)]['gross_demand_quantity']['sum'].hist(bins = 50)\n",
    "\n",
    "plt.xlabel('Total Gross Demand Quantity')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Non-basic Articles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) sales_price - basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram: sales_price, basic items\n",
    "dat[(dat['Kmeans'] == 0) & (dat['sales_price']['mean'] < 200) & (dat['sales_price']['mean'] > 0)]['sales_price']['mean'].hist(bins = 100)\n",
    "\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Basic Articles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) sales_price - non-basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram: sales_price, non-basic items\n",
    "dat[(dat['Kmeans'] != 0) & (dat['sales_price']['mean'] < 200)]['sales_price']['mean'].hist(bins = 100)\n",
    "\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Non-basic Articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Agglomerative Clustering -------------\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters = 3)\n",
    "agglom = pd.Series(agg.fit_predict(dat_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reclassify: majority/non as 0/1 \n",
    "\n",
    "agglom.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglom = (agglom != 4)*1 # lone vector\n",
    "agglom.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.log(dat['net_sales']['sum']), \n",
    "            np.log(dat['sold_qty']['sum']), \n",
    "            c = agglom,\n",
    "            alpha = 0.15)\n",
    "plt.xlabel('log(net_sales)')\n",
    "plt.ylabel('log(sold_qty)')\n",
    "plt.colorbar()\n",
    "plt.title('Agglomerative Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 DBSCAN \n",
    "\n",
    "'density based spatial clustering of applications with noise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN() # 3min 36s\n",
    "dbs = pd.Series(dbscan.fit_predict(dat_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reclassify: majority/non as 0/1 \n",
    "dbs01 = (dbs != -1)*1 # lone vector\n",
    "dbs01.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.log(dat['net_sales']['sum']), \n",
    "            np.log(dat['sold_qty']['sum']), \n",
    "            c = dbs01,\n",
    "            alpha = 0.15)\n",
    "plt.xlabel('log(net_sales)')\n",
    "plt.ylabel('log(sold_qty)')\n",
    "plt.colorbar()\n",
    "plt.title('DBSCAN Classification')\n",
    "plt.rcParams[\"figure.figsize\"] = [16,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Cross methods comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.DataFrame(dat_scaled['Kmeans'])\n",
    "f = pd.DataFrame(dat_feature_sorted['feat_based'])\n",
    "\n",
    "k_f = pd.merge(k, f, left_index = True, right_index = True)\n",
    "\n",
    "k_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_f.groupby(['Kmeans', 'feat_based']).size()\n",
    "(36800 + 7126)/46573 # 0.943% agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(data = {'Ag': agglom, 'Km': Kmeans, 'DB': dbs01})\n",
    "table.groupby(['Ag', 'Km', 'DB']).size()\n",
    "1 - 3924/46573 # 0.9157 kmeans-agglomerative agreement\n",
    "\n",
    "# table[(table['Ag'] == 0) & (table['Km'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Metrics\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/clustering.html for metric information (2.4.2 - __ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.silhouette_score(dat_scaled.drop('Kmeans', axis = 1), dat_scaled['Kmeans']) # higher better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski-Harabaz Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.calinski_harabaz_score(dat_scaled.drop('Kmeans', axis = 1), dat_scaled['Kmeans']) # higher better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.davies_bouldin_score(dat_scaled.drop('Kmeans', axis = 1), dat_scaled['Kmeans']) # lower is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Article Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2 = pd.read_csv('article_reference_data_y2016_18.csv',\n",
    "                 delimiter = '~',\n",
    "                  low_memory = False,\n",
    "                   usecols=['group_article', 'brand', 'sub_brand', 'season_create', 'season_active',\n",
    "                            'graphic', 'gender', 'age_group', 'retail_intro_date_global',\n",
    "                            'retail_exit_date_global', 'material_technology', 'pictogram_composition',\n",
    "                            'price_band', 'gender_age', 'construction_type', 'length_mes_uom_dim',\n",
    "                            'uom_dim', 'height_mes_uom_dim', 'width_mes_uom_dim', 'article_descr',\n",
    "                            'drop_season', 'uom_vol', 'uom_wgt', 'product_fit', 'material_way_type',\n",
    "                            'outer_sole_main_material', 'inner_sole_main_material', 'main_material_lining',\n",
    "                            'main_material_upper', 'dimension_uov', 'dimension_uom', 'carried_over_from',\n",
    "                            'drop_date', 'retail_exit_tgt_season', 'product_franchise', 'age_group_descr',\n",
    "                            'brand_descr', 'sub_brand_descr', 'lifecylce_status_prod_descr', 'brand_asset_descr',\n",
    "                            'rmh_retail_class_descr', 'rmh_retail_department_descr', 'rmh_retail_sub_class_descr',\n",
    "                            'rmh_retail_sub_dept_descr', 'rmh_category_descr', 'rmh_gender_descr',\n",
    "                            'rmh_retail_section_descr', 'rmh_product_division_descr', 'rmh_product_type_descr',\n",
    "                            'spm_color_first_descr', 'spm_color_second_descr', 'spm_color_third_descr',\n",
    "                            'spm_color_fourth_descr', 'product_franchise_descr'\n",
    "                           ]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat2 = dat2[['group_article', 'article_descr']]\n",
    "# dat = dat[['Kmeans']]\n",
    "# dat['article'] = dat.index\n",
    "\n",
    "# dat3 = pd.merge(dat, dat2, left_on= 'article', right_on= 'group_article')\n",
    "# dat3.columns = ('Kmeans', 'article', 'article2', 'descr')\n",
    "\n",
    "# dat3[dat3['Kmeans'] == 0]['descr'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Cluster Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.1 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat2['<feature>'].unique()\n",
    "# len(dat2['<feature'].unique())\n",
    "\n",
    "# Yes (11)\n",
    "# ['rmh_product_type_descr', 'rmh_product_division_descr', 'rmh_retail_section_descr',\n",
    "# 'rmh_gender_descr', 'rmh_category_descr', 'rmh_retail_department_descr',\n",
    "# 'sub_brand_descr', 'brand_descr', 'age_group_descr', 'age_group', 'gender']\n",
    "\n",
    "# No (13)\n",
    "# 'graphic', 'pictogram_composition', 'article_descr', 'lifecylce_status_prod_descr', 'brand_asset_descr', \n",
    "# 'rmh_retail_class_descr', 'rmh_retail_sub_class_descr', 'rmh_retail_sub_dept_descr', 'spm_color_first_descr', \n",
    "# 'spm_color_second_descr', 'spm_color_third_descr','spm_color_fourth_descr', 'product_franchise_descr' \n",
    "\n",
    "\n",
    "dat2.drop(['uom_dim', 'drop_season', 'uom_vol', 'uom_wgt', 'material_way_type', 'inner_sole_main_material', \n",
    "           'main_material_upper', 'outer_sole_main_material', 'main_material_lining', 'carried_over_from', 'drop_date',\n",
    "           'graphic', 'pictogram_composition', 'article_descr', 'lifecylce_status_prod_descr', 'brand_asset_descr', \n",
    "           'rmh_retail_class_descr', 'rmh_retail_sub_class_descr', 'rmh_retail_sub_dept_descr', 'spm_color_first_descr', \n",
    "           'spm_color_second_descr', 'spm_color_third_descr','spm_color_fourth_descr', 'product_franchise_descr' \n",
    "          ], axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datK = pd.DataFrame(dat_scaled['Kmeans'])\n",
    "# dat3 = pd.merge(dat2, datK, left_on = 'group_article', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2.shape\n",
    "len(dat2['group_article'].unique())\n",
    "dat2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.0.2 Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.3 One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_dummy = pd.get_dummies(dat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter = 500).fit(dat3.drop('Kmeans', axis = 1), dat3['Kmeans'])\n",
    "\n",
    "# log_reg = LogisticRegression(max_iter = 500).fit(dat_scaled.drop('Kmeans', axis = 1), np.random.randint(2, size = 46572))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(dat_scaled.drop('Kmeans', axis = 1), dat_scaled['Kmeans'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
