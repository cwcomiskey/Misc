{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Modules, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import clone\n",
    "from sklearn.externals.six.moves import xrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# plotly.tools.set_credentials_file(username='duplinskiy', api_key='RsZHhxIiAGGu7FN9P4bu')\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dat0 = pd.read_csv('seasonal_sales_indicators.csv',\n",
    "                 delimiter = '~')\n",
    "\n",
    "# len(dat0['article_number'].unique()) # 46573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat.columns = [x.replace('t_eu_ecom_dit_dsf_transaction_t.', '') for x in dat.columns] # tidy column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat[dat['gross_demand_quantity'] != 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat = dat[['article_number', 'gross_demand_quantity', 'sold_qty',\n",
    "#            'net_qty', 'gross_sales_gross_disc_net_ret',\n",
    "#            'gross_sales_net_disc_gross_ret', 'net_sales', 'total_markdown',\n",
    "#            'temporary_markdown', 'permanent_markdown', 'employee_markdown',\n",
    "#            'fraction_of_full_price', 'markdown', 'sale']]\n",
    "\n",
    "\n",
    "dat = dat[['article_number', \n",
    "           'gross_demand_quantity', # mean, sum, std\n",
    "           'fraction_of_full_price', # mean, std\n",
    "           'markdown', # mean (INDICATOR) \n",
    "           'sale', # mean (INDICATOR no/sale)\n",
    "           'gross_sales_net_disc_gross_ret']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat['sales_price'] = dat['gross_sales_net_disc_gross_ret'].divide(dat['gross_demand_quantity'])\n",
    "dat.drop('gross_sales_net_disc_gross_ret', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat_grouped = dat.groupby(by = 'article_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat_grouped.agg(['mean', 'std']) # Need count\n",
    "\n",
    "dat.drop([('markdown', 'std'),('sale', 'std')], axis = 1, inplace=True)\n",
    "\n",
    "dat[('gross_demand_quantity','sum')] = dat_grouped['gross_demand_quantity'].sum()\n",
    "\n",
    "dat.fillna(value=0, inplace = True) # Define single transaction article std to be 0\n",
    "dat = dat.replace([np.inf, -np.inf, np.nan], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Save/Load curated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save/load tidied version\n",
    "\n",
    "# dat.to_csv('dat.csv')\n",
    "\n",
    "dat = pd.read_csv('dat.csv', low_memory=False, index_col = 0, header = [0,1]) # gotta encode multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "    (0) dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Principal Component Analysis (for 2-D visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# SCALING: zero mean and unit variance \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(dat)\n",
    "# dat_scaled = scaler.transform(dat)\n",
    "\n",
    "# PRINCIPAL COMPONENT ANALYSIS\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2) # keep the first two principal components of the data\n",
    "pca.fit(dat)\n",
    "\n",
    "# transform data onto the first two principal components\n",
    "dat_pca = pca.transform(dat)\n",
    "\n",
    "# EDA \n",
    "np.isnan(np.log(dat_pca[:,0])).sum()\n",
    "(dat_pca[:,1] < 0).sum()\n",
    "(pd.DataFrame(dat_pca)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Clustering (aka classification, segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Feature-based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1 Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature = 'sales_price'\n",
    "stat = 'mean'\n",
    "\n",
    "dat_feature_sorted = dat.sort_values([(feature, stat)], ascending=False) # Sort\n",
    "\n",
    "# Add cumulative sum of feature\n",
    "dat_feature_sorted['cumulative_feature'] = dat_feature_sorted[feature][stat].cumsum()\n",
    "\n",
    "# Add cumulative percent of feature\n",
    "total = dat_feature_sorted[feature][stat].sum() \n",
    "dat_feature_sorted['cumulative_pct_feature'] = dat_feature_sorted[feature][stat].cumsum()/total\n",
    "\n",
    "# Add cumulative perfect of articles\n",
    "t = pd.Series(range(1, 46573))/46572\n",
    "dat_feature_sorted['cumulative_pct_articles'] = t.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add feature based cluster; i.e. top X are labelled non-basic\n",
    "dat_feature_sorted['feat_based'] = np.concatenate([np.ones(8483), np.zeros(38089)]) # match ____ counts below\n",
    "\n",
    "dat_feature_sorted['feat_based'].value_counts() \n",
    "\n",
    "f = pd.DataFrame(dat_feature_sorted['feat_based'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "    (0) dat\n",
    "    (1) dat_feature_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2 Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dat_feature_sorted['cumulative_pct_articles'], \n",
    "         dat_feature_sorted['cumulative_pct_feature'],\n",
    "        linewidth = 3)\n",
    "\n",
    "# plt.xlabel('Percent of Articles')\n",
    "# plt.ylabel('Percent of Feature of Interst')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "plt.title('Majority, from a Minority of Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = dat_feature_sorted['cumulative_pct_articles'],\n",
    "    y = dat_feature_sorted['cumulative_pct_feature'],\n",
    "    mode = 'lines',\n",
    "    name = '45 degree line'\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "iplot(data, filename='scatter-mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.01 Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SCALING: zero mean and unit variance \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dat)\n",
    "dat_scaled = pd.DataFrame(scaler.transform(dat), columns = dat.columns, index = dat.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "    (0) dat\n",
    "    (1) dat_feature_sorted\n",
    "    (2) dat_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# K-means clustering  --------------------  --------------------\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(dat_scaled)\n",
    "\n",
    "Kmeans = pd.Series(kmeans.predict(dat_scaled)) # Cluster assignments\n",
    "\n",
    "# Cluster counts\n",
    "print(Kmeans.value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Kmeans = (Kmeans != 0)*1 # lone vector\n",
    "Kmeans.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = pd.DataFrame({'kmeans': Kmeans.values}, index = dat_scaled.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(\n",
    "    dat[[\n",
    "        ('gross_demand_quantity', 'mean'), \n",
    "        ('gross_demand_quantity', 'std'),\n",
    "        ('sales_price', 'mean'), \n",
    "        ('gross_demand_quantity', 'sum')\n",
    "        ]], \n",
    "    figsize=(10,10),\n",
    "    diagonal='kde',\n",
    "    c = Kmeans, \n",
    "    alpha = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(dat['gross_demand_quantity']['mean'], \n",
    "            dat['sales_price']['mean'], \n",
    "            c = Kmeans,\n",
    "            alpha = 0.5)\n",
    "plt.title('Sale Price vs. Gross Demand Quantity')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(dat['gross_demand_quantity']['sum'], \n",
    "            dat['sales_price']['mean'], \n",
    "            c = Kmeans,\n",
    "            alpha = 0.5)\n",
    "# plt.title('Sale Price vs. Gross Demand Qty (mean/sum)')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('GDQ Sum')\n",
    "\n",
    "# plt.xlabel('Sales Price')\n",
    "# plt.ylabel('log(sold_qty)')\n",
    "\n",
    "# plt.colorbar()\n",
    "# plt.rcParams[\"figure.figsize\"] = [5,5]\n",
    "# plt.title('K-means Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) gross_demand_quantity - basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram: total gross demand quantity, basic items\n",
    "dat[(dat['Kmeans'] == 0) & (dat['gross_demand_quantity']['sum'] < 10000)]['gross_demand_quantity']['sum'].hist(\n",
    "    bins = 100, \n",
    ")\n",
    "\n",
    "plt.xlabel('Total Gross Demand Quantity')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Basic Articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) gross_demand_quantity - non-basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram: total gross_demand_quantity, non-basic items\n",
    "dat[(dat['Kmeans'] != 0) & (dat['gross_demand_quantity']['sum'] < 5000)]['gross_demand_quantity']['sum'].hist(bins = 50)\n",
    "\n",
    "plt.xlabel('Total Gross Demand Quantity')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Non-basic Articles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) sales_price - basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram: sales_price, basic items\n",
    "dat[(dat['Kmeans'] == 0) & (dat['sales_price']['mean'] < 200) & (dat['sales_price']['mean'] > 0)]['sales_price']['mean'].hist(bins = 100)\n",
    "\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Basic Articles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) sales_price - non-basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram: sales_price, non-basic items\n",
    "dat[(dat['Kmeans'] != 0) & (dat['sales_price']['mean'] < 200)]['sales_price']['mean'].hist(bins = 100)\n",
    "\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Non-basic Articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Agglomerative Clustering -------------\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters = 6)\n",
    "agglom = pd.Series(agg.fit_predict(dat_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Reclassify: majority/non as 0/1 \n",
    "\n",
    "agglom.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agglom = (agglom != 0)*1 # lone vector\n",
    "agglom.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'agglom': agglom.values}, index = dat_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(dat_scaled['gross_demand_quantity']['mean'], \n",
    "            dat_scaled['sales_price']['mean'], \n",
    "            c = agglom,\n",
    "            alpha = 0.25)\n",
    "plt.title('Agglomerative Classes: Sale Price vs. Gross Demand Qty (mean/sum)')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(dat_scaled['gross_demand_quantity']['sum'], \n",
    "            dat_scaled['sales_price']['mean'], \n",
    "            c = agglom,\n",
    "            alpha = 0.25)\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('GDQ Sum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 DBSCAN \n",
    "\n",
    "'density based spatial clustering of applications with noise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN() # 3min 36s\n",
    "dbs = pd.Series(dbscan.fit_predict(dat_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Reclassify: majority/non as 0/1 \n",
    "dbs01 = (dbs != -1)*1 # lone vector\n",
    "dbs01.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbs = pd.DataFrame({'dbs': dbs01.values}, index = dat_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.log(dat['net_sales']['sum']), \n",
    "            np.log(dat['sold_qty']['sum']), \n",
    "            c = dbs01,\n",
    "            alpha = 0.15)\n",
    "plt.xlabel('log(net_sales)')\n",
    "plt.ylabel('log(sold_qty)')\n",
    "plt.colorbar()\n",
    "plt.title('DBSCAN Classification')\n",
    "plt.rcParams[\"figure.figsize\"] = [16,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Cross methods comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_a = pd.merge(k, a, left_index = True, right_index = True)\n",
    "\n",
    "f_k_a = pd.merge(f, k_a, left_index=True, right_index=True)\n",
    "\n",
    "# fka_dbs = pd.merge(f_k_a, dbs, left_index=True, right_index=True)\n",
    "\n",
    "f_k_a.head()\n",
    "\n",
    "f_k_a.groupby(['feat_based', 'kmeans', 'agglom']).size()\n",
    "# (26620 + 4100)/46573 # 0.943% agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = pd.DataFrame(data = {'Ag': agglom, 'Km': Kmeans, 'DB': dbs01})\n",
    "table.groupby(['Ag', 'Km', 'DB']).size()\n",
    "1 - 3924/46573 # 0.9157 kmeans-agglomerative agreement\n",
    "\n",
    "# table[(table['Ag'] == 0) & (table['Km'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Clustering Metrics\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/clustering.html for metric information (2.4.2 - __ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_feature_sorted_scaled = pd.DataFrame(StandardScaler().fit(dat_feature_sorted).transform(dat_feature_sorted), \n",
    "                                         columns = dat_feature_sorted.columns,\n",
    "                                         index = dat_feature_sorted.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# higher better\n",
    "\n",
    "metrics.silhouette_score(dat_feature_sorted_scaled, f['feat_based']) # feature based: 0.555\n",
    "\n",
    "metrics.silhouette_score(dat_feature_sorted, f['feat_based']) # feature based: 0.59568\n",
    "metrics.silhouette_score(dat_scaled, k['kmeans']) # k-means: 0.44598\n",
    "metrics.silhouette_score(dat_scaled, a['agglom']) # agglomerative: 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski-Harabaz Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# higher better\n",
    "\n",
    "metrics.calinski_harabaz_score(dat_feature_sorted_scaled, f['feat_based']) # 20816\n",
    "\n",
    "metrics.calinski_harabaz_score(dat_feature_sorted, f['feat_based']) # 77433\n",
    "metrics.calinski_harabaz_score(dat_scaled, k['kmeans']) # 7474\n",
    "metrics.calinski_harabaz_score(dat_scaled, a['agglom']) # 5147\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lower is better\n",
    "\n",
    "metrics.davies_bouldin_score(dat_feature_sorted_scaled, f['feat_based']) # 0.779\n",
    "\n",
    "metrics.davies_bouldin_score(dat_feature_sorted, f['feat_based']) # 0.44\n",
    "metrics.davies_bouldin_score(dat_scaled, k['kmeans']) # 1.186\n",
    "metrics.davies_bouldin_score(dat_scaled, a['agglom']) # 1.47\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Article Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat2 = pd.read_csv('article_reference_data_y2016_18.csv',\n",
    "                 delimiter = '~',\n",
    "                  low_memory = False,\n",
    "                   usecols=['group_article', 'brand', 'sub_brand', 'season_create', 'season_active',\n",
    "                            'graphic', 'gender', 'age_group', 'retail_intro_date_global',\n",
    "                            'retail_exit_date_global', 'material_technology', 'pictogram_composition',\n",
    "                            'price_band', 'gender_age', 'construction_type', 'length_mes_uom_dim',\n",
    "                            'uom_dim', 'height_mes_uom_dim', 'width_mes_uom_dim', 'article_descr',\n",
    "                            'drop_season', 'uom_vol', 'uom_wgt', 'product_fit', 'material_way_type',\n",
    "                            'outer_sole_main_material', 'inner_sole_main_material', 'main_material_lining',\n",
    "                            'main_material_upper', 'dimension_uov', 'dimension_uom', 'carried_over_from',\n",
    "                            'drop_date', 'retail_exit_tgt_season', 'product_franchise', 'age_group_descr',\n",
    "                            'brand_descr', 'sub_brand_descr', 'lifecylce_status_prod_descr', 'brand_asset_descr',\n",
    "                            'rmh_retail_class_descr', 'rmh_retail_department_descr', 'rmh_retail_sub_class_descr',\n",
    "                            'rmh_retail_sub_dept_descr', 'rmh_category_descr', 'rmh_gender_descr',\n",
    "                            'rmh_retail_section_descr', 'rmh_product_division_descr', 'rmh_product_type_descr',\n",
    "                            'spm_color_first_descr', 'spm_color_second_descr', 'spm_color_third_descr',\n",
    "                            'spm_color_fourth_descr', 'product_franchise_descr'\n",
    "                           ]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat2['<feature>'].unique()\n",
    "# len(dat2['<feature'].unique())\n",
    "\n",
    "# Yes (11)\n",
    "# ['rmh_product_type_descr', 'rmh_product_division_descr', 'rmh_retail_section_descr',\n",
    "# 'rmh_gender_descr', 'rmh_category_descr', 'rmh_retail_department_descr',\n",
    "# 'sub_brand_descr', 'brand_descr', 'age_group_descr', 'age_group', 'gender']\n",
    "\n",
    "# No (13)\n",
    "# 'graphic', 'pictogram_composition', 'article_descr', 'lifecylce_status_prod_descr', 'brand_asset_descr', \n",
    "# 'rmh_retail_class_descr', 'rmh_retail_sub_class_descr', 'rmh_retail_sub_dept_descr', 'spm_color_first_descr', \n",
    "# 'spm_color_second_descr', 'spm_color_third_descr','spm_color_fourth_descr', 'product_franchise_descr' \n",
    "\n",
    "\n",
    "dat2.drop(['uom_dim', 'drop_season', 'uom_vol', 'uom_wgt', 'material_way_type', 'inner_sole_main_material', \n",
    "           'main_material_upper', 'outer_sole_main_material', 'main_material_lining', 'carried_over_from', 'drop_date',\n",
    "           'graphic', 'pictogram_composition', 'article_descr', 'lifecylce_status_prod_descr', 'brand_asset_descr', \n",
    "           'rmh_retail_class_descr', 'rmh_retail_sub_class_descr', 'rmh_retail_sub_dept_descr', 'spm_color_first_descr', \n",
    "           'spm_color_second_descr', 'spm_color_third_descr','spm_color_fourth_descr', 'product_franchise_descr' \n",
    "          ], axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save/read data\n",
    "\n",
    "# dat2.to_csv('dat2.csv')\n",
    "\n",
    "dat2 = pd.read_csv('dat2.csv', low_memory=False, index_col = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(dat2['season_create'])\n",
    "\n",
    "dat2.shape\n",
    "len(dat2['group_article'].unique())\n",
    "dat2.dtypes\n",
    "dat2.season_create.unique()\n",
    "\n",
    "{print(x, '-->', len(dat2[x].unique())) for x in dat2.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Cluster Prediction\n",
    "\n",
    "    *Pause this for now, determine if clustering helps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.2 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardize non-numeric features\n",
    "\n",
    "# dt = pd.DataFrame(data = dat2.dtypes, columns = ['type'])\n",
    "# num = dt[(dt['type'] == 'int64') | (dt['type'] == 'float64')].index\n",
    "# obj = dt[(dt['type'] != 'int64') & (dt['type'] != 'float64')].index\n",
    "\n",
    "# dat2_num = dat2[num]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(dat2_num)\n",
    "\n",
    "# dat2_num_scaled = pd.DataFrame(scaler.transform(dat2_num), columns = dat2_num.columns, index = dat2_num.index)\n",
    "# dat2_num_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.3 One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat2_dummy = pd.get_dummies(dat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat2_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = dat2_dummy.drop('Kmeans', axis = 1)\n",
    "y = dat2_dummy['Kmeans']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "tree.score(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Neural Network (vanilla MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mlp = MLPClassifier().fit(X_train, y_train) \n",
    "    # --Arguments--\n",
    "    # solver = <something>\n",
    "    # hidden_layer_sizes = [<nodes_layer_1>, <nodes_layer_2>, ...]\n",
    "    # activation = '<function>'\n",
    "    # alpha = <regularization/complexity_param> \n",
    "    \n",
    "mlp.score(X_train, y_train)\n",
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Baseline eCom Demand Forecaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of prior season data for an article, predict prior season average of ten most similarly **PRICED** items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "    \n",
    "    (0) Group by item, season\n",
    "    (1) Select item\n",
    "    (2) Identify ten most similarly priced items in the same season\n",
    "    (3) Average the sales of those 10 for the last (same) season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat00 = pd.read_csv('seasonal_sales_indicators.csv',\n",
    "                 delimiter = '~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat0 = dat00.copy()\n",
    "\n",
    "dat0.columns = [x.replace('t_eu_ecom_dit_dsf_transaction_t.', '') for x in dat0.columns] # tidy column names\n",
    "\n",
    "dat0.drop(['sales_organization', 'sku', 'total_markdown', 'temporary_markdown', \n",
    "           'permanent_markdown', 'employee_markdown', 'fraction_of_full_price', \n",
    "           'markdown', 'sale'], inplace=True, axis = 1)\n",
    "\n",
    "# {print(x, '-->', len(dat0[x].unique())) for x in dat0.columns}\n",
    "# dat0.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat0 = dat0.groupby(by = ['article_number', 'season']).agg('sum')\n",
    "\n",
    "dat0['price'] = dat0['net_sales']/dat0['net_qty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = {'SS17': 0, 'FW17': 1, 'SS18': 2, 'FW18': 3, 'SS19': 4}\n",
    "dat0['order_id'] = [order[i] for i in dat0.reset_index()['season']]\n",
    "dat0.sort_values(by = ['article_number', 'order_id'], inplace=True)\n",
    "dat0.drop('order_id', axis = 1, inplace=True)\n",
    "\n",
    "dat0.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat0 = dat0[[s in ['SS17', 'SS19'] for s in dat0['season']]]\n",
    "dat0 = dat0.replace([np.inf, -np.inf, np.nan], 0)\n",
    "dat0 = dat0[dat0['price'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat17 = dat0[dat0['season'] == 'SS17'].sort_values('price')\n",
    "dat19 = dat0[dat0['season'] == 'SS19'].sort_values('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat17.price.rolling(11, center=True).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
