{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "source": [
    "# 0.0 Modules, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import clone\n",
    "from sklearn.externals.six.moves import xrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# plotly.tools.set_credentials_file(username='duplinskiy', api_key='RsZHhxIiAGGu7FN9P4bu')\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# 1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 1.1 Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dat0 = pd.read_csv('seasonal_sales_indicators.csv',\n",
    "                 delimiter = '~')\n",
    "\n",
    "# len(dat0['article_number'].unique()) # 46573\n",
    "\n",
    "dat = dat0.copy()\n",
    "\n",
    "dat.columns = [x.replace('t_eu_ecom_dit_dsf_transaction_t.', '') for x in dat.columns] # tidy column names\n",
    "\n",
    "dat = dat[dat['gross_demand_quantity'] != 0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Data\n",
    "\n",
    "    (0) dat0\n",
    "    (1) dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature subset for clustering\n",
    "\n",
    "dat = dat[['article_number', \n",
    "           'gross_demand_quantity', # mean, sum, std\n",
    "           'fraction_of_full_price', # mean, std\n",
    "           'markdown', # mean (INDICATOR) \n",
    "           'sale', # mean (INDICATOR no/sale)\n",
    "           'gross_sales_net_disc_gross_ret']]\n",
    "\n",
    "dat['sales_price'] = dat['gross_sales_net_disc_gross_ret'].divide(dat['gross_demand_quantity'])\n",
    "dat.drop('gross_sales_net_disc_gross_ret', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat_grouped = dat.groupby(by = 'article_number')\n",
    "\n",
    "dat = dat_grouped.agg(['mean', 'std']) # Need count\n",
    "dat.drop([('markdown', 'std'),('sale', 'std')], axis = 1, inplace=True)\n",
    "dat[('gross_demand_quantity','sum')] = dat_grouped['gross_demand_quantity'].sum()\n",
    "\n",
    "dat.fillna(value=0, inplace = True) # Define single transaction article std to be 0\n",
    "dat = dat.replace([np.inf, -np.inf, np.nan], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 1.2 Save/Load curated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save/load tidied version\n",
    "\n",
    "# dat.to_csv('dat.csv')\n",
    "\n",
    "dat = pd.read_csv('dat.csv', low_memory=False, index_col = 0, header = [0,1]) # gotta encode multi-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# 2 Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 2.0 Baseline: feature-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.0.1 Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature = 'sales_price'\n",
    "stat = 'mean'\n",
    "\n",
    "dat_feature_sorted = dat.sort_values([(feature, stat)], ascending=False) # Sort\n",
    "\n",
    "# # --- Feature cumulative sum ---\n",
    "# dat_feature_sorted['cumulative_feature'] = dat_feature_sorted[feature][stat].cumsum()\n",
    "\n",
    "# # --- Feature cumulative percent ---\n",
    "# total = dat_feature_sorted[feature][stat].sum() \n",
    "# dat_feature_sorted['cumulative_pct_feature'] = dat_feature_sorted[feature][stat].cumsum()/total\n",
    "\n",
    "# # --- Cumulative percent of articles ---\n",
    "# t = pd.Series(range(1, 46573))/46572\n",
    "# dat_feature_sorted['cumulative_pct_articles'] = t.values\n",
    "\n",
    "# --- Add feature based cluster; i.e. top X are labelled non-basic ---\n",
    "f = pd.DataFrame(np.concatenate([np.ones(8483), np.zeros(38089)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dat_feature_sorted)\n",
    "dat_feature_sorted_scaled = scaler.transform(dat_feature_sorted)\n",
    "\n",
    "pca = PCA(n_components=2) # keep the first two principal components of the data\n",
    "pca.fit(dat_feature_sorted_scaled)\n",
    "\n",
    "# transform data onto the first two principal components\n",
    "dat_pca = pd.DataFrame(pca.transform(dat_feature_sorted_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.0.2 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- 1 ----\n",
    "plt.rcParams[\"figure.figsize\"] = [16,5]\n",
    "\n",
    "plt.scatter(dat_pca[0], \n",
    "            dat_pca[1], \n",
    "            c = f[0],\n",
    "            alpha = 0.5)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Basic and Non-basic')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.xlabel('Principal Component 1')\n",
    "\n",
    "\n",
    "# --- 2 ----\n",
    "plt.plot(dat_feature_sorted['cumulative_pct_articles'], \n",
    "         dat_feature_sorted['cumulative_pct_feature'],\n",
    "        linewidth = 3)\n",
    "\n",
    "# plt.xlabel('Percent of Articles')\n",
    "# plt.ylabel('Percent of Feature of Interst')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "plt.title('Majority, from a Minority of Articles')\n",
    "\n",
    "\n",
    "# --- 3 ----\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = dat_feature_sorted['cumulative_pct_articles'],\n",
    "    y = dat_feature_sorted['cumulative_pct_feature'],\n",
    "    mode = 'lines',\n",
    "    name = '45 degree line'\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "iplot(data, filename='scatter-mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 2.01 Scale (for ML approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SCALING: zero mean and unit variance \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dat)\n",
    "dat_scaled = pd.DataFrame(scaler.transform(dat), columns = dat.columns, index = dat.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 2.1 k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.1.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# K-means clustering  --------------------  --------------------\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(dat_scaled)\n",
    "\n",
    "Kmeans = pd.Series(kmeans.predict(dat_scaled)) # Cluster assignments\n",
    "\n",
    "# Cluster counts\n",
    "print(Kmeans.value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Kmeans = (Kmeans != 0)*1 # lone vector\n",
    "Kmeans.value_counts()\n",
    "\n",
    "# k = pd.DataFrame({'kmeans': Kmeans.values}, index = dat_scaled.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.1.2 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(\n",
    "    dat[[\n",
    "        ('gross_demand_quantity', 'mean'), \n",
    "        ('gross_demand_quantity', 'std'),\n",
    "        ('sales_price', 'mean'), \n",
    "        ('gross_demand_quantity', 'sum')\n",
    "        ]], \n",
    "    figsize=(10,10),\n",
    "    diagonal='kde',\n",
    "    c = Kmeans, \n",
    "    alpha = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(dat['gross_demand_quantity']['mean'], \n",
    "            dat['sales_price']['mean'], \n",
    "            c = Kmeans,\n",
    "            alpha = 0.5)\n",
    "plt.title('Sale Price vs. Gross Demand Quantity')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(dat['gross_demand_quantity']['sum'], \n",
    "            dat['sales_price']['mean'], \n",
    "            c = Kmeans,\n",
    "            alpha = 0.5)\n",
    "# plt.title('Sale Price vs. Gross Demand Qty (mean/sum)')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('GDQ Sum')\n",
    "\n",
    "# plt.xlabel('Sales Price')\n",
    "# plt.ylabel('log(sold_qty)')\n",
    "\n",
    "# plt.colorbar()\n",
    "# plt.rcParams[\"figure.figsize\"] = [5,5]\n",
    "# plt.title('K-means Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.1.3 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### (a) gross_demand_quantity - basic\n",
    "\n",
    "# Histogram: total gross demand quantity, basic items\n",
    "dat[(dat['Kmeans'] == 0) & (dat['gross_demand_quantity']['sum'] < 10000)]['gross_demand_quantity']['sum'].hist(bins = 100)\n",
    "\n",
    "plt.xlabel('Total Gross Demand Quantity')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Basic Articles')\n",
    "\n",
    "#### (b) gross_demand_quantity - non-basic\n",
    "\n",
    "# Histogram: total gross_demand_quantity, non-basic items\n",
    "dat[(dat['Kmeans'] != 0) & (dat['gross_demand_quantity']['sum'] < 5000)]['gross_demand_quantity']['sum'].hist(bins = 50)\n",
    "\n",
    "plt.xlabel('Total Gross Demand Quantity')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Non-basic Articles')\n",
    "\n",
    "\n",
    "#### (c) sales_price - basic\n",
    "\n",
    "# Histogram: sales_price, basic items\n",
    "dat[(dat['Kmeans'] == 0) & (dat['sales_price']['mean'] < 200) & (dat['sales_price']['mean'] > 0)]['sales_price']['mean'].hist(bins = 100)\n",
    "\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Basic Articles')\n",
    "\n",
    "\n",
    "#### (d) sales_price - non-basic\n",
    "\n",
    "# Histogram: sales_price, non-basic items\n",
    "dat[(dat['Kmeans'] != 0) & (dat['sales_price']['mean'] < 200)]['sales_price']['mean'].hist(bins = 100)\n",
    "\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Quantity')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,12]\n",
    "plt.title('Non-basic Articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 2.2 Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Agglomerative Clustering -------------\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters = 6)\n",
    "agglom = pd.Series(agg.fit_predict(dat_scaled))\n",
    "\n",
    "agglom.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Reclassify: majority/non as 0/1 \n",
    "\n",
    "agglom = (agglom != 0)*1 # lone vector\n",
    "agglom.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'agglom': agglom.values}, index = dat_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(dat_scaled['gross_demand_quantity']['mean'], \n",
    "            dat_scaled['sales_price']['mean'], \n",
    "            c = agglom,\n",
    "            alpha = 0.25)\n",
    "plt.title('Agglomerative Classes: Sale Price vs. Gross Demand Qty (mean/sum)')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(dat_scaled['gross_demand_quantity']['sum'], \n",
    "            dat_scaled['sales_price']['mean'], \n",
    "            c = agglom,\n",
    "            alpha = 0.25)\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('GDQ Sum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 2.3 DBSCAN \n",
    "\n",
    "'density based spatial clustering of applications with noise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps = 0.145, min_samples = 5) # 3min 36s\n",
    "dbs = pd.Series(dbscan.fit_predict(dat_scaled))\n",
    "\n",
    "dbs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Reclassify: majority/non as 0/1 \n",
    "dbs01 = (dbs == -1)*1 # lone vector\n",
    "dbs01.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbs = pd.DataFrame({'dbs': dbs01.values}, index = dat_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.log(dat['net_sales']['sum']), \n",
    "            np.log(dat['sold_qty']['sum']), \n",
    "            c = dbs01,\n",
    "            alpha = 0.15)\n",
    "plt.xlabel('log(net_sales)')\n",
    "plt.ylabel('log(sold_qty)')\n",
    "plt.colorbar()\n",
    "plt.title('DBSCAN Classification')\n",
    "plt.rcParams[\"figure.figsize\"] = [16,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 2.4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.4.1 Cross methods comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# k_a = pd.merge(k, a, left_index = True, right_index = True)\n",
    "# f_k_a = pd.merge(f, k_a, left_index=True, right_index=True)\n",
    "# fka_dbs = pd.merge(f_k_a, dbs, left_index=True, right_index=True)\n",
    "\n",
    "# .groupby(['feat_based', 'kmeans', 'agglom']).size()\n",
    "\n",
    "# table = pd.DataFrame(data = {'Ag': agglom, 'Km': Kmeans, 'DB': dbs01})\n",
    "# table.groupby(['Ag', 'Km', 'DB']).size()\n",
    "1 - 3924/46573 # 0.9157 kmeans-agglomerative agreement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.4.2 Clustering Metrics\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/clustering.html for metric information (2.4.2 - __ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_feature_sorted_scaled = pd.DataFrame(StandardScaler().fit(dat_feature_sorted).transform(dat_feature_sorted), \n",
    "                                         columns = dat_feature_sorted.columns,\n",
    "                                         index = dat_feature_sorted.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Silhouette Coefficient\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# higher better\n",
    "\n",
    "metrics.silhouette_score(dat_feature_sorted_scaled, f['feat_based']) # feature based: 0.555\n",
    "\n",
    "metrics.silhouette_score(dat_feature_sorted, f['feat_based']) # feature based: 0.59568\n",
    "metrics.silhouette_score(dat_scaled, k['kmeans']) # k-means: 0.44598\n",
    "metrics.silhouette_score(dat_scaled, a['agglom']) # agglomerative: 0.33\n",
    "metrics.silhouette_score(dat_scaled, dbs['dbs']) # DBSCAN: 0.25\n",
    "\n",
    "\n",
    "#### Calinski-Harabaz Index\n",
    "\n",
    "# higher better\n",
    "\n",
    "metrics.calinski_harabaz_score(dat_feature_sorted_scaled, f['feat_based']) # 20816\n",
    "\n",
    "metrics.calinski_harabaz_score(dat_feature_sorted, f['feat_based']) # 77433\n",
    "metrics.calinski_harabaz_score(dat_scaled, k['kmeans']) # 7474\n",
    "metrics.calinski_harabaz_score(dat_scaled, a['agglom']) # 5147\n",
    "metrics.calinski_harabaz_score(dat_scaled, dbs['dbs']) # 2311\n",
    "\n",
    "\n",
    "#### Davies-Bouldin Index\n",
    "\n",
    "# lower is better\n",
    "\n",
    "metrics.davies_bouldin_score(dat_feature_sorted_scaled, f['feat_based']) # 0.779\n",
    "\n",
    "metrics.davies_bouldin_score(dat_feature_sorted, f['feat_based']) # 0.44\n",
    "metrics.davies_bouldin_score(dat_scaled, k['kmeans']) # 1.186\n",
    "metrics.davies_bouldin_score(dat_scaled, a['agglom']) # 1.47\n",
    "metrics.davies_bouldin_score(dat_scaled, dbs['dbs']) # 2.51\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# 3 Article Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_ref = pd.read_csv('article_reference_data_y2016_18.csv',\n",
    "                 delimiter = '~',\n",
    "                  low_memory = False,\n",
    "                   usecols=['group_article', 'brand', 'sub_brand', 'season_create', 'season_active',\n",
    "                            'graphic', 'gender', 'age_group', 'retail_intro_date_global',\n",
    "                            'retail_exit_date_global', 'material_technology', 'pictogram_composition',\n",
    "                            'price_band', 'gender_age', 'construction_type', 'length_mes_uom_dim',\n",
    "                            'uom_dim', 'height_mes_uom_dim', 'width_mes_uom_dim', 'article_descr',\n",
    "                            'drop_season', 'uom_vol', 'uom_wgt', 'product_fit', 'material_way_type',\n",
    "                            'outer_sole_main_material', 'inner_sole_main_material', 'main_material_lining',\n",
    "                            'main_material_upper', 'dimension_uov', 'dimension_uom', 'carried_over_from',\n",
    "                            'drop_date', 'retail_exit_tgt_season', 'product_franchise', 'age_group_descr',\n",
    "                            'brand_descr', 'sub_brand_descr', 'lifecylce_status_prod_descr', 'brand_asset_descr',\n",
    "                            'rmh_retail_class_descr', 'rmh_retail_department_descr', 'rmh_retail_sub_class_descr',\n",
    "                            'rmh_retail_sub_dept_descr', 'rmh_category_descr', 'rmh_gender_descr',\n",
    "                            'rmh_retail_section_descr', 'rmh_product_division_descr', 'rmh_product_type_descr',\n",
    "                            'spm_color_first_descr', 'spm_color_second_descr', 'spm_color_third_descr',\n",
    "                            'spm_color_fourth_descr', 'product_franchise_descr'\n",
    "                           ]\n",
    "                     )\n",
    "\n",
    "dat_ref.drop(['uom_dim', 'drop_season', 'uom_vol', 'uom_wgt', 'material_way_type', 'inner_sole_main_material', \n",
    "           'main_material_upper', 'outer_sole_main_material', 'main_material_lining', 'carried_over_from', 'drop_date',\n",
    "           'graphic', 'pictogram_composition', 'article_descr', 'lifecylce_status_prod_descr', 'brand_asset_descr', \n",
    "           'rmh_retail_class_descr', 'rmh_retail_sub_class_descr', 'rmh_retail_sub_dept_descr', 'spm_color_first_descr', \n",
    "           'spm_color_second_descr', 'spm_color_third_descr','spm_color_fourth_descr',\n",
    "            'length_mes_uom_dim', 'height_mes_uom_dim', 'width_mes_uom_dim'], axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat_ref.to_csv('dat_ref.csv')\n",
    "\n",
    "dat_ref = pd.read_csv('dat_ref.csv', low_memory=False, index_col = 0) \n",
    "# dat_ref = dat_ref[dat_ref['retail_intro_date_global'] != '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert retail_intro_date_global to a date for adding color to FW_18 vs. FW_17 plot\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# dat_ref['retail_intro_date_global'] = dat_ref['retail_intro_date_global'].apply(str)\n",
    "\n",
    "# pd.to_datetime(datetime.strptime(dat_ref['retail_intro_date_global'][0], '%Y%m%d'))\n",
    "# pd.to_datetime(datetime.strptime('20180414', '%Y%m%d'))\n",
    "\n",
    "dat_ref['retail_intro_date_global'] = pd.Series([pd.to_datetime(datetime.strptime(x, '%Y%m%d')) for x in dat_ref['retail_intro_date_global']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_ref.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat_ref.dtypes\n",
    "{print(x, '-->', len(dat_ref[x].unique())) for x in dat_ref.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "{print(x, '-->', dat_ref[x].unique()) for x in dat_ref.columns}\n",
    "\n",
    "# pd.to_datetime(dat2['season_create'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# 4 Cluster Prediction\n",
    "\n",
    "    *On hold, determine if clustering helps first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# 5 Prior season baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Note: key article -- C77124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat[dat['article_number'] == 'C77124']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Note: what is low volume? What is high volume?\n",
    "\n",
    "    - Small: < 100 units\n",
    "    - Large: > 30000 units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 5.0 Andras Embeddings/distances, Exasol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyexasol\n",
    "from pyexasol import ExaConnection\n",
    "\n",
    "# Id: comischr\n",
    "# Pwd: Comiskey021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "C = pyexasol.connect(dsn='10.143.86.51:8583', user='comischr', password='Comiskey021', compression=True, encryption=True)\n",
    "\n",
    "# dfAndrasFeatures = C.export_to_pandas(\"\"\"select * from READ_SCV.ARTICLE_EMBEDDINGS\"\"\")\n",
    "# dfAndrasDistances = C.export_to_pandas(\"\"\"select * from READ_SCV.ARTICLE_DISTANCE\"\"\")\n",
    "\n",
    "# dfAndreasDistances = C.export_to_pandas(\"\"\"select * from READ_SCV.PPC_SIM_ARTICLEPAIR_EUCL_V\"\"\")\n",
    "\n",
    "dfAndreasDistances = C.export_to_pandas(\"\"\"select * from READ_SCV.PPC_SIM_ARTICLEPAIR_EUCL_V where EUCL_SCORE < 7.8 / 2\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dfAndrasDistances.to_csv('dfAndrasDistances.csv')\n",
    "\n",
    "dfAndrasDistances = pd.read_csv('dfAndrasDistances.csv')\n",
    "dfAndrasDistances.drop(['Unnamed: 0', 'COSINE'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 5.1 Similarity-based Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retain season introduced column\n",
    "\n",
    "dat0 = pd.read_csv('sales_and_features1_mini.csv',\n",
    "                 delimiter = '~',\n",
    "                  low_memory = False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat0 = pd.read_csv('sales_and_features.csv',\n",
    "                 delimiter = '~',\n",
    "                  low_memory = False,\n",
    "                   usecols=['sl1.consumer_order_date', \n",
    "                            'sl1.sales_organization', \n",
    "                            'sl1.country',\n",
    "                            'sl1.article_number', \n",
    "                            'sl1.brand',\n",
    "                            'sl1.gross_demand_quantity', \n",
    "                            'sl1.sold_qty', \n",
    "                            'sl1.net_qty',\n",
    "                            'sl1.net_sales', \n",
    "                            'sl1.std_margin',\n",
    "                            'sl1.return_qty', \n",
    "                            'sl1.article_promotion_main_category_group', \n",
    "                            'fw_or_ss', \n",
    "                            'season'\n",
    "                           ]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat0.columns = [x.replace('sl1.', '') for x in dat0.columns] # tidy column names\n",
    "dat0['consumer_order_date'] = pd.to_datetime(dat0['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dat0.to_csv('dat.csv')\n",
    "\n",
    "dat = pd.read_csv('dat.csv', low_memory=False, index_col = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dat0 = pd.read_csv('seasonal_sales_indicators.csv', delimiter = '~')\n",
    "# dat.columns = [x.replace('t_eu_ecom_dit_dsf_transaction_t.', '') for x in dat.columns] # tidy column names\n",
    "\n",
    "dat = dat0.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat = dat[dat['gross_demand_quantity'] != 0] \n",
    "dat.drop(['sales_organization', 'country', 'brand', 'sold_qty', 'net_qty', 'net_sales', \n",
    "          'std_margin', 'return_qty', 'article_promotion_main_category_group',\n",
    "          'fw_or_ss'], \n",
    "         inplace=True, axis = 1)\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat = dat.groupby(by = ['article_number', 'season']).agg('sum')[['gross_demand_quantity']] # aggregation step\n",
    "dat.reset_index(inplace=True)\n",
    "\n",
    "dfAndrasDistances = pd.read_csv('dfAndrasDistances.csv')\n",
    "dfAndrasDistances.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data:\n",
    "    # (1) dat\n",
    "    # (2) dfAndrasDistances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = dat[dat['season'] == 'FW17'][['article_number', 'gross_demand_quantity']]\n",
    "kahuna = (\n",
    "    pd.merge(dfAndrasDistances, d, left_on = 'A1', right_on = 'article_number').\n",
    "    drop(['article_number', 'COSINE'], axis = 1).\n",
    "    rename(columns = {'gross_demand_quantity': 'A1_FW17'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kahuna = (\n",
    "    pd.merge(kahuna, d, left_on = 'A2', right_on = 'article_number').\n",
    "    drop(['article_number'], axis = 1).\n",
    "    rename(columns = {'gross_demand_quantity': 'A2_FW17'})\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = dat[dat['season'] == 'FW18'][['article_number', 'gross_demand_quantity']]\n",
    "kahuna = (\n",
    "    pd.merge(kahuna, d, left_on = 'A1', right_on = 'article_number').\n",
    "    drop('article_number', axis = 1).\n",
    "    rename(columns = {'gross_demand_quantity': 'A1_FW18'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighbors = (\n",
    "    pd.DataFrame(kahuna.groupby('A1')['EUCLIDEAN'].nsmallest(10)).\n",
    "    reset_index()\n",
    "        )\n",
    "\n",
    "kahuna = (\n",
    "    pd.merge(kahuna, neighbors, left_on = ['A1', kahuna.index], right_on = ['A1', 'level_1']).\n",
    "    drop(['level_1', 'EUCLIDEAN_y'], axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = (\n",
    "    pd.DataFrame(kahuna.groupby('A1')['A2_FW17'].mean()).\n",
    "    reset_index().\n",
    "    rename(columns = {'A2_FW17': 'A1_FW18_pred'})\n",
    ")\n",
    "\n",
    "kahuna = pd.merge(kahuna, preds, left_on='A1', right_on='A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kahuna_skinny = kahuna[['A1', 'A1_FW17', 'A1_FW18', 'A1_FW18_pred']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kahuna_ref = (pd.merge(kahuna_skinny, dat_ref, left_on='A1', right_on='group_article').\n",
    "              # drop(['retail_intro_date_global'], axis = 1).\n",
    "              drop_duplicates()\n",
    "             )\n",
    "              \n",
    "\n",
    "kahuna_ref.shape\n",
    "kahuna_ref.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### MAPEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kahuna_skinny[(kahuna_skinny['A1_FW17'] > 500) & (kahuna_skinny['A1_FW18'] < 25)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- MAPE ---- Mean Absolute Percent Error ----\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return pd.Series(np.mean(np.abs((y_true - y_pred) / y_true)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mape(kahuna_skinny['A1_FW18'], kahuna_skinny['A1_FW18_pred']) # prediction: mean of similars\n",
    "\n",
    "mape(kahuna_skinny['A1_FW18'], kahuna_skinny['A1_FW17']) # predcition: last year\n",
    "\n",
    "\n",
    "\n",
    "# ---- SANITY CHECK ----\n",
    "mape(kahuna_skinny['A1_FW18'][0:4], kahuna_skinny['A1_FW18_pred'][0:4]) # 2336.97\n",
    "\n",
    "(np.abs(567.9 - 34)/34 + np.abs(744.8 - 52)/52 + np.abs(276.8 - 7)/7 + np.abs(269.1 - 10)/10)/4 * 100 # 2336.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- Prediction: mean of similars ----\n",
    "kahuna_skinny['Pct_Err'] = np.abs(kahuna_skinny['A1_FW18_pred'] - kahuna_skinny['A1_FW18'])/kahuna_skinny['A1_FW18'] * 100\n",
    "kahuna_skinny['Pct_Err'].describe() # overall MAPE\n",
    "\n",
    "# ---- Prediction: last year ----\n",
    "kahuna_skinny['Pct_Err2'] = np.abs(kahuna_skinny['A1_FW17'] - kahuna_skinny['A1_FW18'])/kahuna_skinny['A1_FW18'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- wAPE ---- weighted absolute percent error\n",
    "\n",
    "tot_gdq = kahuna_skinny['A1_FW18'].sum() \n",
    "tot_gdq # 819215\n",
    "\n",
    "kahuna_skinny['pct_gdq'] = kahuna_skinny['A1_FW18']/tot_gdq \n",
    "kahuna_skinny['pct_gdq'].sum() # 1.0\n",
    "\n",
    "np.average(kahuna_skinny['Pct_Err'], weights = kahuna_skinny['pct_gdq']) # 140.99\n",
    "np.average(kahuna_skinny['Pct_Err'], weights = kahuna_skinny['A1_FW18']) # 140.99\n",
    "\n",
    "np.sum((kahuna_skinny['A1_FW18']*kahuna_skinny['Pct_Err']))/tot_gdq # 140.99\n",
    "np.sum(kahuna_skinny['Pct_Err']*kahuna_skinny['pct_gdq']) # 140.99\n",
    "\n",
    "# ---- SANITY CHECK ---- \n",
    "\n",
    "# wAPE by hand: 1704.466\n",
    "(34*(np.abs(567.9 - 34)/34) + 52*(np.abs(744.8 - 52)/52) + 7*(np.abs(276.8 - 7)/7) + 10*(np.abs(269.1 - 10)/10))/103*100\n",
    "\n",
    "# wAPE : 1704.466\n",
    "np.sum(kahuna_skinny['Pct_Err'][0:4]*kahuna_skinny['pct_gdq'][0:4])/np.sum(kahuna_skinny['pct_gdq'][0:4]) # 1704.466\n",
    "\n",
    "np.average(kahuna_skinny['Pct_Err'][0:4], weights = kahuna_skinny['pct_gdq'][0:4]) # 1704.466\n",
    "\n",
    "# Sanity intact : )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---- BENCHMARK ----\n",
    "kahuna_skinny['benchmark0'] = 0\n",
    "kahuna_skinny['benchmark'] = 2\n",
    "\n",
    "mape(kahuna_skinny['A1_FW18'], kahuna_skinny['benchmark']) # 92.36\n",
    "\n",
    "kahuna_skinny['Pct_Err00'] = np.abs(kahuna_skinny['benchmark0'] - kahuna_skinny['A1_FW18'])/kahuna_skinny['A1_FW18'] * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---- BENCHMARK!! ---- MAPE biased toward under-prediction\n",
    "\n",
    "benchmark_preds = np.arange(0, 10, 0.1) # predictions\n",
    "benchmark_mape = [mape(kahuna_skinny['A1_FW18'], i) for i in benchmark_preds] # calculated MAPE\n",
    "\n",
    "pd.DataFrame(benchmark_mape, index = benchmark_preds).plot() # plot\n",
    "\n",
    "plt.title('Benchmark Models -- MAPE by Prediction Constant')\n",
    "plt.ylabel('MAPE')\n",
    "plt.xlabel('Prediction Constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- MAPE by bin ---\n",
    "\n",
    "# bins\n",
    "bins = [0, 100, 10000, 30000]\n",
    "kahuna_skinny['bin'] = pd.cut(np.array(kahuna_skinny['A1_FW18']), bins)\n",
    "\n",
    "# ---- Similarity-based prediction, by bin ----\n",
    "kahuna_skinny.groupby('bin')['Pct_Err'].describe()[['count', 'mean']]\n",
    "\n",
    "# ---- Prediction: last year ---- MAPE by bins\n",
    "kahuna_skinny.groupby('bin')['Pct_Err2'].describe()[['count', 'mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "kahuna_skinny[kahuna_skinny['Pct_Err'] < 3000]['Pct_Err'].hist(bins = 100)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "kahuna_skinny[(kahuna_skinny['Pct_Err'] < 3000) & (kahuna_skinny['A1_FW17'] < 21)]['Pct_Err'].hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.log(kahuna_skinny['A1_FW17']), \n",
    "            np.log(kahuna_skinny['A1_FW18']), \n",
    "            # c = dbs01,\n",
    "            alpha = 0.15)\n",
    "# plt.xlabel('FW17 net_qty')\n",
    "# plt.ylabel('Abs Pct Error')\n",
    "\n",
    "kahuna_skinny[['A1_FW17', 'A1_FW18']].corr()\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = [16,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Binned MAPES by RMHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kahuna_ref.shape\n",
    "kahuna_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat0['consumer_order_date'] == dat0['consumer_order_date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = [0, 100, 10000, 30000]\n",
    "kahuna_ref['bin'] = pd.cut(np.array(kahuna_skinny['A1_FW17']), bins)\n",
    "\n",
    "# ---- Similarity-based prediction, by bin ----\n",
    "kahuna_ref.groupby('bin')['Pct_Err'].describe()[['count', 'mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat_ref.dtypes\n",
    "{print(x, '-->', len(kahuna_ref[x].unique())) for x in kahuna_ref.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kahuna_ref.groupby(['product_franchise_descr', 'bin'])['Pct_Err'].describe()[['count', 'mean']]\n",
    "\n",
    "# Conclusion: no discernable pattern in MAPE by RMH categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 5.3 SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dat0 = pd.read_csv('seasonal_sales_indicators.csv',\n",
    "                 delimiter = '~')\n",
    "\n",
    "# dat_ref = pd.read_csv('dat_ref.csv', low_memory=False, index_col = 0) \n",
    "\n",
    "dat = dat0.copy()\n",
    "\n",
    "dat.columns = [x.replace('t_eu_ecom_dit_dsf_transaction_t.', '') for x in dat.columns] # tidy column names\n",
    "dat = dat[dat['gross_demand_quantity'] != 0] \n",
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "\n",
    "datter = dat.merge(dat_ref, left_on = 'article_number', right_on = 'group_article', how = 'left')\n",
    "datter.shape\n",
    "\n",
    "{print(x, '-->', len(dat[x].unique())) for x in dat.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 5.4 Time Series EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_g = dat.groupby(by = ['article_number', 'consumer_order_date']) # data grouped\n",
    "\n",
    "dat_aggs = pd.DataFrame(dat_g.agg('sum')['gross_demand_quantity']) # data aggregates\n",
    "\n",
    "dat_aggs.reset_index(level = 'article_number', inplace=True)\n",
    "\n",
    "dat_aggs = dat_aggs.groupby('article_number')\n",
    "dat_aggs = dat_aggs.resample('W').sum() # 'aggregate' to weekly sums by article\n",
    "\n",
    "dat_aggs = dat_aggs.reset_index()\n",
    "\n",
    "# Convert article_number to column, gross_demand_quantity to data\n",
    "dat_aggs_pivoted = dat_aggs.pivot(index = 'consumer_order_date', \n",
    "                                  columns = 'article_number', \n",
    "                                  values = 'gross_demand_quantity')\n",
    "\n",
    "# dat.fillna(value=0, inplace = True) # Define single transaction article std to be 0\n",
    "plt.rcParams[\"figure.figsize\"] = [18,6]\n",
    "\n",
    "dat_aggs_pivoted[dat_aggs_pivoted.columns[0:20]].plot()\n",
    "\n",
    "\n",
    "# S21490 (arbitrarily) for learning time series EDA\n",
    "\n",
    "datS21 = (dat_aggs[dat_aggs['article_number'] == 'S21490'].\n",
    "          set_index('consumer_order_date').\n",
    "          drop('article_number', axis = 1)\n",
    "         )\n",
    "\n",
    "weekly = datS21.resample('W').sum() # 'aggregate' to weekly sums\n",
    "\n",
    "weekly.head()\n",
    "weekly.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# 6 Generalized Autoregressive Scoring models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 6.0 Mean of mids (monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv('dat.csv', low_memory=False, index_col = 0) \n",
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "\n",
    "dat = dat[dat['gross_demand_quantity'] != 0] \n",
    "dat.drop(['sales_organization', 'country', 'brand', 'sold_qty', 'net_qty', 'net_sales', \n",
    "          'std_margin', 'return_qty', 'article_promotion_main_category_group',\n",
    "          'fw_or_ss'], \n",
    "         inplace=True, axis = 1)\n",
    "\n",
    "dat0 = dat.copy()\n",
    "\n",
    "dat_ref = pd.read_csv('dat_ref.csv', low_memory=False, index_col = 0) \n",
    "dat_ref.head()\n",
    "# {print(x, '-->', dat_ref[x].unique()) for x in dat_ref.columns}\n",
    "\n",
    "# ---- To filter to shoes ----\n",
    "dat_ref = (dat_ref[['group_article', 'rmh_product_type_descr']].\n",
    "           drop_duplicates()\n",
    "          )\n",
    "dat_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use dat_ref to filter dat to shoes only\n",
    "dat_ref = dat_ref[['SHOES' in x for x in dat_ref['rmh_product_type_descr']]] # filter to shoes\n",
    "dat = pd.merge(dat, dat_ref, left_on='article_number', right_on='group_article') # merge to reduce dat to shoes\n",
    "dat.drop(['group_article', 'rmh_product_type_descr'], axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gross_demand_quantity by article, order date \n",
    "dat_g = dat.groupby(by = ['article_number', 'consumer_order_date']) # data grouped\n",
    "dat_g = pd.DataFrame(dat_g.agg('sum')['gross_demand_quantity']) # data aggregates\n",
    "dat_g.reset_index(level = 'article_number', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# weekly article_number gross_demand_quantity\n",
    "dat_g = dat_g.groupby('article_number').resample('W').sum() \n",
    "dat_g.reset_index(level = 'article_number', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add columns: year, week \n",
    "dat_g['year'] = [t.year for t in dat_g.index]\n",
    "dat_g['week'] = [t.week for t in dat_g.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----- article gross_demand_quantity BY YEAR -----\n",
    "dat_by_year = (dat_g.drop('week', axis = 1).\n",
    "               groupby(['article_number', 'year']).\n",
    "               sum().\n",
    "               reset_index()\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----- 2017 articles: 100 < gross_demand_quantity < 10000 \n",
    "mids2017 = dat_by_year[\n",
    "    (dat_by_year['year'] == 2017) \n",
    "    & \n",
    "    (dat_by_year['gross_demand_quantity'] > 100)\n",
    "    &\n",
    "    (dat_by_year['gross_demand_quantity'] < 10000)]\n",
    "\n",
    "# ----- 2018 articles: 100 < gross_demand_quantity < 10000 \n",
    "mids2018 = dat_by_year[\n",
    "    (dat_by_year['year'] == 2018) \n",
    "    & \n",
    "    (dat_by_year['gross_demand_quantity'] > 100)\n",
    "    &\n",
    "    (dat_by_year['gross_demand_quantity'] < 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat_g_2017mids = dat_g[[a in mids2017['article_number'].unique() for a in dat_g['article_number']]]\n",
    "dat_g_2018mids = dat_g[[a in mids2018['article_number'].unique() for a in dat_g['article_number']]]\n",
    "\n",
    "# pd.cut(np.array(dat_by_year['gross_demand_quantity']), [0, 100, 10000, 30000]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A = (dat_g_2017mids.\n",
    "     groupby('week').\n",
    "     mean().\n",
    "     drop('year', axis = 1).\n",
    "     rename(columns = {'gross_demand_quantity': '2017mean'})\n",
    "    )\n",
    "B = (dat_g_2018mids.\n",
    "     groupby('week').\n",
    "     mean().\n",
    "     drop('year', axis = 1).\n",
    "     rename(columns = {'gross_demand_quantity': '2018mean'})\n",
    "    )\n",
    "pd.merge(A, B, left_index=True, right_index=True).plot() # AWESOME PLOT\n",
    "plt.title('Shoes: 100 < gross_demand_quantity < 10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----- article gross_demand_quantity BY YEAR -----\n",
    "dat_by_year = (dat_g.drop('week', axis = 1).\n",
    "               groupby(['article_number', 'year']).\n",
    "               sum().\n",
    "               reset_index()\n",
    "              )\n",
    "\n",
    "# ----- 2017 articles: 100 < gross_demand_quantity < 10000 \n",
    "mids2017 = dat_by_year[\n",
    "    (dat_by_year['year'] == 2017) \n",
    "    & \n",
    "    (dat_by_year['gross_demand_quantity'] > 100)\n",
    "    &\n",
    "    (dat_by_year['gross_demand_quantity'] < 10000)]\n",
    "\n",
    "# ----- 2018 articles: 100 < gross_demand_quantity < 10000 \n",
    "mids2018 = dat_by_year[\n",
    "    (dat_by_year['year'] == 2018) \n",
    "    & \n",
    "    (dat_by_year['gross_demand_quantity'] > 100)\n",
    "    &\n",
    "    (dat_by_year['gross_demand_quantity'] < 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate MAPE of 2017/2018 articles against their own mean\n",
    "\n",
    "mids2017['gross_demand_quantity'].mean()\n",
    "mape(mids2017['gross_demand_quantity'], mids2017['gross_demand_quantity'].mean()) # 2017\n",
    "\n",
    "mape(mids2018['gross_demand_quantity'], mids2018['gross_demand_quantity'].mean()) # 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Time series plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# .pivot(...) ----- for time series plotting\n",
    "dat_g_2017mids_pivot = dat_g.pivot(columns = 'article_number', values = 'gross_demand_quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_g_2017mids_pivot['monthly_avg'] = dat_g_2017mids_pivot.apply(np.mean, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2017 residuals by article\n",
    "resids = dat_g_2017mids_pivot.copy()\n",
    "for c in dat_g_2017mids_pivot.columns:\n",
    "    resids[c] = dat_g_2017mids_pivot[c] - dat_g_2017mids_pivot['monthly_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot of residuals (random subset)\n",
    "\n",
    "from random import sample \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]\n",
    "resids[sample(list(resids.columns), 10)].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mid-range articles for 2017: data through 2019\n",
    "dat_g.head()\n",
    "dat_g['year'].value_counts()\n",
    "\n",
    "t = pd.DataFrame(dat_g.groupby(dat_g.index)['gross_demand_quantity'].mean())\n",
    "t.head()\n",
    "t.shape\n",
    "t.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Linear regression on 2017 mid range articles (not sure what use this is)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "t['X'] = range(1, 107)\n",
    "y = t['gross_demand_quantity']\n",
    "t.head()\n",
    "\n",
    "reg = sm.OLS(t['gross_demand_quantity'], sm.add_constant(t['X'])).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 6.1 Mean baseline + simliarity-based adjustment \n",
    "    --> annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv('dat.csv', low_memory=False, index_col = 0) # Wall time: 1min 47s\n",
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat[dat['gross_demand_quantity'] != 0] \n",
    "dat.drop(['sales_organization', 'country', 'brand', 'sold_qty', 'net_qty', 'net_sales', \n",
    "          'std_margin', 'return_qty', 'article_promotion_main_category_group',\n",
    "          'fw_or_ss'], \n",
    "         inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat0 = dat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_ref = pd.read_csv('dat_ref.csv', low_memory=False, index_col = 0) \n",
    "# {print(x, '-->', dat_ref[x].unique()) for x in dat_ref.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- add reference information, for filter to shoes ----\n",
    "dat_ref = (dat_ref[['group_article', 'rmh_product_type_descr']].\n",
    "           drop_duplicates()\n",
    "          )\n",
    "dat_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use dat_ref to filter dat to shoes only\n",
    "dat_ref = dat_ref[['SHOES' in x for x in dat_ref['rmh_product_type_descr']]] # filter to shoes\n",
    "dat = pd.merge(dat, dat_ref, left_on='article_number', right_on='group_article') # merge to reduce dat to shoes\n",
    "dat.drop(['group_article', 'rmh_product_type_descr'], axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gross_demand_quantity groupby article, order date \n",
    "dat_g = dat.groupby(by = ['article_number', 'consumer_order_date']) # data grouped\n",
    "dat_g = pd.DataFrame(dat_g.agg('sum')['gross_demand_quantity']) # data aggregates\n",
    "dat_g.reset_index(level = 'article_number', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Add columns: year, week \n",
    "dat['year'] = [t.year for t in dat['consumer_order_date']]\n",
    "dat['week'] = [t.week for t in dat['consumer_order_date']]\n",
    "\n",
    "# Wall time: 1min 33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# article_demand_quantity by year\n",
    "dat_annual = (dat[['article_number', 'year', 'gross_demand_quantity']].\n",
    "              groupby(['article_number', 'year']).\n",
    "              sum().\n",
    "              reset_index()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_annual.groupby('year')['gross_demand_quantity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# filter to 2017/2018 articles with mid-range demand; for MAPE, plots\n",
    "m = mids2017['article_number'].unique()\n",
    "dat_g_2017mids = dat_g[[a in m for a in dat_g['article_number']]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = mids2018['article_number'].unique()\n",
    "dat_g_2018mids = dat_g[[a in m for a in dat_g['article_number']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add columns: year, week \n",
    "dat_g['year'] = [t.year for t in dat_g.index]\n",
    "dat_g['week'] = [t.week for t in dat_g.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----- article gross_demand_quantity BY YEAR -----\n",
    "dat_by_year = (dat_g.drop('week', axis = 1).\n",
    "               groupby(['article_number', 'year']).\n",
    "               sum().\n",
    "               reset_index()\n",
    "              )\n",
    "\n",
    "# ----- 2017 articles: 100 < gross_demand_quantity < 10000 \n",
    "mids2017 = dat_by_year[\n",
    "    (dat_by_year['year'] == 2017) \n",
    "    & \n",
    "    (dat_by_year['gross_demand_quantity'] > 100)\n",
    "    &\n",
    "    (dat_by_year['gross_demand_quantity'] < 10000)]\n",
    "\n",
    "# ----- 2018 articles: 100 < gross_demand_quantity < 10000 \n",
    "mids2018 = dat_by_year[\n",
    "    (dat_by_year['year'] == 2018) \n",
    "    & \n",
    "    (dat_by_year['gross_demand_quantity'] > 100)\n",
    "    &\n",
    "    (dat_by_year['gross_demand_quantity'] < 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_by_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----- Plot 2017 & 2018 mid-range article means -----\n",
    "A = (dat_g_2017mids.\n",
    "     groupby('week').\n",
    "     mean().\n",
    "     drop('year', axis = 1).\n",
    "     rename(columns = {'gross_demand_quantity': '2017mean'})\n",
    "    )\n",
    "B = (dat_g_2018mids.\n",
    "     groupby('week').\n",
    "     mean().\n",
    "     drop('year', axis = 1).\n",
    "     rename(columns = {'gross_demand_quantity': '2018mean'})\n",
    "    )\n",
    "pd.merge(A, B, left_index=True, right_index=True).plot()\n",
    "plt.title('Shoes: 100 < gross_demand_quantity < 10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate MAPE of 2017/2018 articles against their own mean\n",
    "\n",
    "mids2017['gross_demand_quantity'].mean()\n",
    "mape(mids2017['gross_demand_quantity'], mids2017['gross_demand_quantity'].mean()) # 2017\n",
    "\n",
    "mape(mids2018['gross_demand_quantity'], mids2018['gross_demand_quantity'].mean()) # 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dfAndrasDistances.to_csv('dfAndrasDistances.csv')\n",
    "\n",
    "dfAndrasDistances = pd.read_csv('dfAndrasDistances.csv')\n",
    "dfAndrasDistances.drop(['Unnamed: 0', 'COSINE'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfAndrasDistances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Consider article X:\n",
    "    # (1) article X existed in 2017\n",
    "        # (a) 2018 prediction is mean line\n",
    "    \n",
    "    # (2) article X did not exist in 2017\n",
    "        # (a) Find n most similar articles of those that did exist in 2017\n",
    "        # (b) See how they relate to mean line\n",
    "        # (c) Take... average of those?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat.groupby(by = ['article_number', 'season']).agg('sum')[['gross_demand_quantity']] # aggregate by year\n",
    "dat.reset_index(inplace=True)\n",
    "\n",
    "dfAndrasDistances = pd.read_csv('dfAndrasDistances.csv')\n",
    "dfAndrasDistances.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "d = dat[dat['season'] == 'FW17'][['article_number', 'gross_demand_quantity']]\n",
    "kahuna = (\n",
    "    pd.merge(dfAndrasDistances, d, left_on = 'A1', right_on = 'article_number').\n",
    "    drop(['article_number', 'COSINE'], axis = 1).\n",
    "    rename(columns = {'gross_demand_quantity': 'A1_FW17'})\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "kahuna = (\n",
    "    pd.merge(kahuna, d, left_on = 'A2', right_on = 'article_number').\n",
    "    drop(['article_number'], axis = 1).\n",
    "    rename(columns = {'gross_demand_quantity': 'A2_FW17'})\n",
    "    )\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "d = dat[dat['season'] == 'FW18'][['article_number', 'gross_demand_quantity']]\n",
    "kahuna = (\n",
    "    pd.merge(kahuna, d, left_on = 'A1', right_on = 'article_number').\n",
    "    drop('article_number', axis = 1).\n",
    "    rename(columns = {'gross_demand_quantity': 'A1_FW18'})\n",
    "         )\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "neighbors = (\n",
    "    pd.DataFrame(kahuna.groupby('A1')['EUCLIDEAN'].nsmallest(10)).\n",
    "    reset_index()\n",
    "        )\n",
    "\n",
    "kahuna = (\n",
    "    pd.merge(kahuna, neighbors, left_on = ['A1', kahuna.index], right_on = ['A1', 'level_1']).\n",
    "    drop(['level_1', 'EUCLIDEAN_y'], axis = 1)\n",
    ")\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "preds = (\n",
    "    pd.DataFrame(kahuna.groupby('A1')['A2_FW17'].mean()).\n",
    "    reset_index().\n",
    "    rename(columns = {'A2_FW17': 'A1_FW18_pred'})\n",
    ")\n",
    "\n",
    "kahuna = pd.merge(kahuna, preds, left_on='A1', right_on='A1')\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "kahuna_skinny = kahuna[['A1', 'A1_FW17', 'A1_FW18', 'A1_FW18_pred']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Past Performance EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical raw MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "carryover = pd.read_csv('Stock left and of season.csv', low_memory=False, index_col = 0, \n",
    "                                          usecols=['season', 'article_number', 'ecom_available_stock']) \n",
    "\n",
    "carryover.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# dat0 = pd.read_csv('dat.csv', low_memory=False, index_col = 0) # Wall time: 1min 47s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "\n",
    "dat = dat[dat['gross_demand_quantity'] != 0] \n",
    "dat.drop(['sales_organization', 'country', 'brand', 'sold_qty', 'std_margin', \n",
    "          'return_qty', 'article_promotion_main_category_group', 'fw_or_ss'], \n",
    "         inplace=True, axis = 1)\n",
    "\n",
    "dat = dat.groupby(by = ['article_number', 'season']).agg('sum')[['net_qty']] # aggregate by year\n",
    "dat.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat = pd.merge(dat, carryover, left_on=['article_number', 'season'], right_on=['article_number', 'season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# multiple ecom_available_stock on accident; so find min\n",
    "dat1 = (\n",
    "    pd.DataFrame(dat.groupby(['article_number', 'season'])['ecom_available_stock'].min()).\n",
    "    reset_index())\n",
    "\n",
    "# leftover was 0, so understocked (maybe, unless sold leftovers)\n",
    "dat_understock = dat1[dat1['ecom_available_stock'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat2 = (pd.merge(dat1, dat, how = 'left').\n",
    "        drop_duplicates()\n",
    "       )\n",
    "dat2 = dat2[dat2['season'] != 'SS19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat2 = dat2[(dat2['net_qty'] != 0) & (dat2['ecom_available_stock'] != 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pct error = [forecast - actual]/actual = (net_qty + stock - net_qty)/net_qty\n",
    "dat2['percent_error'] = dat2['ecom_available_stock']/dat2['net_qty']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = [0, 100, 1000, 30000]\n",
    "dat2['bin'] = pd.cut(np.array(dat2['net_qty']), bins)\n",
    "\n",
    "# ---- Similarity-based prediction, by bin ----\n",
    "# kahuna_ref.groupby('bin')['Pct_Err'].describe()[['count', 'mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = dat2.groupby(['season', 'bin'])['percent_error'].describe()[['count', 'mean', '50%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Sort by season ---\n",
    "\n",
    "order = {'SS17': 0, 'FW17': 1, 'SS18': 2, 'FW18': 3, 'SS19': 4}\n",
    "d['order_id'] = [order[i] for i in d.reset_index()['season']]\n",
    "\n",
    "d.sort_values(by = ['order_id', 'bin'], inplace=True)\n",
    "d.drop('order_id', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d.round().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understock EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "carryover = pd.read_csv('Stock left and of season.csv', low_memory=False, index_col = 0, \n",
    "                                          usecols=['season', 'article_number', 'ecom_available_stock']) \n",
    "\n",
    "carryover.reset_index(inplace = True)\n",
    "\n",
    "# Remove pseudo-duplicates: multiple ecom_available_stock on accident, so find min\n",
    "carryover = (\n",
    "    pd.DataFrame(carryover.groupby(['article_number', 'season'])['ecom_available_stock'].min()).\n",
    "    reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "dat0 = pd.read_csv('dat.csv', low_memory=False, index_col = 0) # Wall time: 1min 47s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# aggregate to article-seasonal net_qty\n",
    "dat = dat.groupby(by = ['article_number', 'season']).agg('sum')[['net_qty']] # aggregate by year\n",
    "dat.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add beginning of season stock information\n",
    "dat = pd.merge(dat, carryover, left_on=['article_number', 'season'], right_on=['article_number', 'season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# leftover was 0, so understocked (maybe, unless sold leftovers)\n",
    "dat_understock = dat[dat['ecom_available_stock'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove current season\n",
    "dat_understock = dat_understock[dat_understock['season'] != 'SS19']\n",
    "\n",
    "# remove small potatoes items\n",
    "dat_understock = dat_understock[dat_understock['net_qty'] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = [0, 100, 200, 500, 1000, 2000, 5000, 10000, 30000]\n",
    "dat_understock['bin'] = pd.cut(np.array(dat_understock['net_qty']), bins)\n",
    "\n",
    "d = pd.DataFrame(dat_understock.groupby(['season', 'bin'])['article_number'].describe()['count'])\n",
    "\n",
    "# --- Sort by season ---\n",
    "\n",
    "order = {'SS17': 0, 'FW17': 1, 'SS18': 2, 'FW18': 3, 'SS19': 4}\n",
    "d['order_id'] = [order[i] for i in d.reset_index()['season']]\n",
    "\n",
    "d.sort_values(by = ['order_id', 'bin'], inplace=True)\n",
    "d.drop('order_id', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_understock[dat_understock['season'] == 'SS17']['net_qty'].hist(bins = 50)\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = [12,6]\n",
    "# dat_understock[dat_understock['net_qty'] <2500].groupby('season')['net_qty'].hist(bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_number</th>\n",
       "      <th>season</th>\n",
       "      <th>net_qty</th>\n",
       "      <th>ecom_available_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001354</td>\n",
       "      <td>FW17</td>\n",
       "      <td>1502</td>\n",
       "      <td>4728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001354</td>\n",
       "      <td>FW18</td>\n",
       "      <td>1805</td>\n",
       "      <td>2499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001354</td>\n",
       "      <td>SS17</td>\n",
       "      <td>276</td>\n",
       "      <td>5498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001354</td>\n",
       "      <td>SS18</td>\n",
       "      <td>1254</td>\n",
       "      <td>2932.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001354</td>\n",
       "      <td>SS19</td>\n",
       "      <td>429</td>\n",
       "      <td>4558.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_number season  net_qty  ecom_available_stock\n",
       "0         001354   FW17     1502                4728.0\n",
       "1         001354   FW18     1805                2499.0\n",
       "2         001354   SS17      276                5498.0\n",
       "3         001354   SS18     1254                2932.0\n",
       "4         001354   SS19      429                4558.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_number</th>\n",
       "      <th>season</th>\n",
       "      <th>net_qty</th>\n",
       "      <th>ecom_available_stock</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>033620</td>\n",
       "      <td>SS18</td>\n",
       "      <td>326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(200, 500]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>087609</td>\n",
       "      <td>FW17</td>\n",
       "      <td>547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(500, 1000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>087609</td>\n",
       "      <td>SS18</td>\n",
       "      <td>243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(200, 500]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>288193</td>\n",
       "      <td>FW18</td>\n",
       "      <td>6883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5000, 10000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>288193</td>\n",
       "      <td>SS18</td>\n",
       "      <td>2540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(2000, 5000]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_number season  net_qty  ecom_available_stock            bin\n",
       "136         033620   SS18      326                   0.0     (200, 500]\n",
       "211         087609   FW17      547                   0.0    (500, 1000]\n",
       "213         087609   SS18      243                   0.0     (200, 500]\n",
       "240         288193   FW18     6883                   0.0  (5000, 10000]\n",
       "242         288193   SS18     2540                   0.0   (2000, 5000]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <th>bin</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">SS17</th>\n",
       "      <th>(100, 200]</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(200, 500]</th>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(500, 1000]</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1000, 2000]</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2000, 5000]</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">FW17</th>\n",
       "      <th>(100, 200]</th>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(200, 500]</th>\n",
       "      <td>2092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(500, 1000]</th>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1000, 2000]</th>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2000, 5000]</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5000, 10000]</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10000, 30000]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">SS18</th>\n",
       "      <th>(100, 200]</th>\n",
       "      <td>1271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(200, 500]</th>\n",
       "      <td>1361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(500, 1000]</th>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1000, 2000]</th>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2000, 5000]</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5000, 10000]</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10000, 30000]</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">FW18</th>\n",
       "      <th>(100, 200]</th>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(200, 500]</th>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(500, 1000]</th>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1000, 2000]</th>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2000, 5000]</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5000, 10000]</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10000, 30000]</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count\n",
       "season bin                 \n",
       "SS17   (100, 200]        91\n",
       "       (200, 500]       118\n",
       "       (500, 1000]       31\n",
       "       (1000, 2000]       7\n",
       "       (2000, 5000]       8\n",
       "FW17   (100, 200]      1977\n",
       "       (200, 500]      2092\n",
       "       (500, 1000]      787\n",
       "       (1000, 2000]     281\n",
       "       (2000, 5000]      63\n",
       "       (5000, 10000]      5\n",
       "       (10000, 30000]     2\n",
       "SS18   (100, 200]      1271\n",
       "       (200, 500]      1361\n",
       "       (500, 1000]      592\n",
       "       (1000, 2000]     235\n",
       "       (2000, 5000]      69\n",
       "       (5000, 10000]      5\n",
       "       (10000, 30000]     2\n",
       "FW18   (100, 200]      1500\n",
       "       (200, 500]      1603\n",
       "       (500, 1000]      742\n",
       "       (1000, 2000]     322\n",
       "       (2000, 5000]     122\n",
       "       (5000, 10000]     10\n",
       "       (10000, 30000]     1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()\n",
    "dat_understock.head()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
