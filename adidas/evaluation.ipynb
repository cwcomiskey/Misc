{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps:\n",
    "\n",
    "1. Calculate 'true' SS19 net demand quantity.\n",
    "2. Forecast SS19 true net demand with pre-SS19 data, using the methodologies of our previous delivery.\n",
    "3. Forecast SS19 true net demand with pre-SS19 data, using the improved/current methodologies.\n",
    "4. Evaluate the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 5000)\n",
    "\n",
    "import scipy\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.tsatools import detrend\n",
    "\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat0 = pd.read_csv('data/ch4k_df_eu.csv')\n",
    "ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, error_bad_lines = False, \n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "                                  'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()\n",
    "dat = dat.dropna()\n",
    "dat = dat[(dat.season_net_qty > 100)]\n",
    "dat = dat.sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = set(dat[dat.season == 'SS18'].article_number)\n",
    "b = set(dat[dat.season == 'SS19'].article_number)\n",
    "\n",
    "carryovers = a.intersection(b)\n",
    "\n",
    "\n",
    "len(carryovers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat[['article_number', 'year', 'week', 'country', 'season', 'net_qty', 'clearance', 'season_net_qty', 'buy_availability']]\n",
    "\n",
    "dat = pd.merge(\n",
    "    pd.DataFrame(dat.groupby(['article_number', 'season', 'year', 'week'])['net_qty'].sum().reset_index()),\n",
    "    pd.DataFrame(dat.groupby(['article_number', 'season', 'year', 'week'])['buy_availability', 'clearance', 'season_net_qty'].mean().round(2).reset_index())\n",
    ")\n",
    "\n",
    "# For sorting\n",
    "# key = {'FW16': 1, 'SS17': 2, 'FW17': 3, 'SS18': 4, 'FW18': 5, 'SS19': 6, 'FW19': 7, 'SS20': 8}\n",
    "# dat['order_key'] = [key[s] for s in dat.season]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat['adj_net_qty'] = np.where(dat.net_qty * (1 - dat.clearance) > 0, \n",
    "                              dat.net_qty * (1 - dat.clearance),\n",
    "                              0)\n",
    "\n",
    "dat= pd.merge(\n",
    "    dat,\n",
    "    pd.DataFrame(dat[dat.buy_availability > 0.35].\n",
    "             groupby(['article_number', 'season'])['adj_net_qty'].\n",
    "             mean().\n",
    "             round(1)*26\n",
    "            ).rename(columns = {'adj_net_qty': 'adj_seas_net_qty'}).reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "dat['adj_seas_net_qty'] = [max(x, y) for x, y in zip(dat['season_net_qty'], dat['adj_seas_net_qty'])] # max of original and adjustment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat[['net_qty', 'adj_net_qty']].describe()\n",
    "     \n",
    "# dat[['season_net_qty', 'adj_seas_net_qty']].drop_duplicates().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_delivery = dat[dat.season == 'SS18'][['article_number', 'season', 'adj_seas_net_qty']].drop_duplicates()\n",
    "#pd.crosstab(index = dat[['season', 'article_number']].drop_duplicates().season, columns = 'count')\n",
    "\n",
    "dat_delivery = dat_delivery[dat_delivery.article_number.isin(carryovers)]\n",
    "dat_delivery['DAA_0'] = dat_delivery.adj_seas_net_qty*1.1\n",
    "dat_delivery.shape\n",
    "\n",
    "mothership = dat_delivery[['article_number', 'DAA_0']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mothership.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buyer_table = pd.read_csv('data/Buyers Predictions.csv', low_memory = False, error_bad_lines = False, sep = \",\")\n",
    "buyer_table = buyer_table[(buyer_table.season == 'SS19')]\n",
    "\n",
    "buyer_table = buyer_table[['article', 'season', 'ecom_marketing_forecast']].rename(columns = {'article': 'article_number'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mothership = pd.merge(\n",
    "    mothership,\n",
    "    buyer_table[['article_number', 'ecom_marketing_forecast']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAS w/ buy_availability (only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(par = np.array([0.5, 0.9, 0, 1, 0])):\n",
    "    # np.random.seed(3)\n",
    "    parameters = {}\n",
    "\n",
    "    parameters['alpha'] = par[0]\n",
    "    parameters['beta'] = par[1]\n",
    "    parameters['omega'] = par[2]* (1-par[1])    # one way to choose that is omega/(1-beta) = unconditional mean \n",
    "    parameters['sigma'] = par[3]\n",
    "    parameters['f0'] = par[4]                   # one way to choose is unconditional mean \n",
    "\n",
    "    return parameters\n",
    "\n",
    "def loglik(y, f, x, sigma):\n",
    "    ll = -1/2*np.log(2*np.pi ) - 1/2*np.log(sigma) - 1/(2*sigma)*(y - x*f)**2 \n",
    "    return ll\n",
    "\n",
    "\n",
    "def score_compute(y, f, x, parameters, epsilon = 1e-7 ):\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    \n",
    "    score = (y - x*f)/sigma\n",
    "    # score = (y - x*f)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def filterGAS(y, x, parameters):\n",
    "    \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    score0 = score_compute(y[0,:],  f0, x[0,:], parameters, epsilon = 1e-7) \n",
    "    f = np.zeros((len(y),1))\n",
    "    \n",
    "    f[0,:] = f0\n",
    "    for t in range(1,len(y)):\n",
    "        scoret = score_compute(y[t-1,:], f[t-1,:], x[t-1,:], parameters, epsilon = 1e-7) \n",
    "        f[t,:] = omega + alpha*scoret + beta*f[t-1,:] \n",
    "\n",
    "    return f\n",
    "\n",
    "def loglikest(par, y, x):\n",
    "    parameters = initialize_parameters(par)\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    # f0 = parameters[\"f0\"]\n",
    "    \n",
    "    f = filterGAS(y, x, parameters) \n",
    "    ll = np.zeros((len(y), 1))\n",
    "    m = len(y)\n",
    "\n",
    "    for t in range(0, len(y)):\n",
    "         ll[t,:] = loglik(y[t,:], f[t,:], x[t,:], sigma)\n",
    "    loglik_res = -(np.sum(ll))/m\n",
    "        \n",
    "#     else:\n",
    "#         loglik_res=10**9 # causing gradient problems??\n",
    "\n",
    "    return loglik_res\n",
    "\n",
    "def score_compute_2(y, f, x, parameters, epsilon = 1e-7 ):\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    \n",
    "    score = (y - x*f) # ** The 'type = 2' modification **\n",
    "    \n",
    "    return score\n",
    "\n",
    "def filterGAS_2(y, x, parameters):\n",
    "    \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    score0 = score_compute_2(y[0,:],  f0, x[0,:], parameters, epsilon = 1e-7) \n",
    "    f = np.zeros((len(y),1))\n",
    "    \n",
    "    f[0,:] = f0\n",
    "    for t in range(1,len(y)):\n",
    "        scoret = score_compute_2(y[t-1,:], f[t-1,:], x[t-1,:], parameters, epsilon = 1e-7) \n",
    "        f[t,:] = omega + alpha*scoret + beta*f[t-1,:] \n",
    "\n",
    "    return f\n",
    "\n",
    "def loglikest_2(par, y, x):\n",
    "    parameters = initialize_parameters(par)\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    # f0 = parameters[\"f0\"]\n",
    "    \n",
    "    f = filterGAS_2(y, x, parameters) \n",
    "    ll = np.zeros((len(y), 1))\n",
    "    m = len(y)\n",
    "\n",
    "    for t in range(0, len(y)):\n",
    "         ll[t,:] = loglik(y[t,:], f[t,:], x[t,:], sigma)\n",
    "    loglik_res = -(np.sum(ll))/m\n",
    "        \n",
    "#     else:\n",
    "#         loglik_res=10**9 # causing gradient problems??\n",
    "\n",
    "    return loglik_res\n",
    "\n",
    "def GAS_est(df):\n",
    "    \n",
    "    y = df.net_qty.values          # observed demand (response)\n",
    "    x = df.buy_availability.values # buy_availability (explanatory)\n",
    "\n",
    "    y = y.reshape((len(y),1)) \n",
    "    x = x.reshape((len(y),1))\n",
    "    \n",
    "    ret = pd.DataFrame()\n",
    "    ret['year'] = df['year']\n",
    "    ret['week'] = df['week']\n",
    "        \n",
    "    abc = scipy.optimize.minimize(\n",
    "        loglikest,                                       # function to minimize (log likelihood y|x,theta)\n",
    "        np.array([0.8, 0.9, np.mean(y), 1, np.mean(y)]), # initial parameter values (starting)\n",
    "        args=(y, x), \n",
    "        options ={'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12},\n",
    "        method='L-BFGS-B', \n",
    "        bounds=((0,  None),             # alpha\n",
    "                (-1, 1),                # beta\n",
    "                (0.001, None),  # omega \n",
    "                (0.001, None),          # sigma\n",
    "                (0.001, None)   # f\n",
    "               )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # --- CONVERGENCE control flow ---\n",
    "    if abc.success == True:\n",
    "        \n",
    "        x1par = initialize_parameters(abc.x) \n",
    "        GAS = filterGAS(y, x, x1par)\n",
    "        \n",
    "        ret['GAS_est'] = GAS\n",
    "        ret['Convergence'] = [abc.success] * len(y)\n",
    "        ret['Convg type'] = ['One'] * len(y)\n",
    "        \n",
    "    # **Modification if first algorithm fails\n",
    "    elif abc.success == False:\n",
    "        \n",
    "        print('Convergence failure notification')\n",
    "        \n",
    "        abc = scipy.optimize.minimize(\n",
    "            loglikest_2,                                       # function to minimize (log likelihood y|x,theta)\n",
    "            np.array([0.8, 0.9, np.mean(y), 1, np.mean(y)]),   # initial parameter values (starting)\n",
    "            args=(y, x), \n",
    "            options ={'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12},\n",
    "            method='L-BFGS-B', \n",
    "            bounds=((0,  None),             # alpha\n",
    "                    (-1, 1),                # beta\n",
    "                    (0.001, None),  # omega \n",
    "                    (0.001, None),          # sigma\n",
    "                    (0.001, None)   # f\n",
    "                   )\n",
    "                )\n",
    "        \n",
    "        if abc.success == False:\n",
    "            print('Unresolved convergence failure')\n",
    "\n",
    "        x1par = initialize_parameters(abc.x) \n",
    "        GAS = filterGAS_2(y, x, x1par)\n",
    "        \n",
    "        ret['GAS_est'] = GAS\n",
    "        ret['Convergence'] = [abc.success] * len(y)\n",
    "        ret['Convg type'] = ['Two'] * len(y)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat0 = pd.read_csv('data/ch4k_df_eu.csv')\n",
    "\n",
    "# ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, # index_col = 0, \n",
    "#                        error_bad_lines = False,\n",
    "#                        usecols = ['article_no', 'model_no', 'art_desc', \n",
    "#                                   'key_cat_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "#                                   'franchise', 'franchise_family',\n",
    "#                                   'prod_grp_desc', 'prod_type_desc']                      \n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()\n",
    "\n",
    "dat = dat.dropna()\n",
    "dat = dat[(dat.season_net_qty > 100) & (dat.season_net_qty != 0) & (dat.season == 'SS18')]\n",
    "\n",
    "# For constructing seasonality reference by product_type\n",
    "# ref_dat = ref_dat0.copy()\n",
    "# ref_dat = ref_dat.drop_duplicates()[['article_no', 'prod_type_desc', 'model_no', 'art_desc', 'key_cat_desc', 'sports_cat_desc', 'rmh_cat_desc', 'franchise', 'franchise_family', 'prod_grp_desc']]\n",
    "# dat = pd.merge(dat, ref_dat, left_on='article_number', right_on='article_no', how = 'left')\n",
    "\n",
    "dat = dat[dat.article_number.isin(carryovers)] \n",
    "dat = dat[['article_number', 'year', 'week', 'country', 'season', 'net_qty', 'clearance', 'season_net_qty', 'buy_availability']]\n",
    "dat = dat.sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat['adj_net_qty'] = np.where(dat.net_qty * (1 - dat.clearance) > 0, \n",
    "#                               dat.net_qty * (1 - dat.clearance),\n",
    "#                               0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "dat_GAS = dat[dat.article_number.isin(a)].groupby(['article_number', 'country']).apply(GAS_est)\n",
    "dat_GAS = dat_GAS.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mothership = pd.merge(\n",
    "    mothership,\n",
    "    pd.DataFrame(dat_GAS.groupby(['article_number'])['GAS_est'].apply(sum).round(2)).reset_index()\n",
    ")\n",
    "mothership['GAS_est'] = mothership.GAS_est*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mothership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat = pd.merge(\n",
    "#     dat,\n",
    "#     dat_GAS[['article_number', 'country', 'year', 'week', 'GAS_est']]\n",
    "# )\n",
    "# dat = dat.sort_values(['article_number', 'country', 'year', 'week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot -- EDA\n",
    "\n",
    "# a = np.random.choice(list(carryovers), size = 1, replace = False)[0]\n",
    "\n",
    "# dat_a = dat[(dat.article_number == a) & (dat.country == 'EU')]\n",
    "# dat_a.head()\n",
    "# dat_a.shape\n",
    "\n",
    "# plt.rcParams['font.size'] = 11\n",
    "# plt.rcParams['legend.fontsize'] = 'medium'\n",
    "# plt.rcParams['figure.titlesize'] = 'medium'\n",
    "# plt.rcParams[\"figure.figsize\"] = [18,12]\n",
    "\n",
    "# plt.subplot(3,1,1)\n",
    "# plt.plot(dat_a.week.round(0).astype(str), dat_a[['net_qty', 'GAS_est']], linewidth = 2.5)\n",
    "\n",
    "# plt.subplot(3,1,2)\n",
    "# plt.ylim(0,1)\n",
    "# plt.plot(dat_a.week.round(0).astype(str), dat_a[['buy_availability']], linewidth = 2.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat0 = pd.read_csv('data/ch4k_df_eu.csv')\n",
    "# ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, error_bad_lines = False, \n",
    "#                        usecols = ['article_no', 'model_no', 'art_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "#                                   'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat0.copy()\n",
    "ref_dat = ref_dat0.copy()\n",
    "\n",
    "dat = dat.dropna()\n",
    "dat = dat[(dat.season_net_qty > 100) & (dat.season.isin(['SS17', 'SS18']))]\n",
    "\n",
    "# For constructing seasonality reference by product_type\n",
    "ref_dat = ref_dat[['article_no', 'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].drop_duplicates() \n",
    "dat = pd.merge(dat, ref_dat, left_on='article_number', right_on='article_no', how = 'left')\n",
    "dat = dat.sort_values(['article_number', 'year', 'week'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- All-seasons cat-level means ----\n",
    "seasonality_dat = dat.copy()\n",
    "seasonality_dat.shape\n",
    "\n",
    "seasonality_dat = pd.merge(\n",
    "    pd.DataFrame(seasonality_dat.groupby(['article_number', 'season', 'year', 'week'])['net_qty'].sum()).reset_index(),\n",
    "    seasonality_dat[['article_number', 'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].drop_duplicates()\n",
    ").dropna()\n",
    "\n",
    "\n",
    "# seasonality_dat[seasonality_dat[['article_number', 'season', 'year', 'week', 'net_qty']].duplicated()].article_number.unique()\n",
    "\n",
    "# seasonality_dat[seasonality_dat.isna().any(axis=1)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality_sport   = pd.DataFrame(seasonality_dat.groupby(['sports_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'sport_weekly_mean'})\n",
    "seasonality_rmh     = pd.DataFrame(seasonality_dat.groupby(['rmh_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'rmh_weekly_mean'})\n",
    "seasonality_gndr    = pd.DataFrame(seasonality_dat.groupby(['gender_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'gender_weekly_mean'})\n",
    "seasonality_agegrp  = pd.DataFrame(seasonality_dat.groupby(['age_group_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'age_weekly_mean'})\n",
    "seasonality_frnchse = pd.DataFrame(seasonality_dat.groupby(['franchise', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'franchise_weekly_mean'})\n",
    "seasonality_prdgrp  = pd.DataFrame(seasonality_dat.groupby(['prod_grp_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'prd_grp_weekly_mean'})\n",
    "\n",
    "seasonality_dfs = [seasonality_sport, seasonality_rmh, seasonality_gndr, seasonality_agegrp, seasonality_frnchse, seasonality_prdgrp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SS18 seasonality (cat-level means) ----\n",
    "seasonality_dat_SS18 = seasonality_dat[seasonality_dat.season == 'SS18']\n",
    "\n",
    "seasonality_sport_SS18   = pd.DataFrame(seasonality_dat_SS18.groupby(['sports_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'sport_weekly_mean_SS18'})\n",
    "seasonality_rmh_SS18     = pd.DataFrame(seasonality_dat_SS18.groupby(['rmh_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'rmh_weekly_mean_SS18'})\n",
    "seasonality_gndr_SS18    = pd.DataFrame(seasonality_dat_SS18.groupby(['gender_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'gender_weekly_mean_SS18'})\n",
    "seasonality_agegrp_SS18  = pd.DataFrame(seasonality_dat_SS18.groupby(['age_group_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'age_weekly_mean_SS18'})\n",
    "seasonality_frnchse_SS18 = pd.DataFrame(seasonality_dat_SS18.groupby(['franchise', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'franchise_weekly_mean_SS18'})\n",
    "seasonality_prdgrp_SS18 = pd.DataFrame(seasonality_dat_SS18.groupby(['prod_grp_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'prd_grp_weekly_mean_SS18'})\n",
    "\n",
    "seasonality_dfs_SS18 = [seasonality_sport_SS18, seasonality_rmh_SS18, seasonality_gndr_SS18, seasonality_agegrp_SS18, seasonality_frnchse_SS18, seasonality_prdgrp_SS18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SS18-SS19 carryovers: SS18 data ----\n",
    "carryovers_SS18 = seasonality_dat[(seasonality_dat.season == 'SS18') & \n",
    "                                  (seasonality_dat.article_number.isin(carryovers))\n",
    "                                 ].rename(columns = {'net_qty': 'article_net_qty'})\n",
    "\n",
    "carryovers_SS18 = carryovers_SS18.sort_values(['article_number', 'season', 'year', 'week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all-season mean column to SS18 df --- to calculate SS19 deseas\n",
    "\n",
    "net_qty_cols = ['sport_weekly_mean', 'rmh_weekly_mean', 'gender_weekly_mean', 'age_weekly_mean', 'franchise_weekly_mean', 'prd_grp_weekly_mean'] # seasonality_df[<net_qty_by_another_name>] \n",
    "abbrevs = ['sp', 'rmh', 'gndr', 'age', 'frnchse', 'prd_grp']\n",
    "\n",
    "for i in range(6):\n",
    "    seasonality_dfs_SS18[i] = pd.DataFrame(pd.merge(seasonality_dfs[i], seasonality_dfs_SS18[i]))\n",
    "    seasonality_dfs_SS18[i][abbrevs[i] + '_deseas'] = seasonality_dfs_SS18[i].loc[:, net_qty_cols[i] + '_SS18'] - seasonality_dfs_SS18[i].loc[:, net_qty_cols[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for regressing article net_qty on seasonalities\n",
    "\n",
    "def regress(df):\n",
    "    # for article a's level of each category, retreive weekly means, then regress \n",
    "\n",
    "#     print(df.columns)\n",
    "#     print(df.iloc[:5,:5])\n",
    "    \n",
    "    ret = pd.DataFrame()\n",
    "    ret['year'] = df['year']\n",
    "    ret['week'] = df['week']\n",
    "    \n",
    "    # article net_demand_qty\n",
    "    y = df[['article_net_qty', 'week']].set_index('week')\n",
    "\n",
    "    # article category-level combination weekly means\n",
    "    # set_index() for joining\n",
    "    x_sport   = seasonality_sport[seasonality_sport.sports_cat_desc == df.sports_cat_desc.unique()[0]].set_index('week')\n",
    "    x_rmh     = seasonality_rmh[seasonality_rmh.rmh_cat_desc == df.rmh_cat_desc.unique()[0]].set_index('week')\n",
    "    x_gndr    = seasonality_gndr[seasonality_gndr.gender_desc == df.gender_desc.unique()[0]].set_index('week')\n",
    "    x_agegrp  = seasonality_agegrp[seasonality_agegrp.age_group_desc == df.age_group_desc.unique()[0]].set_index('week')\n",
    "    x_frnchse = seasonality_frnchse[seasonality_frnchse.franchise == df.franchise.unique()[0]].set_index('week')\n",
    "    x_prdgrp  = seasonality_prdgrp[seasonality_prdgrp.prod_grp_desc == df.prod_grp_desc.unique()[0]].set_index('week')\n",
    "\n",
    "    # design matrix (ensure 'week' alignment)\n",
    "    yX = (pd.merge(y, x_sport, left_index=True, right_index=True).\n",
    "          merge(x_rmh, left_index=True, right_index=True).\n",
    "          merge(x_gndr, left_index=True, right_index=True).\n",
    "          merge(x_agegrp, left_index=True, right_index=True).\n",
    "          merge(x_frnchse, left_index=True, right_index=True).\n",
    "          merge(x_prdgrp, left_index=True, right_index=True).\n",
    "          drop(['sports_cat_desc', 'rmh_cat_desc', 'gender_desc',\n",
    "               'age_group_desc', 'franchise', 'prod_grp_desc'], axis = 1))\n",
    "\n",
    "    # predict article 'a' net_demand_qty with 5 article 'a' category-level seasonalities\n",
    "    # print(df.article_number.unique())\n",
    "    \n",
    "    mod = sm.OLS(yX.article_net_qty, yX.drop('article_net_qty', axis = 1), missing='drop').fit()\n",
    "#     print(df.article_number.unique())\n",
    "#     print(round(mod.rsquared, 2))\n",
    "#     print()\n",
    "    \n",
    "    ret['seas_preds'] = mod.predict()\n",
    "    ret['deseas_net_qty'] = df['article_net_qty'] - ret['seas_preds']\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "carryovers_SS18.shape\n",
    "\n",
    "# Component seasonality predictions\n",
    "carryovers_SS18 = pd.merge(\n",
    "    carryovers_SS18,\n",
    "    carryovers_SS18.groupby(['article_number']).apply(regress).reset_index().drop('level_1', axis = 1)\n",
    ")\n",
    "carryovers_SS18.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_0(par = np.array([0.5, 0.9, 0, 1, 0])):\n",
    "    parameters = {}\n",
    "\n",
    "    parameters['alpha'] = par[0]\n",
    "    parameters['beta'] = par[1]\n",
    "    parameters['omega'] = par[2]* (1-par[1])\n",
    "    parameters['sigma'] = par[3]\n",
    "    parameters['f0'] = par[4]                   \n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def loglik_0(y, f, sigma):\n",
    "    ll = -1/2*np.log(2*np.pi ) - 1/2*np.log(sigma) - 1/(2*sigma)*(y - f)**2 \n",
    "    return ll\n",
    "\n",
    "\n",
    "def score_compute_0(y, f, parameters, epsilon = 1e-7 ):\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    \n",
    "    score = (y - f)/sigma\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def filterGAS_0(y, parameters):\n",
    "    \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    \n",
    "    score0 = score_compute_0(y[0,:],  f0, parameters, epsilon = 1e-7) \n",
    "    \n",
    "    f = np.zeros((len(y),1))\n",
    "    f[0,:] = f0\n",
    "    \n",
    "    for t in range(1,len(y)):\n",
    "        scoret = score_compute_0(y[t-1,:], f[t-1,:], parameters, epsilon = 1e-7) \n",
    "        f[t,:] = omega + alpha*scoret + beta*f[t-1,:] \n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def loglikest_0(par, y):\n",
    "    \n",
    "    parameters = initialize_parameters_0(par)\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    \n",
    "    f = filterGAS_0(y, parameters) \n",
    "    ll = np.zeros((len(y), 1))\n",
    "    m = len(y)\n",
    "\n",
    "    for t in range(0, len(y)):\n",
    "         ll[t,:] = loglik_0(y[t,:], f[t,:], sigma)\n",
    "            \n",
    "    loglik_res = -(np.sum(ll))/m\n",
    "\n",
    "    return loglik_res\n",
    "\n",
    "\n",
    "def GAS_est_0(df, col):\n",
    "    \n",
    "    y = df.loc[:, col].values \n",
    "    y = y.reshape((len(y),1)) \n",
    "    \n",
    "    ret = pd.DataFrame()\n",
    "    ret['week'] = df['week']\n",
    "        \n",
    "    abc = scipy.optimize.minimize(\n",
    "        loglikest_0,                                       \n",
    "        np.array([0.8, 0.9, np.mean(y), 1, np.mean(y)]), \n",
    "        args = y, \n",
    "        options = {'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12},\n",
    "        method ='L-BFGS-B', \n",
    "        bounds =((0,  None),    # alpha\n",
    "                (-1, 1),        # beta\n",
    "                (0.001, None),  # omega \n",
    "                (0.001, None),  # sigma\n",
    "                (0.001, None)   # f\n",
    "               )\n",
    "    )\n",
    "    \n",
    "    if abc.success == False:\n",
    "        print('C1 failed for article', abc.message)\n",
    "\n",
    "    x1par = initialize_parameters_0(abc.x) \n",
    "    GAS = filterGAS_0(y, x1par)\n",
    "    ret['GAS_est'] = GAS\n",
    "    \n",
    "    return ret     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "categories =  ['sports_cat_desc', 'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise',      'prod_grp_desc']\n",
    "deseas_cols = ['sp_deseas',       'rmh_deseas',   'gndr_deseas', 'age_deseas',     'frnchse_deseas', 'prd_grp_deseas'] # seasonality_df[<net_qty_by_another_name>] \n",
    "abbrevs =     ['sp',              'rmh',          'gndr',        'agegrp',         'frnchse',        'prdgrp']\n",
    "\n",
    "for i in range(6):\n",
    "    seasonality_dfs_SS18[i] = pd.DataFrame(pd.merge(\n",
    "        seasonality_dfs_SS18[i], \n",
    "        seasonality_dfs_SS18[i].groupby(categories[i]).apply(GAS_est_0, col = deseas_cols[i]).reset_index().drop('level_1', axis = 1)\n",
    "    )).rename(columns = {'GAS_est': 'GAS_est_' + abbrevs[i] + '_deseas'}) # distinct names for GAS_est column\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_mv(par = np.array([0.5, 0.9, 0, 1, 0, 0, 0, 0, 0, 0, 0])):\n",
    "    parameters = {}\n",
    "\n",
    "    parameters['alpha'] = par[0]\n",
    "    parameters['beta'] = par[1]\n",
    "    parameters['omega'] = par[2]* (1-par[1])\n",
    "    parameters['sigma'] = par[3]\n",
    "    parameters['f0'] = par[4]\n",
    "    \n",
    "    # l for lambda\n",
    "    parameters['l_sport'] = par[5]\n",
    "    parameters['l_rmh'] = par[6]    \n",
    "    parameters['l_gender'] = par[7]\n",
    "    parameters['l_age'] = par[8]\n",
    "    parameters['l_franchise'] = par[9]\n",
    "    parameters['l_prod_grp'] = par[10]\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def loglik_mv(y, f, \n",
    "              f_sp, f_rmh, f_gndr, f_age, f_fr, f_pr, \n",
    "              l_sport, l_rmh, l_gender, l_age, l_franchise, l_prod_grp, \n",
    "              sigma):\n",
    "    \n",
    "    ll = -1/2*np.log(2*np.pi ) - 1/2*np.log(sigma) - 1/(2*sigma)*(y - (f + l_sport*f_sp + l_rmh*f_rmh + l_gender*f_gndr + l_age*f_age + l_franchise*f_fr + l_prod_grp*f_pr))**2 \n",
    "    \n",
    "    return ll\n",
    "\n",
    "\n",
    "def score_compute_mv(y, f, \n",
    "                     f_sp, f_rmh, f_gndr, f_age, f_fr, f_pr, \n",
    "                     parameters, epsilon = 1e-7):\n",
    "          \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "          \n",
    "    l_sport = parameters['l_sport'] \n",
    "    l_rmh = parameters['l_rmh'] \n",
    "    l_gender = parameters['l_gender'] \n",
    "    l_age = parameters['l_age'] \n",
    "    l_franchise = parameters['l_franchise'] \n",
    "    l_prod_grp = parameters['l_prod_grp']\n",
    "    \n",
    "    score = (y - (f + l_sport*f_sp + l_rmh*f_rmh + l_gender*f_gndr + l_age*f_age + l_franchise*f_fr + l_prod_grp*f_pr))/sigma\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def filterGAS_mv(y, f_sp, f_rmh, f_gndr, f_age, f_fr, f_pr, parameters):\n",
    "    \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "          \n",
    "    l_sport = parameters['l_sport'] \n",
    "    l_rmh = parameters['l_rmh'] \n",
    "    l_gender = parameters['l_gender'] \n",
    "    l_age = parameters['l_age'] \n",
    "    l_franchise = parameters['l_franchise'] \n",
    "    l_prod_grp = parameters['l_prod_grp']\n",
    "    \n",
    "    # print('filterGAS_mv:', y.shape)\n",
    "    \n",
    "    score0 = score_compute_mv(y[0,:], f0, f_sp[0,:], f_rmh[0,:], f_gndr[0,:], f_age[0,:], f_fr[0,:], f_pr[0,:], parameters, epsilon = 1e-7) \n",
    "    \n",
    "    f = np.zeros((len(y), 1))\n",
    "    f[0,:] = f0\n",
    "    \n",
    "    for t in range(1,len(y)):\n",
    "        scoret = score_compute_mv(y[t-1,:], f[t-1,:], f_sp[t-1,:], f_rmh[t-1,:], f_gndr[t-1,:], f_age[t-1,:], f_fr[t-1,:], f_pr[t-1,:], parameters, epsilon = 1e-7) \n",
    "        f[t,:] = omega + alpha*scoret + beta*f[t-1,:] \n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def loglikest_mv(parameters, y, f_sp, f_rmh, f_gndr, f_age, f_fr, f_pr):\n",
    "    \n",
    "    parameters = initialize_parameters_mv(parameters)\n",
    "          \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    sigma = parameters[\"sigma\"]\n",
    "          \n",
    "    l_sport = parameters['l_sport'] \n",
    "    l_rmh = parameters['l_rmh'] \n",
    "    l_gender = parameters['l_gender'] \n",
    "    l_age = parameters['l_age'] \n",
    "    l_franchise = parameters['l_franchise'] \n",
    "    l_prod_grp = parameters['l_prod_grp']\n",
    "          \n",
    "    ll = np.zeros((len(y), 1))\n",
    "    m = len(y)\n",
    "    \n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(len(y), 1)\n",
    "        \n",
    "    f = filterGAS_mv(y, f_sp, f_rmh, f_gndr, f_age, f_fr, f_pr, parameters) \n",
    "          \n",
    "    for t in range(0, len(y)):\n",
    "         ll[t,:] = loglik_mv(y[t,:], f[t,:], \n",
    "                             f_sp[t,:], f_rmh[t,:], f_gndr[t,:], f_age[t,:], f_fr[t,:], f_pr[t,:], \n",
    "                             l_sport, l_rmh, l_gender, l_age, l_franchise, l_prod_grp, \n",
    "                             sigma)\n",
    "            \n",
    "    loglik_res = -(np.sum(ll))/m\n",
    "\n",
    "    return loglik_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- create df for next GAS-ing ----\n",
    "for i in range(6):\n",
    "    if i == 0:\n",
    "        dat_factors = pd.merge(carryovers_SS18, seasonality_dfs_SS18[i], how = 'left')\n",
    "    else:\n",
    "        dat_factors = pd.merge(dat_factors, seasonality_dfs_SS18[i], how = 'left')\n",
    "    \n",
    "    \n",
    "\n",
    "# dat_factors.columns\n",
    "dat_factors = dat_factors[['article_number', 'year', 'week', 'season', \n",
    "            'deseas_net_qty', \n",
    "            'GAS_est_sp_deseas', 'GAS_est_rmh_deseas', 'GAS_est_gndr_deseas', \n",
    "            'GAS_est_agegrp_deseas', 'GAS_est_frnchse_deseas', 'GAS_est_prdgrp_deseas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAS behemoth\n",
    "\n",
    "def GAS_est_mv(df):\n",
    "    \n",
    "    # For article a's level of each category \n",
    "        # Retrieve deseasonalized y*\n",
    "        # Retreive appropriate GAS factors\n",
    "        # Then apply multi-factor GAS \n",
    "            \n",
    "    ret = pd.DataFrame()\n",
    "    ret['week'] = df['week']\n",
    "    \n",
    "    y = df.deseas_net_qty.values     \n",
    "    y = y.reshape(len(y), 1) \n",
    "        \n",
    "    f_sp = df.GAS_est_sp_deseas.values.reshape((len(y),1)) \n",
    "    f_rmh = df.GAS_est_rmh_deseas.values.reshape((len(y),1)) \n",
    "    f_gndr = df.GAS_est_gndr_deseas.values.reshape((len(y),1)) \n",
    "    f_age = df.GAS_est_agegrp_deseas.values.reshape((len(y),1)) \n",
    "    f_fr = df.GAS_est_frnchse_deseas.values.reshape((len(y),1)) \n",
    "    f_pr = df.GAS_est_prdgrp_deseas.values.reshape((len(y),1)) \n",
    "    \n",
    "    abc = scipy.optimize.minimize(\n",
    "        loglikest_mv,                                       \n",
    "        np.array([0.8, 0.9, np.mean(y), 1, np.mean(y), 0, 0, 0, 0, 0, 0]), \n",
    "        args = (y, f_sp, f_rmh, f_gndr, f_age, f_fr, f_pr),\n",
    "        options = {'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12},\n",
    "        method ='L-BFGS-B', \n",
    "        bounds =(\n",
    "                (0,  None),             # alpha\n",
    "                (-1, 1),                # beta\n",
    "                (0.001, None),          # omega \n",
    "                (0.001, None),          # sigma\n",
    "                (0.001, None),          # f\n",
    "                (None, None),           # lambda_sport\n",
    "                (None, None),           # lambda_rmh\n",
    "                (None, None),           # lambda_gender\n",
    "                (None, None),           # lambda_age\n",
    "                (None, None),           # lambda_franchise\n",
    "                (None, None)            # lambda_prod_grp\n",
    "               )\n",
    "    )\n",
    "      \n",
    "    # --- CONVERGENCE check message ---\n",
    "    if abc.success == False:\n",
    "        print('Convergence failed', abc.message)\n",
    "        print('Article:', df.article_number.unique()[0])\n",
    "        print()\n",
    "\n",
    "    x1par = initialize_parameters_mv(abc.x) \n",
    "    GAS = filterGAS_mv(y, f_sp, f_rmh, f_gndr, f_age, f_fr, f_pr, x1par)\n",
    "    ret['deseas_GAS_est'] = GAS\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.choice(dat_factors.article_number.unique(), size = 50, replace = False)\n",
    "dat_factors_subset = dat_factors[dat_factors.article_number.isin(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 10 = 1 min\n",
    "# 20 = 1 min 20s\n",
    "# 50 = 2 min 45s\n",
    "\n",
    "dat_factors_subset\n",
    "dat_factors_subset = pd.DataFrame(pd.merge(\n",
    "        dat_factors_subset, \n",
    "        dat_factors_subset.groupby('article_number').apply(GAS_est_mv).reset_index().drop('level_1', axis = 1)\n",
    "    ))\n",
    "\n",
    "dat_factors_subset.shape\n",
    "dat_factors_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_est = seas + deseas = \n",
    "#       = carryover_SS18['seas_preds'] + dat_factors['deseas_GAS_est']\n",
    "\n",
    "carryovers_SS18 = pd.merge(\n",
    "    carryovers_SS18,\n",
    "    dat_factors_subset[['article_number', 'season', 'year', 'week', 'deseas_GAS_est']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_number</th>\n",
       "      <th>DAA_0</th>\n",
       "      <th>ecom_marketing_forecast</th>\n",
       "      <th>GAS_est</th>\n",
       "      <th>seas_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>807295</td>\n",
       "      <td>860.20</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2376.297</td>\n",
       "      <td>878.016382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AJ5881</td>\n",
       "      <td>258.50</td>\n",
       "      <td>407.0</td>\n",
       "      <td>382.404</td>\n",
       "      <td>236.854112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BD5321</td>\n",
       "      <td>293.70</td>\n",
       "      <td>250.0</td>\n",
       "      <td>789.118</td>\n",
       "      <td>293.830528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJ9174</td>\n",
       "      <td>549.12</td>\n",
       "      <td>489.0</td>\n",
       "      <td>926.541</td>\n",
       "      <td>496.170246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BK3161</td>\n",
       "      <td>552.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>700.469</td>\n",
       "      <td>545.993015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BP6233</td>\n",
       "      <td>220.22</td>\n",
       "      <td>998.0</td>\n",
       "      <td>341.451</td>\n",
       "      <td>204.931391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BR1065</td>\n",
       "      <td>223.08</td>\n",
       "      <td>440.0</td>\n",
       "      <td>236.137</td>\n",
       "      <td>222.729893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BR5110</td>\n",
       "      <td>294.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>342.837</td>\n",
       "      <td>294.257353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BY9434</td>\n",
       "      <td>532.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>981.068</td>\n",
       "      <td>567.415294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BY9544</td>\n",
       "      <td>6755.32</td>\n",
       "      <td>6831.0</td>\n",
       "      <td>8351.530</td>\n",
       "      <td>6113.531021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CF2143</td>\n",
       "      <td>291.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.804</td>\n",
       "      <td>266.803161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CF5210</td>\n",
       "      <td>371.80</td>\n",
       "      <td>558.0</td>\n",
       "      <td>359.612</td>\n",
       "      <td>349.788643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CF7210</td>\n",
       "      <td>323.18</td>\n",
       "      <td>506.0</td>\n",
       "      <td>273.900</td>\n",
       "      <td>273.521109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CF9628</td>\n",
       "      <td>632.06</td>\n",
       "      <td>792.0</td>\n",
       "      <td>582.802</td>\n",
       "      <td>592.799678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CG4562</td>\n",
       "      <td>3247.20</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>3319.118</td>\n",
       "      <td>3114.994334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CM9542</td>\n",
       "      <td>391.82</td>\n",
       "      <td>454.0</td>\n",
       "      <td>308.594</td>\n",
       "      <td>312.921713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CN1042</td>\n",
       "      <td>589.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>476.300</td>\n",
       "      <td>470.264205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CV5941</td>\n",
       "      <td>2410.98</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>2054.272</td>\n",
       "      <td>2103.171622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CV7576</td>\n",
       "      <td>234.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.831</td>\n",
       "      <td>196.699866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CV8258</td>\n",
       "      <td>198.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>213.961</td>\n",
       "      <td>196.243904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>F37319</td>\n",
       "      <td>368.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>560.956</td>\n",
       "      <td>327.601873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M20326</td>\n",
       "      <td>4504.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11780.692</td>\n",
       "      <td>4278.878123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>S12241</td>\n",
       "      <td>143.00</td>\n",
       "      <td>300.0</td>\n",
       "      <td>354.079</td>\n",
       "      <td>134.169320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S21490</td>\n",
       "      <td>13241.80</td>\n",
       "      <td>22222.0</td>\n",
       "      <td>25815.097</td>\n",
       "      <td>12741.873073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_number     DAA_0  ecom_marketing_forecast    GAS_est    seas_preds\n",
       "0          807295    860.20                    900.0   2376.297    878.016382\n",
       "1          AJ5881    258.50                    407.0    382.404    236.854112\n",
       "2          BD5321    293.70                    250.0    789.118    293.830528\n",
       "3          BJ9174    549.12                    489.0    926.541    496.170246\n",
       "4          BK3161    552.20                      0.0    700.469    545.993015\n",
       "5          BP6233    220.22                    998.0    341.451    204.931391\n",
       "6          BR1065    223.08                    440.0    236.137    222.729893\n",
       "7          BR5110    294.80                      0.0    342.837    294.257353\n",
       "8          BY9434    532.40                      0.0    981.068    567.415294\n",
       "9          BY9544   6755.32                   6831.0   8351.530   6113.531021\n",
       "10         CF2143    291.72                      0.0    287.804    266.803161\n",
       "11         CF5210    371.80                    558.0    359.612    349.788643\n",
       "12         CF7210    323.18                    506.0    273.900    273.521109\n",
       "13         CF9628    632.06                    792.0    582.802    592.799678\n",
       "14         CG4562   3247.20                   3300.0   3319.118   3114.994334\n",
       "15         CM9542    391.82                    454.0    308.594    312.921713\n",
       "16         CN1042    589.16                      0.0    476.300    470.264205\n",
       "17         CV5941   2410.98                   3500.0   2054.272   2103.171622\n",
       "18         CV7576    234.52                      0.0    193.831    196.699866\n",
       "19         CV8258    198.00                      1.0    213.961    196.243904\n",
       "20         F37319    368.94                      0.0    560.956    327.601873\n",
       "21         M20326   4504.50                      0.0  11780.692   4278.878123\n",
       "22         S12241    143.00                    300.0    354.079    134.169320\n",
       "23         S21490  13241.80                  22222.0  25815.097  12741.873073"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(\n",
    "    mothership,\n",
    "    pd.DataFrame(carryovers_SS18.groupby('article_number')['seas_preds'].apply(sum)*1.1).reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carryovers_SS18['net_qty_est'] = carryovers_SS18.seas_preds + y_star.y_star \n",
    "# y^* = f + lambda1*f_s1 + lambda2*f_s2 + ... + f_s5\n",
    "# y_hat = y_hat_seas + y_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS to reverse engineer coefficients and predict!!\n",
    "\n",
    "# Combine into one df for regression trick\n",
    "dat_factors2 = pd.merge(\n",
    "    dat_factors,\n",
    "    carryovers_SS18[['article_number', 'season', 'year', 'week', 'deseas_GAS_est']]\n",
    ")\n",
    "\n",
    "# recreate coefficients, predict\n",
    "def regress2(df):\n",
    "    ret = pd.DataFrame()\n",
    "    ret['week'] = df.week\n",
    "\n",
    "    ret['y_star'] = sm.OLS(df.deseas_net_qty, \n",
    "           df[['deseas_GAS_est', \n",
    "               'GAS_est_sp_deseas', 'GAS_est_rmh_deseas', 'GAS_est_gndr_deseas', \n",
    "               'GAS_est_agegrp_deseas', 'GAS_est_frnchse_deseas', 'GAS_est_prdgrp_deseas']]\n",
    "          ).fit().predict().round(2)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "# true-er deseasonalized \n",
    "y_star = pd.DataFrame(dat_factors2.groupby('article_number').\n",
    "                      apply(regress2).\n",
    "                      reset_index()).drop('level_1', axis = 1)\n",
    "\n",
    "carryovers_SS18 = pd.merge(carryovers_SS18, y_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal component + true-er deseasonalized component\n",
    "carryovers_SS18['y_est'] = carryovers_SS18.seas_preds + carryovers_SS18.y_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_number</th>\n",
       "      <th>DAA_0</th>\n",
       "      <th>ecom_marketing_forecast</th>\n",
       "      <th>GAS_est</th>\n",
       "      <th>y_est</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>807295</td>\n",
       "      <td>860.20</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2376.297</td>\n",
       "      <td>872.747382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AJ5881</td>\n",
       "      <td>258.50</td>\n",
       "      <td>407.0</td>\n",
       "      <td>382.404</td>\n",
       "      <td>253.255112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BD5321</td>\n",
       "      <td>293.70</td>\n",
       "      <td>250.0</td>\n",
       "      <td>789.118</td>\n",
       "      <td>296.976528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJ9174</td>\n",
       "      <td>549.12</td>\n",
       "      <td>489.0</td>\n",
       "      <td>926.541</td>\n",
       "      <td>552.875246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BK3161</td>\n",
       "      <td>552.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>700.469</td>\n",
       "      <td>546.609015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BP6233</td>\n",
       "      <td>220.22</td>\n",
       "      <td>998.0</td>\n",
       "      <td>341.451</td>\n",
       "      <td>213.291391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BR1065</td>\n",
       "      <td>223.08</td>\n",
       "      <td>440.0</td>\n",
       "      <td>236.137</td>\n",
       "      <td>223.169893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BR5110</td>\n",
       "      <td>294.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>342.837</td>\n",
       "      <td>292.893353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BY9434</td>\n",
       "      <td>532.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>981.068</td>\n",
       "      <td>552.917294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BY9544</td>\n",
       "      <td>6755.32</td>\n",
       "      <td>6831.0</td>\n",
       "      <td>8351.530</td>\n",
       "      <td>5995.864021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CF2143</td>\n",
       "      <td>291.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.804</td>\n",
       "      <td>281.587161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CF5210</td>\n",
       "      <td>371.80</td>\n",
       "      <td>558.0</td>\n",
       "      <td>359.612</td>\n",
       "      <td>365.892643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CF7210</td>\n",
       "      <td>323.18</td>\n",
       "      <td>506.0</td>\n",
       "      <td>273.900</td>\n",
       "      <td>274.236109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CF9628</td>\n",
       "      <td>632.06</td>\n",
       "      <td>792.0</td>\n",
       "      <td>582.802</td>\n",
       "      <td>607.858678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CG4562</td>\n",
       "      <td>3247.20</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>3319.118</td>\n",
       "      <td>3351.384334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CM9542</td>\n",
       "      <td>391.82</td>\n",
       "      <td>454.0</td>\n",
       "      <td>308.594</td>\n",
       "      <td>304.979713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CN1042</td>\n",
       "      <td>589.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>476.300</td>\n",
       "      <td>476.292205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CV5941</td>\n",
       "      <td>2410.98</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>2054.272</td>\n",
       "      <td>2110.739622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CV7576</td>\n",
       "      <td>234.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.831</td>\n",
       "      <td>203.827866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CV8258</td>\n",
       "      <td>198.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>213.961</td>\n",
       "      <td>198.025904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>F37319</td>\n",
       "      <td>368.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>560.956</td>\n",
       "      <td>341.153873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M20326</td>\n",
       "      <td>4504.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11780.692</td>\n",
       "      <td>4440.501123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>S12241</td>\n",
       "      <td>143.00</td>\n",
       "      <td>300.0</td>\n",
       "      <td>354.079</td>\n",
       "      <td>138.624320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S21490</td>\n",
       "      <td>13241.80</td>\n",
       "      <td>22222.0</td>\n",
       "      <td>25815.097</td>\n",
       "      <td>13224.828073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_number     DAA_0  ecom_marketing_forecast    GAS_est         y_est\n",
       "0          807295    860.20                    900.0   2376.297    872.747382\n",
       "1          AJ5881    258.50                    407.0    382.404    253.255112\n",
       "2          BD5321    293.70                    250.0    789.118    296.976528\n",
       "3          BJ9174    549.12                    489.0    926.541    552.875246\n",
       "4          BK3161    552.20                      0.0    700.469    546.609015\n",
       "5          BP6233    220.22                    998.0    341.451    213.291391\n",
       "6          BR1065    223.08                    440.0    236.137    223.169893\n",
       "7          BR5110    294.80                      0.0    342.837    292.893353\n",
       "8          BY9434    532.40                      0.0    981.068    552.917294\n",
       "9          BY9544   6755.32                   6831.0   8351.530   5995.864021\n",
       "10         CF2143    291.72                      0.0    287.804    281.587161\n",
       "11         CF5210    371.80                    558.0    359.612    365.892643\n",
       "12         CF7210    323.18                    506.0    273.900    274.236109\n",
       "13         CF9628    632.06                    792.0    582.802    607.858678\n",
       "14         CG4562   3247.20                   3300.0   3319.118   3351.384334\n",
       "15         CM9542    391.82                    454.0    308.594    304.979713\n",
       "16         CN1042    589.16                      0.0    476.300    476.292205\n",
       "17         CV5941   2410.98                   3500.0   2054.272   2110.739622\n",
       "18         CV7576    234.52                      0.0    193.831    203.827866\n",
       "19         CV8258    198.00                      1.0    213.961    198.025904\n",
       "20         F37319    368.94                      0.0    560.956    341.153873\n",
       "21         M20326   4504.50                      0.0  11780.692   4440.501123\n",
       "22         S12241    143.00                    300.0    354.079    138.624320\n",
       "23         S21490  13241.80                  22222.0  25815.097  13224.828073"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(\n",
    "    mothership,\n",
    "    pd.DataFrame(carryovers_SS18.groupby('article_number')['y_est'].sum()*1.1).reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat0.copy()\n",
    "\n",
    "dat = dat.dropna()\n",
    "dat = dat[(dat.season_net_qty > 100) & (dat.season == 'SS19')]\n",
    "dat = dat.groupby('article_number')['net_qty'].apply('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_SS18 = dat[dat.season == 'SS18'].copy().rename(columns = {'net_qty': 'article_net_qty'}).drop(['clearance', 'margin', 'gross_demand_quantity', 'season_gross_demand_quantity'], axis = 1)\n",
    "\n",
    "# # ---- For development select subset of articles ----\n",
    "# articles = dat_SS18.article_number.unique()\n",
    "# articles_subset = np.random.choice(articles, size = 5, replace = False)\n",
    "\n",
    "# dat_SS18 = dat_SS18[dat_SS18.article_number.isin(articles_subset)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.random.choice(articles_subset, size = 1, replace = False)\n",
    "# dat_SS18[dat_SS18.article_number == a[0]][['article_net_qty', 'seas_preds']].plot(linewidth = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
