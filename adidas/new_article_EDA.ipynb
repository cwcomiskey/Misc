{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modules -- \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "# import dask.dataframe as ddf\n",
    "# import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 800)\n",
    "\n",
    "import scipy\n",
    "\n",
    "import statsmodels.api as sm\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# from statsmodels.tsa.tsatools import detrend\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat0 = pd.read_csv('data/ch4k.csv')\n",
    "ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, error_bad_lines = False, \n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "                                  'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc',\n",
    "                                  'brand_desc', 'bus_unit_desc', 'rmh_cat_desc'])\n",
    "\n",
    "# Remove clearance transactions!!\n",
    "# dat0['clearance'] = dat0.clearance.fillna(0) \n",
    "# dat0['net_qty'] = (1 - dat0.clearance)*dat0.net_qty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wrangling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()\n",
    "\n",
    "dat = dat[(dat.net_qty > 0) & (dat.season.isin(['SS17', 'SS18', 'SS19']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SS19 = pd.read_excel('data/ecom_SS19.xlsx').dropna()\n",
    "SS19.shape\n",
    "\n",
    "SS19 = SS19[SS19.carryover_FW18 == 'NO'] # new articles only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Within article price, cost, margin averages \n",
    "dat[['price', 'cost', 'margin']] = (\n",
    "    dat.\n",
    "    groupby('article_number', group_keys=False)[['price', 'cost', 'margin']].\n",
    "    transform(lambda x: np.nanmean(x).round(2))\n",
    "             )\n",
    "\n",
    "\n",
    "# Remove articles where 'price = NA' in all transactions\n",
    "dat = dat[~dat.price.isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Correct -- \n",
    "\n",
    "dat = dat.sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# ---- Replace NAs and zeros (w/ no impact replacements) ----\n",
    "dat['buy_availability'] = dat.buy_availability.fillna(1) # assume full availability\n",
    "dat['buy_availability'] = np.where(dat.buy_availability == 0, 1, dat.buy_availability) # replace 0\n",
    "\n",
    "\n",
    "# ---- Smooth buy_availability ----\n",
    "def roll(df):\n",
    "    return df.rolling(window = 5, min_periods = 1, center = True).mean()\n",
    "\n",
    "dat['buy_availability'] = dat.groupby(['article_number', 'country'])['buy_availability'].apply(roll)\n",
    "\n",
    "\n",
    "# ---- Correct ----\n",
    "dat['corr_net_qty'] = (dat.net_qty / dat.buy_availability).round()\n",
    "\n",
    "\n",
    "# ---- Aggregate to season ----\n",
    "dat['corr_season_net_qty'] = dat.groupby(['article_number', 'season'])['corr_net_qty'].transform(lambda x: np.sum(x))\n",
    "dat['corr_season_net_qty'] = np.where(dat.corr_season_net_qty > dat.season_net_qty, dat.corr_season_net_qty, dat.season_net_qty) # only if >\n",
    "\n",
    "dat['avg_buy_availability'] = dat.groupby(['article_number', 'season'])['buy_availability'].transform(lambda x: np.nanmean(x).round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Time series shenanigans: alternative correction -- \n",
    "\n",
    "# pd.DataFrame([5, 5,5,5,5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5]).ewm(com = 1).mean().plot()\n",
    "\n",
    "# pd.DataFrame([5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5]).rolling(window = 5, center = True).mean().plot()\n",
    "\n",
    "# from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "\n",
    "# yw.loc[:,'YEAR'] = [str(x)[:-2] for x in yw.year]\n",
    "# yw.loc[:,'WEEK'] = [str(x)[:-2] for x in yw.week]\n",
    "\n",
    "# yw.loc[:,'date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(yw.YEAR, yw.WEEK)]\n",
    "\n",
    "# arimax = sm.tsa.statespace.SARIMAX(yX.net_qty,\n",
    "#                                    order = (1,0,1),\n",
    "#                                    seasonal_order = (0,0,0,0),\n",
    "#                                    exog = yX.drop('net_qty', axis = 1),\n",
    "#                                    enforce_stationarity=False, \n",
    "#                                    enforce_invertibility=False,\n",
    "#                                    missing = 'drop').fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Keep first season only\n",
    "dat = (dat.sort_values(['article_number', 'season']).drop_duplicates(subset = 'article_number'))\n",
    "\n",
    "\n",
    "dat = dat[['article_number', 'brand', 'season', 'season_net_qty', 'corr_season_net_qty', 'avg_buy_availability', \n",
    "           'art_desc', 'sports_cat_desc', 'rmh_cat_desc', 'franchise', \n",
    "           'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc', \n",
    "           'price', 'cost', 'margin']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dat = dat.set_index('article_number')\n",
    "\n",
    "dat = dat[dat.corr_season_net_qty > 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "81.5*(40/37) + 81.5*0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "95/57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "89.5*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# art = np.random.choice(dat.index, size = 500, replace = False)\n",
    "\n",
    "# Just articles new in SS19, with season_net_qty > 200\n",
    "art = set(SS19.article_number).intersection(set(dat.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Hiearachical Sample (HS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hierarchical sampling -- \n",
    "\n",
    "from itertools import permutations \n",
    "from itertools import combinations\n",
    "\n",
    "d = {}\n",
    "cats = ['sports_cat_desc', 'rmh_cat_desc', 'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc']\n",
    "\n",
    "# Create tidy dataframe, add results to it\n",
    "\n",
    "for a in art:\n",
    "    p = dat.loc[a, 'price']\n",
    "    \n",
    "    net_qtys = pd.Series()\n",
    "    \n",
    "    dat_p = dat[(dat.price >= 0.9*p) & (dat.price <= 1.1*p)]\n",
    "    \n",
    "    for c in cats: \n",
    "        dat_a = dat_p[dat_p[c] == dat_p.loc[a, c]] # filter to that level of that category        \n",
    "        net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    \n",
    "    for c2 in combinations(cats, 2):\n",
    "        dat_a = dat_p[(dat_p[c2[0]] == dat_p.loc[a, c2[0]]) &\n",
    "                      (dat_p[c2[1]] == dat_p.loc[a, c2[1]])]\n",
    "        net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "        \n",
    "    for c3 in combinations(cats, 3):\n",
    "        dat_a = dat_p[(dat_p[c3[0]] == dat_p.loc[a, c3[0]]) &\n",
    "                      (dat_p[c3[1]] == dat_p.loc[a, c3[1]]) &\n",
    "                      (dat_p[c3[2]] == dat_p.loc[a, c3[2]])]\n",
    "        net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    \n",
    "    for c4 in combinations(cats, 4):\n",
    "        dat_a = dat_p[(dat_p[c4[0]] == dat_p.loc[a, c4[0]]) &\n",
    "                      (dat_p[c4[1]] == dat_p.loc[a, c4[1]]) &\n",
    "                      (dat_p[c4[2]] == dat_p.loc[a, c4[2]]) &\n",
    "                      (dat_p[c4[3]] == dat_p.loc[a, c4[3]])]\n",
    "        net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    \n",
    "    for c4 in combinations(cats, 5):\n",
    "        dat_a = dat_p[(dat_p[c4[0]] == dat_p.loc[a, c4[0]]) &\n",
    "                      (dat_p[c4[1]] == dat_p.loc[a, c4[1]]) &\n",
    "                      (dat_p[c4[2]] == dat_p.loc[a, c4[2]]) &\n",
    "                      (dat_p[c4[3]] == dat_p.loc[a, c4[3]]) &\n",
    "                      (dat_p[c4[4]] == dat_p.loc[a, c4[4]])]\n",
    "        net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    \n",
    "    for c4 in combinations(cats, 6):\n",
    "        dat_a = dat_p[(dat_p[c4[0]] == dat_p.loc[a, c4[0]]) &\n",
    "                      (dat_p[c4[1]] == dat_p.loc[a, c4[1]]) &\n",
    "                      (dat_p[c4[2]] == dat_p.loc[a, c4[2]]) &\n",
    "                      (dat_p[c4[3]] == dat_p.loc[a, c4[3]]) &\n",
    "                      (dat_p[c4[4]] == dat_p.loc[a, c4[4]]) &\n",
    "                      (dat_p[c4[5]] == dat_p.loc[a, c4[5]])]\n",
    "        net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    \n",
    "    d[a] = {\n",
    "        'mean': net_qtys.mean(),\n",
    "        'max': net_qtys.max(),\n",
    "        '50': np.percentile(net_qtys, 50),\n",
    "        '70': np.percentile(net_qtys, 70),\n",
    "        '80': np.percentile(net_qtys, 80),\n",
    "        '90': np.percentile(net_qtys, 90),\n",
    "        'length': len(net_qtys)\n",
    "           }\n",
    "    if len(d) % 50 == 0:\n",
    "        print(len(d))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dat_art = dat[dat.index.isin(art)] # SS19-ers only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Profit fcn -- \n",
    "def P(d, margin, cost, b):\n",
    "    if d > b:    # CANNOT satisfy demand\n",
    "        return b*margin\n",
    "    \n",
    "    elif d <= b: # CAN satisfy demand\n",
    "        return d*margin - (b - d)*cost\n",
    "    \n",
    "    else:\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Save/load HSing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# save d\n",
    "# import json\n",
    "\n",
    "# json_i = json.dumps(d)\n",
    "# f = open(\"d.json\", \"w\")\n",
    "# f.write(json_i)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "with open('data/d.json') as json_file:\n",
    "    d = json.load(json_file)\n",
    "\n",
    "\n",
    "# {k: d[k] for k in sorted(d.keys())[:2]}\n",
    "# {k: d_test[k] for k in sorted(d_test.keys())[:2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "{k: d[k] for k in sorted(d.keys())[:2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Back to business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "pct_7 = '70'\n",
    "pct_8 = '80'\n",
    "pct_9 = '90'\n",
    "\n",
    "preds = pd.DataFrame([(a, d[a][pct_7], d[a][pct_8], d[a][pct_9], d[a]['length']) for a in d.keys()]).round()\n",
    "preds.columns = ('article_number', 'pctl_7', 'pctl_8', 'pctl_9', 'length')\n",
    "preds = preds.set_index('article_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# -- logistic regression add-on ----- \n",
    "\n",
    "# dat_art0 = pd.merge(dat_art, preds, left_index = True, right_index = True\n",
    "#                    ).merge(log_reg, left_index=True, right_index=True)\n",
    "\n",
    "# dat_art0.columns\n",
    "\n",
    "# # Logistic regression angle for <500 articles\n",
    "# dat_art0['pred'] = np.where(dat_art0['>500'], dat_art0['pctl_u'], dat_art0['pctl_l'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dat_art.shape\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dat_art2 = pd.merge(dat_art, preds, left_index=True, right_index=True, how = 'left') # dat_art + HierSamp preds\n",
    "\n",
    "dat_art2 = dat_art2[['season', 'season_net_qty', 'corr_season_net_qty', 'art_desc', 'sports_cat_desc', \n",
    "                     'rmh_cat_desc', 'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc', \n",
    "                     'price', 'margin', 'cost', 'pctl_7', 'pctl_8', 'pctl_9', 'length']] # .sort_values('APE', ascending = False)\n",
    "\n",
    "dat_art2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_art4 = pd.merge(dat_art2, SS19[['article_number', 'Ecom_FC_RMA']], left_index = True, right_on = 'article_number', how = 'left').set_index('article_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dat_art5 = dat_art4.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dat_art5[(dat_art5.corr_season_net_qty > dat_art5.season_net_qty) &\n",
    "         (dat_art5.corr_season_net_qty > dat_art5.Ecom_FC_RMA) & \n",
    "         (dat_art5.corr_season_net_qty < dat_art5.pctl_9)][['season', 'season_net_qty', 'corr_season_net_qty', 'pctl_7', 'pctl_8', 'pctl_9', 'Ecom_FC_RMA', 'art_desc',\n",
    "       'sports_cat_desc', 'franchise', \n",
    "       'prod_type_desc', 'price', 'margin',\n",
    "       'cost', 'length']]#[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    " \n",
    "dat_art5[dat_art5.index == 'EE3708']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# HSample -- just one article -- \n",
    "demo = {}\n",
    "\n",
    "a = 'EE3708'\n",
    "p = dat.loc[a, 'price']\n",
    "\n",
    "net_qtys = pd.Series()\n",
    "\n",
    "dat_p = dat[(dat.price >= 0.9*p) & (dat.price <= 1.1*p)]\n",
    "\n",
    "for c in cats: \n",
    "    dat_a = dat_p[dat_p[c] == dat_p.loc[a, c]] # filter to that level of that category        \n",
    "    net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    nq1 = net_qtys.copy()\n",
    "\n",
    "for c2 in combinations(cats, 2):\n",
    "    dat_a = dat_p[(dat_p[c2[0]] == dat_p.loc[a, c2[0]]) &\n",
    "                  (dat_p[c2[1]] == dat_p.loc[a, c2[1]])]\n",
    "    net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    nq2 = net_qtys.copy()\n",
    "\n",
    "\n",
    "for c3 in combinations(cats, 3):\n",
    "    dat_a = dat_p[(dat_p[c3[0]] == dat_p.loc[a, c3[0]]) &\n",
    "                  (dat_p[c3[1]] == dat_p.loc[a, c3[1]]) &\n",
    "                  (dat_p[c3[2]] == dat_p.loc[a, c3[2]])]\n",
    "    net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    nq3 = net_qtys.copy()\n",
    "\n",
    "\n",
    "for c4 in combinations(cats, 4):\n",
    "    dat_a = dat_p[(dat_p[c4[0]] == dat_p.loc[a, c4[0]]) &\n",
    "                  (dat_p[c4[1]] == dat_p.loc[a, c4[1]]) &\n",
    "                  (dat_p[c4[2]] == dat_p.loc[a, c4[2]]) &\n",
    "                  (dat_p[c4[3]] == dat_p.loc[a, c4[3]])]\n",
    "    net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    nq4 = net_qtys.copy()\n",
    "\n",
    "for c4 in combinations(cats, 5):\n",
    "    dat_a = dat_p[(dat_p[c4[0]] == dat_p.loc[a, c4[0]]) &\n",
    "                  (dat_p[c4[1]] == dat_p.loc[a, c4[1]]) &\n",
    "                  (dat_p[c4[2]] == dat_p.loc[a, c4[2]]) &\n",
    "                  (dat_p[c4[3]] == dat_p.loc[a, c4[3]]) &\n",
    "                  (dat_p[c4[4]] == dat_p.loc[a, c4[4]])]\n",
    "    net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    nq5 = net_qtys.copy()\n",
    "\n",
    "for c4 in combinations(cats, 6):\n",
    "    dat_a = dat_p[(dat_p[c4[0]] == dat_p.loc[a, c4[0]]) &\n",
    "                  (dat_p[c4[1]] == dat_p.loc[a, c4[1]]) &\n",
    "                  (dat_p[c4[2]] == dat_p.loc[a, c4[2]]) &\n",
    "                  (dat_p[c4[3]] == dat_p.loc[a, c4[3]]) &\n",
    "                  (dat_p[c4[4]] == dat_p.loc[a, c4[4]]) &\n",
    "                  (dat_p[c4[5]] == dat_p.loc[a, c4[5]])]\n",
    "    net_qtys = net_qtys.append(dat_a.corr_season_net_qty)\n",
    "    nq6 = net_qtys.copy()\n",
    "\n",
    "demo[a] = {\n",
    "    'mean': net_qtys.mean(),\n",
    "    'max': net_qtys.max(),\n",
    "    '50': np.percentile(net_qtys, 50),\n",
    "    '70': np.percentile(net_qtys, 70),\n",
    "    '80': np.percentile(net_qtys, 80),\n",
    "    '90': np.percentile(net_qtys, 90),\n",
    "    'length': len(net_qtys)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     5,
     11
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# combinatorics -- \n",
    "\n",
    "for i in combinations(cats, 2):\n",
    "    print(i)\n",
    "\n",
    "def factorial(x):\n",
    "    fact = 1\n",
    "    for i in range(1,x+1): \n",
    "        fact = fact * i\n",
    "    return fact\n",
    "\n",
    "def comb(n, r):\n",
    "    return factorial(n)/(factorial(n-r)*factorial(r))\n",
    "\n",
    "comb(7,4)\n",
    "\n",
    "\n",
    "one = nq1.shape[0]\n",
    "one\n",
    "\n",
    "two = nq2.shape[0] - nq1.shape[0]\n",
    "two\n",
    "\n",
    "three = nq3.shape[0] - two - one\n",
    "three\n",
    "\n",
    "four = nq4.shape[0] - three - two - one\n",
    "four\n",
    "\n",
    "five = nq5.shape[0] - four - three - two - one\n",
    "five\n",
    "\n",
    "six = nq6.shape[0] -  five -  four - three - two - one\n",
    "six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_qtys.shape\n",
    "type(net_qtys)\n",
    "\n",
    "net_qtys.hist(bins = 40, figsize=(7, 5))\n",
    "plt.xlabel('Net Quantity', size = 20)\n",
    "plt.title('Pseudo-Empirical Distribution', size = 20)\n",
    "# plt.axvline(x = 2326, color = 'red', linewidth = 5)\n",
    "\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#P(row['season_net_qty'], row['margin'], row['cost'], row['pred'])\n",
    "\n",
    "dat_art5['eCom_profit'] = dat_art5.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['Ecom_FC_RMA']), axis=1)\n",
    "dat_art5['DAA_profit7']  = dat_art5.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['pctl_7']), axis=1)\n",
    "dat_art5['DAA_profit8']  = dat_art5.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['pctl_8']), axis=1)\n",
    "dat_art5['DAA_profit9']  = dat_art5.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['pctl_9']), axis=1)\n",
    "\n",
    "\n",
    "# dat_art4['DAA_profit_pred']  = dat_art4.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['pred']), axis=1)\n",
    "# dat_art4['DAA_profit_m']  = dat_art4.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['mean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "l = np.array([0, 2000, 5000, 10000, 75000])\n",
    "dat_art5.loc[:,'net_bins'] = pd.cut(dat_art['season_net_qty'], bins = l)\n",
    "\n",
    "dat_art5[['DAA_profit7', 'DAA_profit8', 'DAA_profit9', 'eCom_profit']].sum().round()\n",
    "(dat_art5.groupby('net_bins')[['DAA_profit7', 'DAA_profit8', 'DAA_profit9', 'eCom_profit']].sum()/1000000).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:08:16.045552Z",
     "start_time": "2019-10-31T10:08:15.631571Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Profit w/ logistic regression add-on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Model fitting further down\n",
    "\n",
    "dat_art00 = pd.merge(dat_art0, SS19[['article_number', 'Ecom_FC_RMA']], left_index = True, right_on = 'article_number', how = 'left').drop('article_number', axis = 1)\n",
    "\n",
    "dat_art00['DAA_profit_pctl']  = dat_art00.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['pctl_u']), axis=1)\n",
    "dat_art00['eCom_profit'] = dat_art00.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['Ecom_FC_RMA']), axis=1)\n",
    "dat_art00['DAA_profit']  = dat_art00.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['pred']), axis=1)\n",
    "\n",
    "dat_art00[['DAA_profit', 'eCom_profit', 'DAA_profit_pctl']].sum().round()\n",
    "\n",
    "# Using pctl_l = 50 for predicted <500 unit articles:\n",
    "# --> eCom makes ~18m, we lose ~5m\n",
    "# DAA_profit         108593975.0\n",
    "# eCom_profit        131718527.0\n",
    "# DAA_profit_pctl    106169547.0\n",
    "\n",
    "# Using pctl_l = 70 for predicted <500 unit articles:\n",
    "# --> eCom makes ~18m, we... MAKE MONEY!!\n",
    "# DAA_profit         119232632.0\n",
    "# eCom_profit        131718527.0\n",
    "# DAA_profit_pctl    106169547.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression: profitability BY corrected net_qty bins\n",
    "    \n",
    "l = np.array([0, 500, 1000, 2000, 5000, 10000, 75000])\n",
    "dat_art00.loc[:,'corr_seas_net_qty_bins'] = pd.cut(dat_art00['corr_season_net_qty'], bins = l)\n",
    "\n",
    "dat_art00.groupby('corr_seas_net_qty_bins')[['DAA_profit', 'eCom_profit', 'DAA_profit_pctl']].sum().round()/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Profit summary stats\n",
    "\n",
    "pct\n",
    "dat_art4[(dat_art4.corr_season_net_qty > 500)][['DAA_profit_pct', 'eCom_profit']].sum().round()\n",
    "\n",
    "# '90' # empirical percentile used\n",
    "# DAA_profit_pct     113660189.0\n",
    "# eCom_profit        112574442.0\n",
    "# dtype: float64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# -- EDA --\n",
    "\n",
    "dat_art4['diff']  = dat_art4.apply(lambda row: row['pred_pct'] - row['corr_season_net_qty'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# -- EDA --\n",
    "\n",
    "l = np.array([0, 500, 1000, 2000, 5000, 10000, 75000])\n",
    "dat_art4.loc[:,'prediction_bins'] = pd.cut(dat_art4['pred_pct'], bins = l)\n",
    "\n",
    "# p = np.array([0, 50, 100, 150, 200, 300, 1000])\n",
    "# dat_mini.loc[:,'price_bin'] = pd.cut(dat_mini.price, bins = p)\n",
    "\n",
    "dat_art4.groupby('prediction_bins')[['DAA_profit_pct', 'eCom_profit']].apply(np.mean).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# -- EDA --\n",
    "\n",
    "dat_art4.groupby('prediction_bins')[['price']].apply(np.mean).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# -- EDA --\n",
    "\n",
    "x = 'corr_season_net_qty'\n",
    "y = 'pred_pct'\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]\n",
    "plt.scatter(dat_art4[x], dat_art4[y], alpha = 0.1)\n",
    "\n",
    "plt.xlabel(x)\n",
    "plt.ylabel(y)\n",
    "\n",
    "x = np.linspace(0, 20000,100)\n",
    "y = x\n",
    "plt.plot(x, y, '-b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -- EDA --\n",
    "\n",
    "plt.hist(net_qtys, bins = [0, 100, 250, 500, 750, 1000, 1250, 1500, 1750, 2000, 5000], density = True)\n",
    "\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Empirical distributions\n",
    "\n",
    "import numpy as np\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "ecdf = ECDF(net_qtys)\n",
    "\n",
    "ecdf([100, 500, 1000, 3000])\n",
    "\n",
    "np.percentile(net_qtys, 75)\n",
    "net_qtys.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## decision tree example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer() \n",
    "\n",
    "print(cancer.keys())\n",
    "print()\n",
    "\n",
    "print(cancer.data.shape)\n",
    "print()\n",
    "\n",
    "print(\n",
    "{n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))})\n",
    "print()\n",
    "\n",
    "print(cancer.feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "type(cancer.data)\n",
    "type(cancer.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set\", tree.score(X_train, y_train))\n",
    "print(\"Accuracy on test set:\", tree.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## decision tree article forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### load and wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat0 = pd.read_csv('data/ch4k.csv')\n",
    "ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, error_bad_lines = False, \n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "                                  'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc',\n",
    "                                  'brand_desc', 'bus_unit_desc', 'rmh_cat_desc'])\n",
    "\n",
    "# Remove clearance transactions\n",
    "# dat0['clearance'] = dat0.clearance.fillna(0) \n",
    "# dat0['net_qty'] = (1 - dat0.clearance)*dat0.net_qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:47:31.573653Z",
     "start_time": "2019-11-28T09:47:31.359087Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:47:34.370462Z",
     "start_time": "2019-11-28T09:47:33.889129Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = dat[(dat.net_qty > 0) & (dat.season.isin(['SS17', 'SS18', 'SS19']))]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.shape\n",
    "dat[dat.margin.isna()].shape\n",
    "\n",
    "# Why missing SO MANY price/cost/margins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SS19 = pd.read_excel('data/ecom_SS19.xlsx').dropna()\n",
    "# SS19.shape\n",
    "\n",
    "# SS19 = SS19[SS19.carryover_FW18 == 'NO'] # new articles only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Within article price, cost, margin averages\n",
    "# dat[['price', 'cost', 'margin']] = (\n",
    "#     dat.\n",
    "#     groupby('article_number', group_keys=False)[['price', 'cost', 'margin']].\n",
    "#     transform(lambda x: np.nanmean(x).round(2))\n",
    "#              )\n",
    "\n",
    "\n",
    "# Remove articles where 'price = NA' in all transactions\n",
    "# dat = dat[~dat.price.isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dat = dat.sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# # ---- Replace NAs and zeros (w/ no impact replacements) ----\n",
    "# dat['buy_availability'] = dat.buy_availability.fillna(1) # assume full availability\n",
    "# dat['buy_availability'] = np.where(dat.buy_availability == 0, 1, dat.buy_availability) # replace 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# ---- Smooth buy_availability ----\n",
    "# def roll(df):\n",
    "#     return df.rolling(window = 5, min_periods = 1, center = True).mean()\n",
    "\n",
    "# # dat['buy_availability3'] = dat.groupby(['article_number', 'country'])['buy_availability3'].apply(roll)\n",
    "# dat['buy_availability'] = dat.groupby(['article_number', 'country'])['buy_availability'].apply(roll)\n",
    "\n",
    "# # ---- Corrected net_qty ----\n",
    "# dat['corr_net_qty'] = (dat.net_qty / dat.buy_availability).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# corr_season_net_qty = pd.DataFrame(dat.groupby(['article_number', 'season'], group_keys = False)['corr_net_qty'].apply(sum)).reset_index()\n",
    "# dat['avg_buy_availability'] = dat.groupby(['article_number', 'season'])['buy_availability3'].transform(lambda x: np.nanmean(x).round(3))\n",
    "\n",
    "# dat['corr_season_net_qty'] = dat.groupby(['article_number', 'season'])['corr_net_qty'].transform(lambda x: np.sum(x))\n",
    "# dat['corr_season_net_qty'] = np.where(dat.corr_season_net_qty > dat.season_net_qty, dat.corr_season_net_qty, dat.season_net_qty) # only if >\n",
    "\n",
    "# dat['avg_buy_availability'] = dat.groupby(['article_number', 'season'])['buy_availability'].transform(lambda x: np.nanmean(x).round(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:47:41.334198Z",
     "start_time": "2019-11-28T09:47:40.899882Z"
    }
   },
   "outputs": [],
   "source": [
    "dat['season_net_qty'] = dat.groupby(['article_number', 'brand', 'season'])['net_qty'].transform(sum)\n",
    "\n",
    "dat = dat[dat.season_net_qty > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:47:48.016815Z",
     "start_time": "2019-11-28T09:47:47.600852Z"
    }
   },
   "outputs": [],
   "source": [
    "dat = (dat[['article_number', 'brand', 'season', \n",
    "       'season_net_qty', 'model_no', 'art_desc',\n",
    "       'sports_cat_desc', 'rmh_cat_desc', 'franchise', 'gender_desc',\n",
    "       'age_group_desc', 'prod_grp_desc', 'prod_type_desc']].\n",
    "       sort_values(['article_number', 'season']).\n",
    "       drop_duplicates(subset = 'article_number')\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:50:24.935833Z",
     "start_time": "2019-11-28T09:50:24.904450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_number     0\n",
       "brand              0\n",
       "season             0\n",
       "season_net_qty     0\n",
       "model_no           0\n",
       "art_desc           0\n",
       "sports_cat_desc    0\n",
       "rmh_cat_desc       0\n",
       "franchise          0\n",
       "gender_desc        0\n",
       "age_group_desc     0\n",
       "prod_grp_desc      0\n",
       "prod_type_desc     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.isna().sum()\n",
    "dat = dat[~dat.art_desc.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:51:12.738575Z",
     "start_time": "2019-11-28T09:51:12.728071Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dat = dat.set_index('article_number')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# art = np.random.choice(dat.index, size = 500, replace = False)\n",
    "\n",
    "# Just articles new in SS19, with season_net_qty > 200\n",
    "art = set(SS19.article_number).intersection(set(dat.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:17:44.827230Z",
     "start_time": "2019-11-28T10:17:44.815594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30512, 12)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "30512"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape\n",
    "len(dat.index.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### One-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:24:43.371504Z",
     "start_time": "2019-11-28T10:24:43.305185Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# subset = np.random.choice(dat.index, size = 15000, replace = False)\n",
    "# dat_tree = dat[dat.index.isin(subset)].copy()\n",
    "\n",
    "dat_tree = dat.copy()\n",
    "\n",
    "dat_tree = dat_tree[['season_net_qty', 'sports_cat_desc',\n",
    "                'rmh_cat_desc', 'franchise', 'gender_desc', 'age_group_desc',\n",
    "                'prod_grp_desc', 'prod_type_desc']]\n",
    "\n",
    "dat_tree = pd.get_dummies(dat_tree)\n",
    "\n",
    "# p = np.array([0, 50, 100, 150, 200, 300, 1000])\n",
    "# dat_mini.loc[:,'price_bin'] = pd.cut(dat_mini.price, bins = p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:25:30.199196Z",
     "start_time": "2019-11-28T10:25:30.195548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30512, 381)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_tree.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:38:00.933239Z",
     "start_time": "2019-11-28T10:38:00.487178Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:38:00.933239Z",
     "start_time": "2019-11-28T10:38:00.487178Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.197"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.197"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = dat_tree.drop('season_net_qty', axis = 1)\n",
    "y = dat_tree.season_net_qty\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, random_state=42)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "linreg.score(X, y).round(3)\n",
    "\n",
    "r2_score(y, linreg.predict(X)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T09:36:27.187182Z",
     "start_time": "2019-10-31T09:36:27.185207Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:25:33.098915Z",
     "start_time": "2019-11-28T10:25:32.952986Z"
    },
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.763"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression on corr_net_qty > 500\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = dat_tree.drop('season_net_qty', axis = 1)\n",
    "y = dat_tree.season_net_qty > 500\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "logreg.score(X_train, y_train).round(3)\n",
    "logreg.score(X_test, y_test).round(3)\n",
    "\n",
    "# Could use this in combination with HS method to identify < 500 articles, then use lower percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:27:11.889714Z",
     "start_time": "2019-11-28T10:27:11.860653Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7628, 380)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>season_net_qty</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>5300</td>\n",
       "      <td>1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>371</td>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "season_net_qty  False  True \n",
       "row_0                       \n",
       "False            5300   1438\n",
       "True              371    519"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7628474042999476"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n",
    "pd.crosstab(logreg.predict(X_test), y_test)\n",
    "\n",
    "(5300 + 519)/(7628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_reg = pd.DataFrame(data = logreg.predict(dat_tree_dummies.loc[:, 'price':]), index = dat_tree.index, columns = ['>500'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:43:22.400502Z",
     "start_time": "2019-11-28T10:43:21.048642Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25803565547226226"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.11769572881049484"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.23704042783307422"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.24334923896388805"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X = dat_tree.drop('season_net_qty', axis = 1)\n",
    "y = dat_tree.season_net_qty\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, random_state=42)\n",
    "\n",
    "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "\n",
    "tree.score(X_train, y_train)\n",
    "tree.score(X_test, y_test)\n",
    "tree.score(X, y)\n",
    "\n",
    "DecisionTreeRegressor().fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:47:59.024068Z",
     "start_time": "2019-11-28T10:47:59.021486Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:48:37.940578Z",
     "start_time": "2019-11-28T10:48:37.911839Z"
    }
   },
   "outputs": [],
   "source": [
    "X = dat_tree.drop('season_net_qty', axis = 1)\n",
    "y = dat_tree.season_net_qty > 500\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:48:38.898504Z",
     "start_time": "2019-11-28T10:48:38.298723Z"
    },
    "hideOutput": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state = 0)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:48:38.908353Z",
     "start_time": "2019-11-28T10:48:38.899757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7597011012060828"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T10:48:51.313111Z",
     "start_time": "2019-11-28T10:48:51.308510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2609465128474043"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_preds = pd.DataFrame(data = tree.predict(X), index = dat_tree_dummies.index, columns = ['>500'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dat_art5[dat_art5.pred_90 < dat_art5.season_net_qty].sum()\n",
    "\n",
    "# season_net_qty         227889.0\n",
    "# pred_80                129573.0\n",
    "# Ecom_FC_RMA            252412.0\n",
    "# DAA_profit_pred_80    4725638.0\n",
    "# eCom_profit           6433454.0\n",
    "# diff                  2266284.0\n",
    "# dtype: float64\n",
    "\n",
    "# Comments: \n",
    "    # when we under-forecast... we DRAMATICALLY under-forecast\n",
    "    # HUGE difference in profit\n",
    "\n",
    "dat_art5[dat_art5.pred_90 >= dat_art5.season_net_qty].sum()\n",
    "\n",
    "# season_net_qty         179877.0\n",
    "# pred_80                354269.0\n",
    "# Ecom_FC_RMA            310015.0\n",
    "# DAA_profit_pred_80    4232213.0\n",
    "# eCom_profit           4550809.0\n",
    "# diff                  2017816.0\n",
    "# dtype: float64\n",
    "\n",
    "# Comments: \n",
    "    # in sum, we just slightly over-forecast\n",
    "    # Tiny difference in profit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "## buy_availability correction\n",
    "\n",
    "dat_roll = dat0.copy()\n",
    "\n",
    "dat_roll = dat_roll[(dat_roll.net_qty > 0) & (dat_roll.season.isin(['SS17', 'SS18', 'SS19']))] # No buy_availability before '17\n",
    "\n",
    "\n",
    "dat_roll = dat_roll[['article_number', 'country', 'season', 'year', 'week', 'net_qty', 'season_net_qty', 'art_desc', 'sports_cat_desc',\n",
    "           'rmh_cat_desc', 'franchise', 'gender_desc', 'age_group_desc',\n",
    "           'prod_grp_desc', 'prod_type_desc', 'price', 'margin', 'cost', 'buy_availability']]\n",
    "\n",
    "dat_roll = dat_roll[dat_roll.season_net_qty > 200]\n",
    "\n",
    "# dat_roll[['price', 'margin', 'cost']] = dat_roll[['price', 'margin', 'cost']] # .fillna(0).astype('int')\n",
    "\n",
    "\n",
    "\n",
    "dat_roll = (\n",
    "    dat_roll[['article_number', 'country', 'season', 'year', 'week', 'net_qty', 'buy_availability']].\n",
    "    sort_values(['article_number', 'country', 'year', 'week'])\n",
    ") \n",
    "\n",
    "\n",
    "dat_roll['buy_availability2'] = dat_roll.buy_availability.fillna(1) # assume full availability\n",
    "dat_roll['buy_availability2'] = np.where(dat_roll.buy_availability2 == 0, 1, dat_roll.buy_availability2) # replace 0 \n",
    "\n",
    "pd.crosstab(index = [dat_roll.season, dat_roll.buy_availability2], columns = 'counts')\n",
    "\n",
    "rollers = np.random.choice(dat_roll.article_number, size = 5, replace = False)\n",
    "\n",
    "dat_roll2 = dat_roll[dat_roll.article_number.isin(rollers)].copy()\n",
    "dat_roll2 = dat_roll2.sort_values(['article_number', 'country', 'season', 'year', 'week'])\n",
    "\n",
    "dat_roll2\n",
    "\n",
    "# Smooth buy_availability\n",
    "\n",
    "# Function\n",
    "def roll(df):\n",
    "    return df.rolling(window = 5, min_periods = 1, center = True).mean()\n",
    "\n",
    "dat_roll2['buy_availability3'] = dat_roll2.groupby(['article_number', 'country'])['buy_availability2'].apply(roll)\n",
    "# dat_roll2['buy_availability3'] = np.where(dat_roll2.buy_availability3 > 0.15, dat_roll2.buy_availability3, 0.15)\n",
    "\n",
    "\n",
    "# Corrected net_qty\n",
    "dat_roll2['corr_net_qty'] = (dat_roll2.net_qty / dat_roll2.buy_availability3).round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for pct in ['pred', 'pred_50', 'pred_60', 'pred_70', 'pred_75', 'pred_80', 'pred_90', 'pred_98']:\n",
    "#     col = 'DAA_profit' + '_' + pct\n",
    "#     dat_art4[col]  = dat_art4.apply(lambda row: P(row['season_net_qty'], row['margin'], row['cost'], row[pct]), axis=1)\n",
    "    \n",
    "                          "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
