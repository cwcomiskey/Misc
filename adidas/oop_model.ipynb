{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T12:12:49.847431Z",
     "start_time": "2019-12-12T12:12:48.826158Z"
    },
    "code_folding": [],
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load modules -- \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "# import dask.dataframe as ddf\n",
    "# import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 1700)\n",
    "\n",
    "import scipy\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.tsatools import detrend\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import oop_model as ac # Artem Chris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oop_model as ac \n",
    "\n",
    "ac\n",
    "\n",
    "x = ac.test()\n",
    "\n",
    "x.do_it()\n",
    "\n",
    "ac.test_function(9, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([1, 2, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T12:12:52.745400Z",
     "start_time": "2019-12-12T12:12:52.455804Z"
    },
    "code_folding": [
     0
    ],
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Season zero articles\n",
    "\n",
    "SSYY = 'FW19' # Season zero: SeasonSeasonYearYear\n",
    "\n",
    "buyer_table = pd.read_csv('data/EU_seasons.csv', low_memory = False, error_bad_lines = False, sep = \",\").sort_values(['article_number', 'brand', 'season']) # 26 Aug\n",
    "buyer_table = buyer_table[(buyer_table.season == SSYY)]\n",
    "# S0 = buyer_table[buyer_table.season == SSYY].article_number # Season zero articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T12:13:27.602728Z",
     "start_time": "2019-12-12T12:12:52.837823Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Forecasts -- \n",
    "SS20 = (pd.read_excel('data/DTC_Range_SS20.xlsx', sheet_name='Range', skiprows=5, \n",
    "    usecols=['Article Number (6 digits)', 'Carry Forward', 'eCom Market FC', 'Ecom Ranged RMA (PIM)'])\n",
    "        .rename(columns = {'Article Number (6 digits)': 'article_number'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T12:13:54.395678Z",
     "start_time": "2019-12-12T12:13:27.604286Z"
    },
    "code_folding": [
     0
    ],
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carryovers:  1033\n",
      "\n",
      "adi carryovers:  925\n",
      "rbk carryovers:  108\n"
     ]
    }
   ],
   "source": [
    "# adi + reebok + both -- \n",
    "\n",
    "# ----- adidas ------ \n",
    "\n",
    "# rma1_adi = pd.read_csv('data/article_range_rma1_adidas_fw20.csv', low_memory = False, error_bad_lines = False, sep = \",\", usecols = ['Article Number', 'Market Retail Price', 'WE eCom', 'eCom Range']) # (13618, 4)\n",
    "\n",
    "rma2_adi = pd.read_excel('data/RMA-02_ Market Range Plan_24102019.xlsx', sheet_name='BSO', skiprows=2) # adidas RMA2 -- 21 October\n",
    "\n",
    "rma2_adi = rma2_adi[['Article Number', 'WE eCom 06.11.2019', 'Market Retail Price']]\n",
    "\n",
    "rma2_adi = rma2_adi[rma2_adi['WE eCom 06.11.2019'] != 0]\n",
    "\n",
    "\n",
    "# ----- rbk ------ \n",
    "\n",
    "# rma1_rbk = pd.read_csv('data/article_range_rma1_reebok_fw20.csv', low_memory = False, error_bad_lines = False, sep = \",\", usecols = ['Article_number', 'Total Marketing Forecast', '  Total Net Sales  ', '  Total Inline Forecast  ']) # rma1_rbk.columns[40:]\n",
    "\n",
    "rma2_rbk = pd.read_excel('data/eCom Range Download_Reebok_FW20.xlsx') # Reebok RMA2 - 21 October\n",
    "rma2_rbk = rma2_rbk[['Article Number', 'RBK WE eCom', 'Hub Retail Price']]\n",
    "\n",
    "clssc_rbk = pd.read_csv('data/reebok_classics.csv') # Reebok RMA2 - 21 October\n",
    "\n",
    "addtl_classics = (\n",
    "    set(clssc_rbk.article_number).\n",
    "    symmetric_difference(rma2_rbk['Article Number']).\n",
    "    intersection(clssc_rbk.article_number)\n",
    "                 )\n",
    "\n",
    "# both -- \n",
    "\n",
    "carryovers = (set(buyer_table.article_number).\n",
    "              intersection(set(rma2_adi['Article Number'])).\n",
    "              union(set(buyer_table.article_number).\n",
    "                    intersection(set(rma2_rbk['Article Number'])))\n",
    "             )\n",
    "print('Carryovers: ', len(carryovers))\n",
    "print()\n",
    "\n",
    "print('adi carryovers: ', len(set(buyer_table.article_number).intersection(set(rma2_adi['Article Number']))))\n",
    "print('rbk carryovers: ', len(set(buyer_table.article_number).intersection(set(rma2_rbk['Article Number']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# impact evaluations -- \n",
    "\n",
    "rma2_adi['rev'] = rma2_adi['Sum of WE eCom'] * rma2_adi['Market Retail Price']\n",
    "\n",
    "rma2_adi['carryover'] = rma2_adi['Article Number'].isin(carryovers)\n",
    "\n",
    "print('Total range: ', len(rma2_adi['Article Number'].unique()))\n",
    "rma2_adi.head()\n",
    "rma2_adi.carryover.value_counts() # non/carryover\n",
    "\n",
    "# Revenue total\n",
    "rma2_adi.rev.sum()\n",
    "rma2_adi.groupby('carryover')['rev'].sum().round()\n",
    "\n",
    "rbk_carryovers = carryovers.union(addtl_classics)\n",
    "\n",
    "rma2_rbk['rev'] = rma2_rbk['RBK WE eCom'] * rma2_rbk['Hub Retail Price']\n",
    "\n",
    "rma2_rbk['carryover'] = rma2_rbk['Article Number'].isin(rbk_carryovers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T12:18:39.324606Z",
     "start_time": "2019-12-12T12:18:17.309221Z"
    },
    "code_folding": [
     0
    ],
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data -- \n",
    "dat0 = pd.read_csv('data/ch4k.csv')\n",
    "ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, error_bad_lines = False, \n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "                                  'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc',\n",
    "                                  'brand_desc', 'bus_unit_desc', 'rmh_cat_desc'])\n",
    "\n",
    "# dat0 = dat0[dat0.season.isin(['SS17', 'SS18', 'SS19'])]\n",
    "dat0 = dat0[dat0.season.isin(['FW17', 'FW18', 'FW19'])]\n",
    "\n",
    "# Remove clearance transactions\n",
    "# dat0['clearance'] = dat0.clearance.fillna(0) \n",
    "# dat0['net_qty'] = (1 - dat0.clearance)*dat0.net_qty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T12:18:39.351824Z",
     "start_time": "2019-12-12T12:18:39.325942Z"
    }
   },
   "outputs": [],
   "source": [
    "rbk_stealth = pd.read_excel('data/rbk_stealths.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T12:18:39.357852Z",
     "start_time": "2019-12-12T12:18:39.353342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Articles to make forecasts for\n",
    "articles_of_interest = rbk_stealth.article_number_FW19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# GAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T12:18:39.737606Z",
     "start_time": "2019-12-12T12:18:39.359329Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_GAS = dat0.copy()[['article_number', 'year', 'week', 'country', 'season', 'net_qty', 'buy_availability', 'brand']] \n",
    "\n",
    "# dat_GAS = dat_GAS[(dat_GAS.season == SSYY) & (dat_GAS.article_number.isin(carryovers))].sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# Only SSS19 articles **ALSO IN** SS20 range\n",
    "dat_GAS = (dat_GAS[(dat_GAS.season == SSYY) & (dat_GAS.article_number.isin(articles_of_interest))].\n",
    "           sort_values(['article_number', 'brand', 'country', 'year', 'week']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T12:19:10.118060Z",
     "start_time": "2019-12-12T12:18:42.230034Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_GAS = dat_GAS.groupby(['article_number', 'brand', 'country']).apply(ac.GAS_est).reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dat_GAS = pd.DataFrame(dat_GAS.groupby(['article_number', 'year', 'week'])['GAS_est'].sum()).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# With/out clearance -- \n",
    "\n",
    "# With clearance --\n",
    "# dat_GAS0 = pd.DataFrame(dat_GAS.groupby('article_number')['GAS_est'].sum().round())\n",
    "# dat_GAS0.head()\n",
    "\n",
    "# Without clearance --\n",
    "# dat_GAS1 = pd.DataFrame(dat_GAS.groupby('article_number')['GAS_est'].sum().round())\n",
    "# dat_GAS1.head()\n",
    "\n",
    "# dat_GAS_both = pd.merge(\n",
    "#     dat_GAS0,\n",
    "#     dat_GAS1,\n",
    "#     left_index = True, right_index = True\n",
    "# )\n",
    "\n",
    "# dat_GAS_both['diff'] = dat_GAS0['GAS_est'] - dat_GAS1['GAS_est']\n",
    "\n",
    "# dat_GAS_both['diff'].hist()\n",
    "# dat_GAS_both['diff'].describe().round()\n",
    "\n",
    "\n",
    "# dat_GAS_both['lt0'] = (dat_GAS_both['diff'] <= 0)*1\n",
    "\n",
    "# dat_GAS_both['lt0'].mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df, wrangle, calculate category means -- \n",
    "\n",
    "seasonality_dat = (\n",
    "    dat0[['article_number', 'brand', 'year', 'week', 'country', 'season', 'net_qty', 'sports_cat_desc', \n",
    "          'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].\n",
    "    dropna().\n",
    "    sort_values(['article_number', 'year', 'week']).\n",
    "    copy())\n",
    "\n",
    "seasonality_dat['net_qty2'] = seasonality_dat.groupby(['article_number', 'brand', 'season', 'year', 'week'])['net_qty'].transform(sum)\n",
    "seasonality_dat = seasonality_dat.drop(['brand', 'country', 'net_qty'], axis = 1).drop_duplicates().rename(columns = {'net_qty2': 'net_qty'})\n",
    "\n",
    "# -- Reliable, mirror seasons --\n",
    "seasonality_dat = seasonality_dat[~seasonality_dat.season.isin(['FW14', 'FW15', 'FW16', 'SS14','SS15', 'SS16'])] # Exclude these seasons\n",
    "\n",
    "# ---- Calculate cat-level weekly means across *ALL SEASONS* ---- \n",
    "seasonality_sport   = pd.DataFrame(seasonality_dat.groupby(['sports_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'sport_weekly_mean'})\n",
    "seasonality_rmh     = pd.DataFrame(seasonality_dat.groupby(['rmh_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'rmh_weekly_mean'})\n",
    "seasonality_gndr    = pd.DataFrame(seasonality_dat.groupby(['gender_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'gender_weekly_mean'})\n",
    "seasonality_agegrp  = pd.DataFrame(seasonality_dat.groupby(['age_group_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'age_weekly_mean'})\n",
    "seasonality_frnchse = pd.DataFrame(seasonality_dat.groupby(['franchise', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'franchise_weekly_mean'})\n",
    "seasonality_prdgrp  = pd.DataFrame(seasonality_dat.groupby(['prod_grp_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'prd_grp_weekly_mean'})\n",
    "\n",
    "seasonality_dfs = [seasonality_sport, seasonality_rmh, seasonality_gndr, seasonality_agegrp, seasonality_frnchse, seasonality_prdgrp]\n",
    "\n",
    "# seasonality_dat = seasonality_dat[(seasonality_dat.article_number.isin(carryovers))].sort_values(['article_number', 'year', 'week'])\n",
    "seasonality_dat = seasonality_dat[(seasonality_dat.article_number.isin(articles_of_interest))].sort_values(['article_number', 'year', 'week'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# regress articles of interest on seasonality\n",
    "preds = seasonality_dat.groupby(['article_number']).apply(regress).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Merge back with net_qty\n",
    "preds = (pd.merge(preds, seasonality_dat[seasonality_dat.season == SSYY], how = 'left').\n",
    "         sort_values(['article_number', 'year', 'week'])[['article_number', 'year', 'week', 'net_qty', 'seas_preds']]\n",
    "        )\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Zero out negative preds\n",
    "preds['seas_preds'] = np.where(preds.seas_preds > 0, preds.seas_preds, 0) \n",
    "\n",
    "# Combine: preds = All weeks --AND-- dat_GAS = observed weeks\n",
    "preds = pd.merge(preds, dat_GAS.round(), how = 'left', on =['article_number', 'year', 'week'])\n",
    "\n",
    "# preds = preds.astype({'year': 'int', 'net_qty': 'int', 'seas_preds': 'int', 'GAS_est': 'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Aggregate to Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eCom forecasts -- \n",
    "\n",
    "# rma2_rbk = rma2_rbk.rename(columns = {'RBK WE eCom': 'WE eCom'})\n",
    "# rma2_adi = rma2_adi.rename(columns = {'WE eCom 06.11.2019': 'WE eCom'})\n",
    "# rma2 = pd.concat([rma2_adi[['Article Number', 'WE eCom']], rma2_rbk[['Article Number', 'WE eCom']]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# weekly assignment of y_hat = f(observed, GAS, seasonality)\n",
    "\n",
    "preds['y_hat'] = np.where(np.isnan(preds.GAS_est), preds.seas_preds, (preds.GAS_est + preds.seas_preds)/2) # Evgeniy step\n",
    "\n",
    "preds['y_hat'] = np.where(preds.y_hat > preds.net_qty, preds.y_hat, preds.net_qty) # Risk management step (judgment call)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rma2_rbk = rma2_rbk.rename(columns = {'Article Number': 'article_number', 'RBK WE eCom': 'eCom Market FC'})\n",
    "rma2_rbk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sum over season\n",
    "preds_season = pd.DataFrame(preds.groupby('article_number')['y_hat', 'net_qty'].apply(sum).round())\n",
    "\n",
    "# Growth\n",
    "preds_season['y_hat'] = preds_season.y_hat * 1.1 # default growth rate\n",
    "\n",
    "# Combine DAA + eCom\n",
    "# preds_season = pd.merge(preds_season, SS20[['article_number', 'eCom Market FC']], how = 'left', on = 'article_number').round()\n",
    "preds_season = pd.merge(preds_season, rma2_rbk[['article_number', 'eCom Market FC']], how = 'left', on = 'article_number').round()\n",
    "\n",
    "\n",
    "# preds_season = pd.merge(preds_season.drop('WE eCom', axis = 1), rma2, how = 'left', left_on='article_number', right_on='Article Number').drop('Article Number', axis = 1).round()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add column with # of observed weeks\n",
    "preds_season = pd.merge(\n",
    "    preds_season, pd.DataFrame(preds[(~preds.net_qty.isna()) & (preds.net_qty > 0)].article_number.value_counts()).rename(columns = {'article_number': 'week_count'}),\n",
    "    how = 'left', left_on='article_number', right_index = True)\n",
    "\n",
    "# Retain articles with at least 4 observed weeks\n",
    "preds_season = preds_season[(preds_season.week_count > 3) & (preds_season.net_qty > 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preds_season.loc[:, 'diff'] = preds_season['eCom Market FC'] - preds_season['y_hat']\n",
    "\n",
    "# preds_season.sort_values('diff').set_index('article_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Magnifying glass -- \n",
    "aoi = 'G27706'\n",
    "a = aoi\n",
    "\n",
    "#dat0[dat0.article_number == aoi].sort_values(['country', 'year', 'week'])#.iloc[1,]\n",
    "\n",
    "\n",
    "preds_season[preds_season.article_number == a]\n",
    "\n",
    "dat_a = preds[preds.article_number == a]\n",
    "dat_a.sort_values(['year', 'week']).set_index(['year', 'week']).drop('article_number', axis = 1).apply(np.sum)\n",
    "\n",
    "dat_a[~dat_a.net_qty.isna()]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_a.sort_values(['year', 'week']).set_index(['year', 'week']).plot(linewidth = 3)\n",
    "\n",
    "dat_a.sort_values(['year', 'week']).set_index(['year', 'week'])\n",
    "\n",
    "# dat0[dat0.article_number == aoi].sort_values(['country', 'year', 'week'])#.iloc[1,]\n",
    "\n",
    "# plot -- \n",
    "dat_aoi = dat0[dat0.article_number == aoi].copy()\n",
    "dat_aoi = pd.merge(pd.DataFrame(dat_aoi.groupby(['year', 'week'])['net_qty'].sum()).reset_index(), dat_aoi[['year', 'week']].drop_duplicates())\n",
    "dat_aoi.year = [str(x) for x in dat_aoi.year]\n",
    "dat_aoi.week = [str(x) for x in dat_aoi.week]\n",
    "dat_aoi['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aoi.year, dat_aoi.week)]\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_aoi[['date', 'net_qty']].set_index('date').plot(linewidth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Overbuy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Load cost/margin data\n",
    "cost_margin = dat0.copy()\n",
    "# cost_margin = cost_margin[cost_margin.season.isin(['SS18', 'FW18', 'SS19', 'FW19'])]\n",
    "\n",
    "cost_margin = cost_margin[cost_margin.article_number.isin(articles_of_interest)]\n",
    "\n",
    "cost_margin = pd.DataFrame(cost_margin.groupby('article_number')['price', 'cost', 'margin'].mean().round(2)).dropna() # All but one NA is season_net_qty < 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add cost and margin for optimal overbuy estimation                                                                           \n",
    "preds_season = pd.merge(preds_season, cost_margin, how = 'left', left_on = 'article_number', right_index=True).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Opt_Ovb = preds_season.apply(lambda row: minimize_EL(row['y_hat'], 550 + 0.2*row['y_hat'], row['margin'], row['cost']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_season['Opt_Ovb'] = Opt_Ovb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Overbuy statistics ---\n",
    "# preds_season['pct_overbuy'] = (preds_season.Opt_Ovb - preds_season.y_hat)/preds_season.y_hat*100\n",
    "# b = np.array([0, 1000, 2000, 5000, 10000, 50000])\n",
    "# preds_season['bins'] = pd.cut(preds_season.y_hat, bins = b)\n",
    "# preds_season.groupby('bins')['pct_overbuy'].describe().round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbk_stealth[:2]\n",
    "rbk_stealth.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = pd.merge(preds_season.drop('eCom Market FC', axis = 1), rbk_stealth[['article_number_FW20', 'article_number_FW19']], \n",
    "                        left_on = 'article_number', right_on = 'article_number_FW19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = pd.merge(preds_season.drop('article_number_FW19', axis = 1), rma2_rbk, left_on='article_number_FW20', right_on='article_number').drop('article_number_y', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season['rev'] = preds_season['eCom Market FC']*preds_season['Hub Retail Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season[preds_season.article_number_FW20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Stealth carryovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "carryovers.intersection(['EE6147', 'B22716', 'EE6145', 'EE6146'])\n",
    "\n",
    "carryovers.intersection(['FW5947', 'FV5946', 'FV5943', 'FV5943'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "hideCode": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all -- \n",
    "\n",
    "stealth = pd.read_csv('data/stealth_carryovers_FW2020.csv', low_memory = False, error_bad_lines = False, sep = \";\")\n",
    "\n",
    "# stealth2 = pd.read_csv('data/stealth_carryovers_eu_RMA2.csv').rename(columns = {'article1': 'article', 'article2': 'stealth_article'})\n",
    "# stealth3 = pd.concat([stealth, stealth2])\n",
    "\n",
    "FW19_range = buyer_table.article_number.unique()\n",
    "FW20_range = set(rma2_adi['Article Number']).union(set(rma2_rbk['Article Number']))\n",
    "\n",
    "# New forecasts: IN FW20 --- NOT IN FW19\n",
    "additions = stealth[(stealth.article.isin(FW20_range)) & (~(stealth.article.isin(carryovers)))] \n",
    "\n",
    "find_me = additions.stealth_article\n",
    "\n",
    "dat_stealth = dat0[(dat0.article_number.isin(['BB9103', 'BB9104'])) & (dat0.season.isin(['FW18', 'FW17', 'FW16', 'FW15']))].copy()\n",
    "\n",
    "dat_stealth = dat_stealth[['article_number', 'year', 'week', 'country', 'season', 'net_qty', 'buy_availability']].sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# GAS step\n",
    "\n",
    "dat_GAS_stealth = dat_stealth.groupby(['article_number', 'country']).apply(GAS_est).reset_index()\n",
    "dat_GAS_stealth = pd.DataFrame(dat_GAS_stealth.groupby(['article_number', 'year', 'week'])['GAS_est'].sum()).reset_index()\n",
    "\n",
    "# SEASONALITY step\n",
    "seasonality_dat_stealth = (dat0[dat0.article_number.isin(['BB9103', 'BB9104'])].\n",
    "    copy()[['article_number', 'year', 'week', 'country', 'season', 'net_qty', \n",
    "            'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', \n",
    "            'age_group_desc', 'franchise', 'prod_grp_desc']].\n",
    "    dropna().sort_values(['article_number', 'year', 'week'])\n",
    "                  )\n",
    "\n",
    "# -- Sum over UK/EU, ADD article reference data --\n",
    "seasonality_dat_stealth = pd.merge(\n",
    "    pd.DataFrame(seasonality_dat_stealth.groupby(['article_number', 'season', 'year', 'week'])['net_qty'].sum()).reset_index(), # sum over UK & EU\n",
    "    seasonality_dat_stealth[['article_number', 'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].drop_duplicates() # add reference information\n",
    "    ).dropna().sort_values(['article_number', 'year', 'week'])\n",
    "\n",
    "seasonality_dat_stealth = seasonality_dat_stealth[seasonality_dat_stealth.season == 'FW18']\n",
    "\n",
    "preds_stealth = seasonality_dat_stealth.groupby(['article_number']).apply(regress).reset_index()\n",
    "\n",
    "# Merge back with reference data\n",
    "preds_stealth = pd.merge(\n",
    "    preds_stealth,\n",
    "    seasonality_dat_stealth[seasonality_dat_stealth.season == 'FW18'],\n",
    "    how = 'left').sort_values(['article_number', 'year', 'week'])[['article_number', 'year', 'week', 'net_qty', 'seas_preds']] # .fillna(method='ffill')\n",
    "\n",
    "\n",
    "# Zero out negative preds\n",
    "preds_stealth['seas_preds'] = np.where(preds_stealth.seas_preds > 0, preds_stealth.seas_preds, 0) \n",
    "\n",
    "\n",
    "\n",
    "# Combined observed weeks (partial season)  --- AND --- regression predicted (all) weeks\n",
    "preds_stealth = pd.merge(\n",
    "    preds_stealth,   # all weeks\n",
    "    dat_GAS_stealth, # observed weeks\n",
    "    how = 'left')\n",
    "\n",
    "\n",
    "\n",
    "# weekly assignment of GAS, seasonality, or combination\n",
    "preds_stealth['y_hat'] = np.where(np.isnan(preds_stealth.GAS_est), preds_stealth.seas_preds, (preds_stealth.GAS_est + preds_stealth.seas_preds)/2)\n",
    "\n",
    "\n",
    "\n",
    "# Sum over season\n",
    "preds_season_stealth = pd.DataFrame(preds_stealth.groupby('article_number')['y_hat'].apply(sum).round())\n",
    "\n",
    "\n",
    "\n",
    "# Growth\n",
    "preds_season_stealth['y_hat'] = preds_season_stealth.y_hat # * 1.1 # default growth rate\n",
    "\n",
    "# Match stealth to its carryover\n",
    "preds_season_stealth = pd.merge(\n",
    "    preds_season_stealth,\n",
    "    additions,\n",
    "    how = 'left', left_index = True, right_on = 'stealth_article'\n",
    ").drop('stealth_article', axis = 1).rename(columns = {'article': 'article_number'}).set_index('article_number')\n",
    "\n",
    "# Combine DAA + eCom\n",
    "preds_season_stealth = pd.merge(\n",
    "    preds_season_stealth.reset_index(),  # DAA forecasts\n",
    "    rma2,                                # eCom RMA2 forecast\n",
    "    how = 'left', left_on='article_number', right_on='Article Number'\n",
    ").drop('Article Number', axis = 1).round()\n",
    "\n",
    "# add price/cost for optimal overbuy\n",
    "preds_season_stealth = pd.merge(preds_season_stealth, cost_margin, how = 'left', left_on = 'article_number', right_index=True).round()\n",
    "\n",
    "\n",
    "\n",
    "# see evaluation.ipynb for sd estimation \n",
    "\n",
    "opt_ovb_stealth = pd.DataFrame(preds_season_stealth.\n",
    "                           apply(lambda row: minimize_EL(row['y_hat'], 550 + 0.2*row['y_hat'], row['margin'], row['cost']), axis=1)\n",
    "                          )\n",
    "\n",
    "opt_ovb_stealth = opt_ovb_stealth.rename(columns = {opt_ovb_stealth.columns[0]: 'Opt_Ovb'})\n",
    "\n",
    "# Combine with data\n",
    "preds_season_stealth = pd.merge(\n",
    "    preds_season_stealth, # everything\n",
    "    opt_ovb_stealth,      # optimal overbuy\n",
    "    right_index= True, left_index= True\n",
    ")\n",
    "\n",
    "# IMPACT\n",
    "# preds_season_stealth['impact'] = preds_season_stealth.apply(lambda row: np.abs(row['y_hat'] - row['WE eCom'])*(row['cost'] + row['margin']), axis = 1).round()                                                \n",
    "# preds_season_stealth = preds_season_stealth.sort_values('impact', ascending = False).round()\n",
    "             \n",
    "# Reorder for concatenating\n",
    "preds_season_stealth = preds_season_stealth[['article_number', 'y_hat', 'WE eCom', 'price', 'cost', 'margin', 'Opt_Ovb']]\n",
    "                                            \n",
    "                                            \n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine stealth with the rest\n",
    "\n",
    "preds_season = pd.concat([preds_season, preds_season_stealth], sort=True)[['article_number', 'y_hat', 'WE eCom', 'price', 'cost', 'margin', 'Opt_Ovb']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season.head()\n",
    "preds_season.shape\n",
    "\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Saving Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preds_season.to_excel('data/SS20_forecasts_inclusive.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dat = pd.read_excel('data/SS20_forecasts_all.xlsx')\n",
    "\n",
    "dat = dat[dat.net_forecast > dat.eCom_FC]\n",
    "\n",
    "dat = dat.set_index('article_number')\n",
    "\n",
    "dat.to_excel('data/SS20_forecasts_6Dec2019.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CLEARANCE y/n -- \n",
    "\n",
    "# INCLUDED --  \n",
    "\n",
    "# preds.to_excel('data/preds_all.xlsx')\n",
    "# preds_season.to_excel('data/preds_season_all.xlsx')\n",
    "\n",
    "\n",
    "# NOT included --\n",
    "\n",
    "# preds.to_excel('data/preds.xlsx')\n",
    "preds.head()\n",
    "\n",
    "# preds_season.to_excel('data/preds_season.xlsx')\n",
    "preds_season.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# --- READ IN non/all DATA --- \n",
    "\n",
    "# preds_all = pd.read_excel('data/preds_all.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "# preds_season_all = pd.read_excel('data/preds_season_all.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# preds = pd.read_excel('data/preds.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "# preds_season = pd.read_excel('data/preds_season.xlsx').drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Columns (add, rename, AA), Rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_season['pct_difference'] = ((preds_season.y_hat - preds_season['eCom Market FC'])/preds_season['eCom Market FC'] * 100).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# April Allen requested columns \n",
    "\n",
    "# aa_cols = pd.read_csv('data/columns_22october.csv', ...)\n",
    "# aa_cols = pd.read_csv('data/columns_11November.csv', ...)\n",
    "\n",
    "aa_cols = pd.read_csv('data/columns_21November.csv', low_memory = False, error_bad_lines = False, sep = \";\")\n",
    "aa_cols.at[0, 'FW19_total_market_FC'] = 400\n",
    "\n",
    "# Add leading zero to short article numbers\n",
    "for i in aa_cols.index:\n",
    "    if len(aa_cols.iloc[i]['article_number']) == 5:\n",
    "        aa_cols.at[i, 'article_number'] = '0' + aa_cols.iloc[i]['article_number']\n",
    "        \n",
    "aa_cols = aa_cols.drop_duplicates(subset = 'article_number') # One duplicate\n",
    "\n",
    "preds_season = pd.merge(preds_season, aa_cols, how = 'left', left_on = 'article_number', right_on = 'article_number')\n",
    "\n",
    "preds_season['FW19_total_ecom_SO'] = [np.float(str(x).replace('.', '')) for x in preds_season.FW19_total_ecom_SO] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ref_dat0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season = pd.merge(preds_season, ref_dat0[['article_no', 'model_no', 'art_desc', 'brand_desc',\n",
    "       'bus_unit_desc', 'rmh_cat_desc', 'prod_grp_desc', 'prod_type_desc',\n",
    "       'gender_desc', 'age_group_desc']], left_on='article_number', right_on='article_no').drop('article_no', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "preds_season = preds_season.rename(columns = {'y_hat': 'net_forecast', 'Opt_Ovb': 'buy_recommendation', 'art_desc': 'description', 'eCom Market FC': 'eCom_FC', \n",
    "                                              'prod_type_desc': 'type', 'brand_desc': 'brand', 'bus_unit_desc': 'BU', 'rmh_cat_desc': 'RMH'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_season = preds_season[['article_number',  'brand', 'model_no', 'description', 'type', 'BU', 'RMH', \n",
    "                             'price', 'cost', 'margin', 'net_forecast', 'buy_recommendation', 'eCom_FC']] # 'eCom_ILS1', \n",
    "                             #'FW19_total_market_FC', 'FW19_total_ecom_SO', 'FW19_total_ecom_RDP']] # space after RDP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season = preds_season.set_index('article_number')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Convert to integers to remove '.0' endings\n",
    "for c in preds_season.columns:\n",
    "    if type(preds_season[c][1]) == np.float64:\n",
    "        preds_season.loc[:, c] = preds_season[c].fillna(0).replace(np.inf, 0).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season = preds_season.replace('Error', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Managing 'inclusive' irregularities -- \n",
    "\n",
    "# preds_season.loc[:, 'impact'] = preds_season.apply(lambda row: np.abs(row['net_forecast'] - row['eCom_ILS1'])*(row['cost'] + row['margin']), axis = 1).round()                                                \n",
    "             \n",
    "# preds_season_all = pd.read_excel('data/preds_season_all.xlsx').drop('Unnamed: 0', axis = 1).rename(columns = {'y_hat': 'net_forecast_incl_clearance', 'Opt_Ovb': 'buy_rec_incl_clearance'})\n",
    "\n",
    "\n",
    "# preds_season.shape\n",
    "\n",
    "# preds_season_both = pd.merge(preds_season, preds_season_all[['article_number', 'net_forecast_incl_clearance', 'buy_rec_incl_clearance']], how = 'left')\n",
    "\n",
    "# preds_season_both = preds_season_both[['article_number', 'brand', 'model_no', 'description', 'type', 'BU', 'RMH', 'price', 'cost', 'margin', \n",
    "#                                        'net_forecast', 'buy_recommendation', 'net_forecast_incl_clearance', 'buy_rec_incl_clearance', 'eCom_ILS1', # 'pct_difference',\n",
    "#                                        'FW19_total_market_FC', 'FW19_total_ecom_SO', 'FW19_total_ecom_RDP']].set_index('article_number')\n",
    "\n",
    "# preds_season_both['buy_rec_incl_clearance'] = np.where(preds_season_both['buy_rec_incl_clearance'] > preds_season_both['buy_recommendation'], preds_season_both['buy_rec_incl_clearance'], preds_season_both['buy_recommendation'])\n",
    "# preds_season_both['net_forecast_incl_clearance'] = np.where(preds_season_both['net_forecast_incl_clearance'] > preds_season_both['net_forecast'], preds_season_both['net_forecast_incl_clearance'], preds_season_both['net_forecast'])\n",
    "\n",
    "# # Convert to integers to remove '.0' endings\n",
    "# for c in preds_season_both.columns:\n",
    "#     if type(preds_season_both[c][1]) == np.float64:\n",
    "#         preds_season_both.loc[:, c] = preds_season_both[c].fillna(0).replace(np.inf, 0).astype(int)\n",
    "\n",
    "\n",
    "# preds_season_both = preds_season_both[~preds_season_both.index.isin(unreliable)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load including-clearance forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preds_season_both.to_excel('FW20_forecasts_incl_cl.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preds_all = pd.read_excel('data/preds_all.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "# preds_season_all = pd.read_excel('data/preds_season_all.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# preds = pd.read_excel('data/preds.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "# preds_season = pd.read_excel('data/preds_season.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "preds_season_both = pd.read_excel('FW20_forecasts_incl_cl.xlsx')#.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season_both = preds_season_both.rename(columns = {'eCom_ILS1': 'eCom_21Nov'}).set_index('article_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preds_season_both['net_forecast_diff'] = preds_season_both['net_forecast_incl_clearance'] - preds_season_both['net_forecast']\n",
    "# preds_season_both.sort_values('net_forecast_diff', ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# adidas stealth eda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season_both = pd.read_excel('FW20_forecasts_incl_cl.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ref_dat0 = pd.read_csv('data/article_reference_data.csv', low_memory = False, error_bad_lines = False,\n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'brand_desc', 'colorway_long_descr',\n",
    "                                  'primary_color', 'secondary_color', 'tertiary_color', 'quarternary_color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ref_dat0.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Magnifying Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unreliable forecasts -- \n",
    "\n",
    "unreliable = list(['G26523', 'EJ9682', 'EE9391', 'EE8947', 'EE8943', 'BS0980', 'EE4727']) \n",
    "\n",
    "# preds_season = preds_season[~preds_season.article_number.isin(unreliable)]\n",
    "# preds_season = preds_season[~preds_season.index.isin(unreliable)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preds_season.loc[:, 'diff'] = preds_season['net_forecast'] - preds_season['eCom_FC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season.shape\n",
    "preds_season.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare DAA & eCom -- \n",
    "preds_season.loc[:, 'diff'] = preds_season['net_forecast'] - preds_season['eCom_FC']\n",
    "\n",
    "preds_season.sort_values('diff', ascending = False)[['net_forecast', 'buy_recommendation', 'eCom_FC',\n",
    "       'diff', 'brand', 'model_no', 'description', 'type', 'BU', 'RMH']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Magnifying glass - article \n",
    "\n",
    "aoi = 'S21489'\n",
    "a = aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magnifying class -- \n",
    "\n",
    "preds_season[preds_season.article_number == a][['article_number', 'net_forecast', 'buy_recommendation', \n",
    "        'eCom_FC', 'diff', 'brand', 'model_no', 'description', 'type', 'BU', 'RMH']]\n",
    "\n",
    "dat_a = preds[preds.article_number == a][['week', 'net_qty', 'corrected', 'y_hat']].round()\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_a.sort_values('week').set_index('week').plot(linewidth = 3)\n",
    "\n",
    "dat_a.set_index('week').apply(np.sum).round()\n",
    "dat_a.sort_values('week').set_index('week').round()\n",
    "\n",
    "# dat0[dat0.article_number == aoi].sort_values(['country', 'year', 'week'])#.iloc[1,]\n",
    "\n",
    "# plot -- \n",
    "dat_aoi = dat0[dat0.article_number == aoi].copy()\n",
    "dat_aoi = pd.merge(pd.DataFrame(dat_aoi.groupby(['year', 'week'])['net_qty'].sum()).reset_index(), dat_aoi[['year', 'week']].drop_duplicates())\n",
    "dat_aoi.year = [str(x) for x in dat_aoi.year]\n",
    "dat_aoi.week = [str(x) for x in dat_aoi.week]\n",
    "dat_aoi['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aoi.year, dat_aoi.week)]\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_aoi[['date', 'net_qty']].set_index('date').plot(linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# ---- Comments ---- \n",
    "\n",
    "if preds_season.index.name != 'article_number':\n",
    "    preds_season = preds_season.set_index('article_number')\n",
    "\n",
    "preds_season['notes'] = '-'\n",
    "preds_season.loc['EF0371', 'notes'] = 'Short FW19; our forecast assumes full FW20, with performance on par with observed FW19'\n",
    "preds_season.loc['EE7570', 'notes'] = 'Short FW19, so not a lot of data to work with. Our forecast assumes full FW20, with performance on par with observed FW19'\n",
    "preds_season.loc['F36641', 'notes'] = 'Forecast assumes FULL availability throughout FULL FW20'\n",
    "preds_season.loc['EE6999', 'notes'] = 'late drop, our forecast assumes a full FW20'\n",
    "preds_season.loc['CG6193', 'notes'] = 'Seasonality component of model is predicting strong end to FW19'\n",
    "preds_season.loc['EE6464', 'notes'] = 'Late drop in FW19, but our forecast assumes full FW20.'\n",
    "preds_season.loc['EE7773', 'notes'] = 'Shortened FW19, but forecast assumes full FW20.'\n",
    "preds_season.loc['EE7775', 'notes'] = 'Short FW19, but our forecast assumes full FW20'\n",
    "\n",
    "# preds_season.loc['DV0152', 'notes'] = 'All FW19 transactions are clearance.'\n",
    "# preds_season.loc['DV0169', 'notes'] = 'All FW19 transactions are clearance.'\n",
    "# preds_season.loc['DV2848', 'notes'] = 'Lots of clearance transactions (out of scope) pushed our numbers down'\n",
    "\n",
    "# preds_season.loc['x', 'notes'] =\n",
    "# preds_season.loc['x', 'notes'] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ---- Comments ---- \n",
    "\n",
    "if preds_season_both.index.name != 'article_number':\n",
    "    preds_season_both = preds_season_both.set_index('article_number')\n",
    "\n",
    "preds_season_both['notes'] = '-'\n",
    "preds_season_both.loc['EF0371', 'notes'] = 'Short FW19; our forecast assumes full FW20, with performance on par with observed FW19'\n",
    "preds_season_both.loc['EE7570', 'notes'] = 'Short FW19, so not a lot of data to work with. Our forecast assumes full FW20, with performance on par with observed FW19'\n",
    "preds_season_both.loc['F36641', 'notes'] = 'Forecast assumes FULL availability throughout FULL FW20'\n",
    "preds_season_both.loc['EE6999', 'notes'] = 'late drop, our forecast assumes a full FW20'\n",
    "preds_season_both.loc['CG6193', 'notes'] = 'Seasonality component of model is predicting strong end to FW19'\n",
    "preds_season_both.loc['EE6464', 'notes'] = 'Late drop in FW19, but our forecast assumes full FW20.'\n",
    "preds_season_both.loc['EE7773', 'notes'] = 'Shortened FW19, but forecast assumes full FW20.'\n",
    "preds_season_both.loc['EE7775', 'notes'] = 'Short FW19, but our forecast assumes full FW20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save -- \n",
    "\n",
    "# preds_season.to_excel(\"FW20_forecasts.xlsx\")\n",
    "# preds_season_both.to_excel('FW20_forecasts_incl_cl.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Non-clearance vs. All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Investigate w/ one article focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dat0 = pd.read_csv('data/ch4k.csv')\n",
    "ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, error_bad_lines = False, \n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "                                  'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc',\n",
    "                                  'brand_desc', 'bus_unit_desc', 'rmh_cat_desc'])\n",
    "\n",
    "# All transactions\n",
    "dat_all = dat0.copy()\n",
    "\n",
    "# Non-clearance transactions\n",
    "\n",
    "# dat = dat0.copy()\n",
    "# dat['clearance'] = dat.clearance.fillna(0) \n",
    "# dat['net_qty'] = (1 - dat.clearance)*dat.net_qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aoi = 'EE7570'\n",
    "\n",
    "#dat_eda = dat[(dat.article_number.isin([aoi])) & (dat.season.isin(['FW19', 'FW18', 'FW17', 'FW16', 'FW15']))].copy()\n",
    "dat_eda = dat_all[(dat_all.article_number.isin([aoi])) & (dat_all.season.isin(['FW19', 'FW18', 'FW17', 'FW16', 'FW15']))].copy()\n",
    "\n",
    "dat_eda = dat_eda[['article_number', 'year', 'week', 'country', 'season', 'net_qty', 'buy_availability']].sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# GAS step\n",
    "\n",
    "dat_GAS_eda = dat_eda.groupby(['article_number', 'country']).apply(GAS_est).reset_index()\n",
    "dat_GAS_eda = pd.DataFrame(dat_GAS_eda.groupby(['article_number', 'year', 'week'])['GAS_est'].sum()).reset_index()\n",
    "\n",
    "# both = pd.read_excel('data/both.xlsx')\n",
    "# both['diff'] = both.apply(lambda row: row['net_forecast_y'] - row['net_forecast_x'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# SEASONALITY step\n",
    "\n",
    "seasonality_dat_eda = (dat_all[['article_number', 'year', 'week', 'country', 'season', 'net_qty', \n",
    "                                'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', \n",
    "                                'age_group_desc', 'franchise', 'prod_grp_desc']].\n",
    "                       dropna().\n",
    "                       sort_values(['article_number', 'year', 'week']).\n",
    "                       copy()\n",
    "                  )\n",
    "\n",
    "# -- Sum over UK/EU, ADD article reference data --\n",
    "seasonality_dat_eda = pd.merge(\n",
    "    pd.DataFrame(seasonality_dat_eda.groupby(['article_number', 'season', 'year', 'week'])['net_qty'].sum()).reset_index(), # sum over UK & EU\n",
    "    seasonality_dat_eda[['article_number', 'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].drop_duplicates() # add reference information\n",
    "    ).dropna().sort_values(['article_number', 'year', 'week'])\n",
    "\n",
    "\n",
    "\n",
    "# -- Reliable, mirror seasons --\n",
    "seasonality_dat_eda = seasonality_dat_eda[seasonality_dat_eda.season.isin(['FW15', 'FW16', 'FW17', 'FW18', 'FW19'])]\n",
    "\n",
    "# ---- Calculate cat-level weekly means across *ALL SEASONS* ---- \n",
    "\n",
    "seasonality_sport   = pd.DataFrame(seasonality_dat_eda.groupby(['sports_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'sport_weekly_mean'})\n",
    "seasonality_rmh     = pd.DataFrame(seasonality_dat_eda.groupby(['rmh_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'rmh_weekly_mean'})\n",
    "seasonality_gndr    = pd.DataFrame(seasonality_dat_eda.groupby(['gender_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'gender_weekly_mean'})\n",
    "seasonality_agegrp  = pd.DataFrame(seasonality_dat_eda.groupby(['age_group_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'age_weekly_mean'})\n",
    "seasonality_frnchse = pd.DataFrame(seasonality_dat_eda.groupby(['franchise', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'franchise_weekly_mean'})\n",
    "seasonality_prdgrp  = pd.DataFrame(seasonality_dat_eda.groupby(['prod_grp_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'prd_grp_weekly_mean'})\n",
    "\n",
    "seasonality_dfs = [seasonality_sport, seasonality_rmh, seasonality_gndr, seasonality_agegrp, seasonality_frnchse, seasonality_prdgrp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "seasonality_dat_eda = seasonality_dat_eda[(seasonality_dat_eda.season == 'FW19') & (seasonality_dat_eda.article_number == aoi)]\n",
    "\n",
    "\n",
    "\n",
    "preds_eda = seasonality_dat_eda.groupby(['article_number']).apply(regress).reset_index()\n",
    "\n",
    "# Merge back with reference data\n",
    "preds_eda = pd.merge(\n",
    "    preds_eda,\n",
    "    seasonality_dat_eda,\n",
    "    how = 'left').sort_values(['article_number', 'week'])[['article_number', 'year', 'week', 'net_qty', 'seas_preds']] # .fillna(method='ffill')\n",
    "\n",
    "# Zero out negative preds\n",
    "preds_eda['seas_preds'] = np.where(preds_eda.seas_preds > 0, preds_eda.seas_preds, 0) \n",
    "\n",
    "# Combined observed weeks (partial season)  --- AND --- regression predicted (all) weeks\n",
    "preds_eda = pd.merge(\n",
    "    preds_eda,   # all weeks\n",
    "    dat_GAS_eda, # observed weeks\n",
    "    how = 'left')\n",
    "\n",
    "# weekly assignment of GAS, seasonality, or combination\n",
    "preds_eda['y_hat'] = np.where(np.isnan(preds_eda.GAS_est), preds_eda.seas_preds, (preds_eda.GAS_est + preds_eda.seas_preds)/2).round()\n",
    "\n",
    "# Sum over season\n",
    "# preds_season_eda = pd.DataFrame(preds_eda.groupby('article_number')['y_hat'].apply(sum).round())\n",
    "\n",
    "# Growth\n",
    "# preds_season_eda['y_hat'] = preds_season_eda.y_hat # * 1.1 # default growth rate\n",
    "                                           \n",
    "                                            \n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# EE7570_0 = preds_eda.copy()\n",
    "EE7570_0.head()\n",
    "\n",
    "# EE7570_1 = preds_eda.copy()\n",
    "\n",
    "EE7570_0.head()\n",
    "EE7570_1.head()\n",
    "\n",
    "both[both.article_number == aoi].head()\n",
    "\n",
    "both.shape\n",
    "# both[['net_forecast_x', 'net_forecast_y', 'diff']]\n",
    "\n",
    "both['diff'].describe().round()\n",
    "both['diff'].hist(bins = [-2500, 0, 1000, 12000])\n",
    "# without clearance ---- with clearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# both.to_excel('data/both.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Compare, Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "both = pd.read_excel('data/SS20_forecasts_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preds_season = pd.read_excel('data/SS20_forecasts_5Dec2019.xlsx')#.drop('Unnamed: 0', axis = 1)\n",
    "# preds_season_all = pd.read_excel('data/preds_season_all.xlsx').drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preds_season_all0 = preds_season.copy()\n",
    "# preds_season_all = preds_season.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds_season_all = preds_season_all.rename(columns = {'net_forecast': 'net_forecast_w_clearance', 'buy_recommendation': 'buy_recommendation_w_clearance',})\n",
    "\n",
    "both = pd.merge(preds_season, preds_season_all[['article_number', 'net_forecast_w_clearance', 'buy_recommendation_w_clearance']],\n",
    "                left_on='article_number', right_on = 'article_number')\n",
    "\n",
    "both['buy_recommendation_w_clearance'] = np.where(both['buy_recommendation_w_clearance'] > both['buy_recommendation'], both['buy_recommendation_w_clearance'], both['buy_recommendation'])\n",
    "both['net_forecast_w_clearance'] = np.where(both['net_forecast_w_clearance'] > both['net_forecast'], both['net_forecast_w_clearance'], both['net_forecast'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# both['net_forecast_diff'] = both['net_forecast_w_clearance'] - both['net_forecast']\n",
    "both['buy_rec_diff'] = both['buy_recommendation_w_clearance'] - both['buy_recommendation']\n",
    "\n",
    "both = both.sort_values('buy_rec_diff', ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove clearance transactions\n",
    "dat3 = dat0.copy()\n",
    "dat3['clearance'] = dat3.clearance.fillna(0) \n",
    "dat3['net_qty0'] = (1 - dat3.clearance)*dat3.net_qty\n",
    "\n",
    "both = pd.merge(both, dat3[dat3.article_number.isin(both.article_number) & (dat3.season == 'SS19')].groupby('article_number')['net_qty0', 'net_qty'].sum().astype('int'), \n",
    "         left_on = 'article_number', right_on = 'article_number').rename(columns = {'net_qty0': 'SS19_net_qty0', 'net_qty': 'SS19_net_qty'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "both.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "both[['article_number', 'description', \n",
    "       'RMH', 'SS19_net_qty0', 'SS19_net_qty', 'net_forecast', 'buy_recommendation',\n",
    "       'net_forecast_w_clearance', 'buy_recommendation_w_clearance', 'eCom_FC',\n",
    "       'buy_rec_diff']]#.to_excel('data/forecast_eda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "both[['article_number',  'brand', 'model_no', 'description', 'type', 'BU', 'RMH', \n",
    "        'price', 'cost', 'margin', 'net_forecast', 'buy_recommendation', 'net_forecast_w_clearance', 'buy_recommendation_w_clearance', 'eCom_FC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "both = both#[['article_number',  'brand', 'model_no', 'description', 'type', 'BU', 'RMH', \n",
    "        #'price', 'cost', 'margin', 'net_forecast', 'buy_recommendation', 'net_forecast_w_clearance', 'buy_recommendation_w_clearance', 'eCom_FC']].set_index('article_number')\n",
    "\n",
    "# both.to_excel('data/SS20_forecasts_all.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # adidas v rbk\n",
    "# from pandas import DataFrame, Series\n",
    "# fw20['brand'].value_counts()\n",
    "\n",
    "# # Carryover coverage\n",
    "# fw20['rev'] = fw20.price * fw20.eCom_RMA1\n",
    "# fw20.groupby('brand')['rev'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- Plot -----\n",
    "# a = np.random.choice(preds.article_number.unique(), size = 1, replace = False)[0]\n",
    "aoi = 'DP2405'\n",
    "a = aoi\n",
    "\n",
    "dat_a = preds[preds.article_number == a][['week', 'net_qty', 'GAS_est', 'seas_preds']]\n",
    "        \n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_a.sort_values('week').set_index('week').plot(linewidth = 3)\n",
    "dat_a.sort_values('week').set_index('week').round()\n",
    "\n",
    "dat_a[['GAS_est', 'net_qty', 'seas_preds']].apply(np.sum).round()\n",
    "\n",
    "preds_season.reset_index()[preds_season.index == a]\n",
    "\n",
    "dat_aoi = dat0[dat0.article_number == aoi].copy()\n",
    "\n",
    "dat_aoi = pd.merge(\n",
    "    pd.DataFrame(dat_aoi.groupby(['year', 'week'])['net_qty'].sum()).reset_index(),\n",
    "    dat_aoi[['year', 'week']].drop_duplicates()\n",
    ")\n",
    "\n",
    "dat_aoi.year = [str(x) for x in dat_aoi.year]\n",
    "dat_aoi.week = [str(x) for x in dat_aoi.week]\n",
    "dat_aoi['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aoi.year, dat_aoi.week)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_aoi[['date', 'net_qty']].set_index('date').plot(linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### ================= ARIMAX =================\n",
    "\n",
    "pd.DataFrame(preds)\n",
    "\n",
    "# Subset for troubleshooting\n",
    "a = np.random.choice(seasonality_dat.article_number.unique(), size = 1, replace = False)\n",
    "dat_a = seasonality_dat[seasonality_dat.article_number.isin(a)]\n",
    "\n",
    "\n",
    "y = dat_a[['net_qty', 'week']].set_index('week')\n",
    "\n",
    "x_sport   = seasonality_sport[seasonality_sport.sports_cat_desc == dat_a.sports_cat_desc.unique()[0]].set_index('week')\n",
    "x_rmh     = seasonality_rmh[seasonality_rmh.rmh_cat_desc == dat_a.rmh_cat_desc.unique()[0]].set_index('week')\n",
    "x_gndr    = seasonality_gndr[seasonality_gndr.gender_desc == dat_a.gender_desc.unique()[0]].set_index('week')\n",
    "x_agegrp  = seasonality_agegrp[seasonality_agegrp.age_group_desc == dat_a.age_group_desc.unique()[0]].set_index('week')\n",
    "x_frnchse = seasonality_frnchse[seasonality_frnchse.franchise == dat_a.franchise.unique()[0]].set_index('week')\n",
    "x_prdgrp  = seasonality_prdgrp[seasonality_prdgrp.prod_grp_desc == dat_a.prod_grp_desc.unique()[0]].set_index('week')\n",
    "\n",
    "yX = (pd.merge(y, x_sport, left_index=True, right_index=True, how = 'outer').\n",
    "      merge(x_rmh, left_index=True, right_index=True, how = 'outer').\n",
    "      merge(x_gndr, left_index=True, right_index=True, how = 'outer').\n",
    "      merge(x_agegrp, left_index=True, right_index=True, how = 'outer').\n",
    "      merge(x_frnchse, left_index=True, right_index=True, how = 'outer').\n",
    "      merge(x_prdgrp, left_index=True, right_index=True, how = 'outer').\n",
    "      drop(['sports_cat_desc', 'rmh_cat_desc', 'gender_desc',\n",
    "           'age_group_desc', 'franchise', 'prod_grp_desc'], axis = 1))\n",
    "\n",
    "yw = dat[dat.season == 'SS19'][['year', 'week']].drop_duplicates().sort_values(['year', 'week'])\n",
    "\n",
    "yw.loc[:,'YEAR'] = [str(x)[:-2] for x in yw.year]\n",
    "yw.loc[:,'WEEK'] = [str(x)[:-2] for x in yw.week]\n",
    "\n",
    "yw.loc[:,'date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(yw.YEAR, yw.WEEK)]\n",
    "\n",
    "yX = (pd.merge(yX, yw[['week', 'date']], left_index=True, right_on = 'week').\n",
    "      drop('week', axis = 1).\n",
    "      sort_values('date').\n",
    "      set_index('date'))\n",
    "\n",
    "\n",
    "reg = pd.merge(\n",
    "    regress(dat_a),\n",
    "    yw,\n",
    "    left_index=True, right_on = 'week',\n",
    ").set_index('date')\n",
    "\n",
    "reg\n",
    "\n",
    "# yX.index\n",
    "\n",
    "# r = pd.date_range(start=yX.index.min(), end=yX.index.max(), freq = 'W')\n",
    "# r\n",
    "\n",
    "# yX #.reindex(r) #.fillna(0.0) # .rename_axis('dt').reset_index()\n",
    "\n",
    "arimax = sm.tsa.statespace.SARIMAX(yX.net_qty,\n",
    "                                   order = (1,0,1),\n",
    "                                   seasonal_order = (0,0,0,0),\n",
    "                                   exog = yX.drop('net_qty', axis = 1),\n",
    "                                   enforce_stationarity=False, \n",
    "                                   enforce_invertibility=False,\n",
    "                                   missing = 'drop').fit()\n",
    "\n",
    "# arimax.summary()\n",
    "preds = arimax.predict()\n",
    "yX = pd.merge(\n",
    "    yX, \n",
    "    pd.DataFrame(preds), right_index=True, left_index=True,\n",
    ")\n",
    "\n",
    "yX.columns\n",
    "yX.iloc[:,7]\n",
    "\n",
    "numerator = ((yX.net_qty - preds) ** 2).sum()\n",
    "denominator = ((yX.net_qty - np.mean(yX.net_qty)) ** 2).sum()\n",
    "1 - numerator/denominator\n",
    "\n",
    "\n",
    "\n",
    "p = pd.DataFrame(index = yX.index)\n",
    "p['net_qty'] = yX.net_qty\n",
    "p['preds'] = arimax.predict()\n",
    "p['original'] = yX.iloc[:,7]\n",
    "p.plot()\n",
    "\n",
    "### ================= End ARIMAXperiment ================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -- NA check --\n",
    "# seasonality_dat[pd.isnull(seasonality_dat).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EDA Plots\n",
    "\n",
    "preds_season = preds_season.reset_index()\n",
    "\n",
    "# ---- Plot -----\n",
    "# a = np.random.choice(preds.article_number.unique(), size = 1, replace = False)[0]\n",
    "aoi = 'DV1549'\n",
    "a = aoi\n",
    "\n",
    "dat_a = preds[preds.article_number == a][['week', 'net_qty', 'GAS_est', 'seas_preds']]\n",
    "        \n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_a.sort_values('week').set_index('week').plot(linewidth = 3)\n",
    "dat_a.sort_values('week').set_index('week').round()\n",
    "\n",
    "dat_a[['GAS_est', 'net_qty', 'seas_preds']].apply(np.sum).round()\n",
    "\n",
    "preds_season.reset_index()[preds_season.index == a]\n",
    "\n",
    "dat_aoi = dat0[dat0.article_number == aoi].copy()\n",
    "\n",
    "dat_aoi = pd.merge(\n",
    "    pd.DataFrame(dat_aoi.groupby(['year', 'week'])['net_qty'].sum()).reset_index(),\n",
    "    dat_aoi[['year', 'week']].drop_duplicates()\n",
    ")\n",
    "\n",
    "dat_aoi.year = [str(x) for x in dat_aoi.year]\n",
    "dat_aoi.week = [str(x) for x in dat_aoi.week]\n",
    "dat_aoi['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aoi.year, dat_aoi.week)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_aoi[['date', 'net_qty']].set_index('date').plot(linewidth = 4)\n",
    "\n",
    "# dat_aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# WE_eCom['WE eCom'] = [int(x.replace(\",\", \"\")) for x in WE_eCom['WE eCom']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # DAVID code\n",
    "\n",
    "# # Function Definitions\n",
    "\n",
    "# class parameters():\n",
    "#     def __init__(self, param_array):\n",
    "#         self.alpha = param_array[0]\n",
    "#         self.beta = param_array[1]\n",
    "#         self.omega = param_array[2] / (1-self.beta) # one way to choose that is omega/(1-beta) = uncoMnditional mean\n",
    "#         self.sigma = param_array[3]\n",
    "#         self.f0 = param_array[4] # one way to choose is unconditional mean\n",
    "\n",
    "# def loglik(x, y, f, sigma):\n",
    "#     ll = -1/2*np.log(2*np.pi ) - 1/2*np.log(sigma) - 1/(2*sigma)*(y - x*f)**2\n",
    "#     return ll\n",
    "\n",
    "# def score_compute(x, y, f, sigma):\n",
    "#     return (y - x*f)/sigma\n",
    "\n",
    "# def score_compute_2(x, y, f, sigma=None):\n",
    "#     return(y - x*f) # ** The 'type = 2' modification **\n",
    "\n",
    "# def filterGAS(p, x, y, score_fun):\n",
    "#     score0 = score_fun(x[0,:], y[0,:],p.f0, p.sigma)\n",
    "#     f = np.zeros((len(y),1))\n",
    "#     f[0,:] = p.f0\n",
    "#     for t in range(1,len(y)):\n",
    "#         scoret = score_fun(x[t-1,:], y[t-1,:], f[t-1,:], p.sigma)\n",
    "#         f[t,:] = p.omega + p.alpha*scoret + p.beta*f[t-1,:]\n",
    "#     return f\n",
    "\n",
    "\n",
    "# def loglikest(params, x, y, score_fun):\n",
    "#     p = parameters(params)\n",
    "#     f = filterGAS(p, x, y, score_fun)\n",
    "#     ll = np.zeros((len(y), 1))\n",
    "#     m = len(y)\n",
    "#     for t in range(0, len(y)):\n",
    "#         ll[t,:] = loglik(x[t,:], y[t,:], f[t,:], p.sigma)\n",
    "#     loglik_res = -(np.sum(ll))/m\n",
    "#     return loglik_res\n",
    "\n",
    "\n",
    "# def GAS_optimize(x, y, score_fun, marker_str):\n",
    "#     return scipy.optimize.minimize(\n",
    "#        loglikest,                              # function to minimize (log likelihood y|x,theta)\n",
    "#        np.array([0.8, 0.9, np.mean(y), 1, np.mean(y)]), # initial parameter values (starting)\n",
    "#        args=(x, y, score_fun),\n",
    "#        options ={'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12}, # TODO pass as parameter or create config file\n",
    "#        method='L-BFGS-B',\n",
    "#        bounds=((0,  None),             # alpha\n",
    "#                (-1, 1),                # beta\n",
    "#                (0.001, np.mean(y)*2),  # omega\n",
    "#                (0.001, None),          # sigma\n",
    "#                (0.001, np.mean(y)*2)   # f\n",
    "#               )\n",
    "#        )\n",
    "\n",
    "\n",
    "\n",
    "# def GAS_est(df):\n",
    "#     \"\"\" <High level description of function>\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df : pandas DataFrame\n",
    "#        <Description>\n",
    "#     Returns\n",
    "#     -------\n",
    "#     ret: pandas DataFrame\n",
    "#        <Description>\n",
    "#     Raises\n",
    "#     ------\n",
    "#     (List and description of specific errors generated and thrown based on intenal function requirements)\n",
    "#     OtherError when an other error\n",
    "#     \"\"\"\n",
    "#     y = df.net_qty.values.reshape(-1,1)          # observed demand (response)\n",
    "#     x = df.buy_availability.values.reshape(-1,1)   # buy_availability (explanatory)\n",
    "    \n",
    "#     ret = pd.DataFrame()\n",
    "#     ret[['year','week']] = df[['year','week']]\n",
    "    \n",
    "#     score_fun = score_compute\n",
    "#     marker_str = 'One'\n",
    "    \n",
    "#     opt_result = GAS_optimize(x, y, score_fun, marker_str)\n",
    "    \n",
    "#     if opt_result.success == False:\n",
    "#         score_fun= score_compute_2\n",
    "#         marker_str = 'Two'\n",
    "#         opt_result = GAS_optimize(x, y, score_fun, marker_str)\n",
    "        \n",
    "#     x1par = parameters(opt_result.x)\n",
    "#     GAS = filterGAS(x1par, x, y, score_fun)\n",
    "    \n",
    "#     ret['GAS_est'] = GAS\n",
    "#     ret['Convergence'] = [opt_result.success] * len(y)\n",
    "#     ret['Convg type'] = [marker_str] * len(y)\n",
    "    \n",
    "#     return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# April articles\n",
    "\n",
    "aas = ['DV1549', 'EE1152', 'DV1508', 'ED6024', 'CY4574', 'ED9384', 'BK7345', 'DV2400', 'DH5798']\n",
    "\n",
    "# aoi = 'DV1549'\n",
    "# a = aoi\n",
    "\n",
    "preds_aa = (preds[preds.article_number.isin(aas)][['article_number', 'week', 'net_qty', 'GAS_est', 'seas_preds', 'y_hat']].\n",
    "            sort_values(['article_number', 'week']).\n",
    "            set_index('week')).round()\n",
    "        \n",
    "\n",
    "preds_aa[preds_aa.article_number == 'DV1549']\n",
    "\n",
    "pred_aggs_aa = preds_aa.groupby('article_number')[['net_qty', 'GAS_est', 'seas_preds', 'y_hat']].apply(sum).round()\n",
    "\n",
    "\n",
    "\n",
    "for c in pred_aggs_aa.columns:\n",
    "    if type(pred_aggs_aa[c][1]) == np.float64:\n",
    "        pred_aggs_aa[c] = pred_aggs_aa[c].fillna(0).astype(int)\n",
    "\n",
    "pred_aggs_aa[~np.isnan(pred_aggs_aa.GAS_est)].loc[aas[i-1], 'y_hat'].sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25,32)); # width, height\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.2); # vertical spacing, horizontal spacing\n",
    "for i in range(1, 10):\n",
    "    ax = fig.add_subplot(5, 2, i, )\n",
    "    preds_ax = preds_aa[preds_aa.article_number == aas[i-1]]\n",
    "    ax.plot(preds_ax.index, preds_ax['net_qty'], linewidth=4.5, label = 'Observed net_qty')\n",
    "    ax.plot(preds_ax.index, preds_ax['y_hat'], linewidth=4.5, label = 'Model net_qty estimate')\n",
    "    ax.set_title('Article: ' + aas[i-1] + \n",
    "                 ' \\n net_qty STD: ' + str(pred_aggs_aa.loc[aas[i-1], 'net_qty']) + \n",
    "                 ' \\n Full availability estimate STD: ' + str(preds_aa[(~np.isnan(preds_aa.GAS_est)) & (preds_aa.article_number == aas[i-1])].y_hat.sum().astype(int)) +\n",
    "                 ' \\n FW19 full season estimate: ' + str(pred_aggs_aa.loc[aas[i-1], 'y_hat']), \n",
    "                 fontsize=16)\n",
    "    ax.legend()\n",
    "\n",
    "fig.savefig('aa_fig.png')    \n",
    "\n",
    "pass;\n",
    "\n",
    "\n",
    "\n",
    "preds_aa[['GAS_est', 'net_qty', 'seas_preds']].apply(np.sum).round()\n",
    "\n",
    "preds_season.reset_index()[preds_season.index.isin(aas)]\n",
    "\n",
    "dat_aa = dat0[dat0.article_number.isin(aas)].copy()\n",
    "\n",
    "dat_aa = pd.merge(\n",
    "    pd.DataFrame(dat_aa.groupby(['year', 'week'])['net_qty'].sum()).reset_index(),\n",
    "    dat_aa[['year', 'week']].drop_duplicates()\n",
    ")\n",
    "\n",
    "dat_aa.year = [str(x) for x in dat_aa.year]\n",
    "dat_aa.week = [str(x) for x in dat_aa.week]\n",
    "dat_aa['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aa.year, dat_aa.week)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_aa[['date', 'net_qty']].set_index('date').plot(linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------- Product Lifecycle EDA -------------------\n",
    "\n",
    "dat_lifespan = dat0.copy()\n",
    "\n",
    "# Just article & season\n",
    "dat_lifespan = dat_lifespan[['article_number', 'season']].drop_duplicates()\n",
    "\n",
    "# Lifespan\n",
    "dat_lifespan = pd.DataFrame(dat_lifespan.article_number.value_counts()).reset_index().rename(columns = {'index': 'article', 'article_number': 'lifespan'})\n",
    "\n",
    "# net_qty per season\n",
    "dat_lifespan2 = pd.DataFrame(dat0.groupby(['article_number', 'season'])['net_qty'].apply(sum)).reset_index()\n",
    "\n",
    "# Combine\n",
    "dat_lifespan = pd.merge(\n",
    "    dat_lifespan2,\n",
    "    dat_lifespan,\n",
    "    how = 'left', left_on= 'article_number', right_on = 'article'\n",
    ").drop('article', axis = 1)\n",
    "\n",
    "# --- For sorting ---\n",
    "key = {'SS14': 1, 'FW14': 2, 'SS15': 3, 'FW15': 4, \n",
    "       'SS16': 5, 'FW16': 6, 'SS17': 7, 'FW17': 8, \n",
    "       'SS18': 9, 'FW18': 10, 'SS19': 11, 'FW19': 12}\n",
    "dat_lifespan['order_key'] = [key[s] for s in dat_lifespan.season]\n",
    "\n",
    "dat_lifespan = dat_lifespan.sort_values(['article_number', 'order_key'])\n",
    "\n",
    "dat_lifespan.drop\n",
    "\n",
    "def lifecycle_season_assign(df):\n",
    "    df = df.sort_values('order_key')\n",
    "\n",
    "    ret = pd.DataFrame()\n",
    "    ret['season'] = df.season\n",
    "    ret['lifecycle_season'] = list(range(1, df.shape[0] + 1))\n",
    "\n",
    "    return ret\n",
    "\n",
    "lifecycle = dat_lifespan.groupby('article_number').apply(lifecycle_season_assign).reset_index().drop('level_1', axis = 1)\n",
    "\n",
    "dat_lifecycle = pd.merge(\n",
    "    dat_lifespan,\n",
    "    lifecycle\n",
    ")\n",
    "\n",
    "\n",
    "dat_lifecycle\n",
    "\n",
    "dat_lifecycle_medians = pd.DataFrame(dat_lifecycle.groupby(['lifespan', 'lifecycle_season'])['net_qty'].mean()).reset_index()\n",
    "\n",
    "dat_lifecycle_medians = dat_lifecycle_medians[~dat_lifecycle_medians.lifespan.isin([1, 12])]\n",
    "\n",
    "dat_lifecycle_medians\n",
    "\n",
    "t = dat_lifecycle_medians.copy()\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.lineplot(x = t.lifecycle_season, y = t.net_qty, hue= t.lifespan, palette=\"tab10\", linewidth=4).set_title('Means by Age, for Life-length Groups')\n",
    "\n",
    "# set_index(['lifespan', 'lifecycle_season'])\n",
    "\n",
    "\n",
    "\n",
    "sns.palplot(sns.color_palette())\n",
    "\n",
    "\n",
    "# Tweaks, Rules\n",
    "\n",
    "# preds['corrected_net_qty'] = (preds.GAS_est + preds.seas_preds)/2\n",
    "\n",
    "# preds['corrected_net_qty'] = np.where(preds.corrected_net_qty < preds.net_qty, preds.net_qty, preds.corrected_net_qty)\n",
    "\n",
    "# preds['y_hat'] = np.where(preds.y_hat < preds.net_qty, preds.net_qty, preds.y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preds_season.loc['G26535', 'notes'] = 'We have very few observed weeks for this article, due to late season launch; so at this stage the forecast for this article is unreliable.'\n",
    "# preds_season.loc['EC6229', 'notes'] = 'We have very few observed weeks for this article, due to late season launch; so at this stage the forecast for this article is unreliable.'\n",
    "# preds_season.loc['G26487', 'notes'] = 'We have very few observed weeks for this article, due to late season launch; so at this stage the forecast for this article is unreliable.'\n",
    "# preds_season.loc['G26523', 'notes'] = 'We have very few observed weeks for this article, due to late season launch; so at this stage the forecast for this article is unreliable.'\n",
    "# preds_season.loc['ED8712', 'notes'] = 'We have very few observed weeks for this article; so at this stage the forecast for this article is unreliable.'\n",
    "\n",
    "# preds_season.loc['ED8620', 'notes'] = 'We have very few observed weeks for this article; so at this stage the forecast for this article is unreliable.'\n",
    "# preds_season.loc['EJ9682', 'notes'] = 'With less than 10 weeks of observed net_qty for this article, one of which was quite large, the model is misbehaving and making pathological inferences --- this forecast is not reliable.'\n",
    "# preds_season.loc['BD7611', 'notes'] = 'We have almost no non-clearance data on this article, and our (nonsensical?) forecast reflects this; actual net_qty of ~700 is same order of magnitude as eCom forecast -- which is good.'\n",
    "# preds_season.loc['DV0170', 'notes'] = 'We have almost no non-clearance data on this article, and our (nonsensical?) forecast reflects this; actual net_qty of ~700 is same order of magnitude as eCom forecast -- which is good.'\n",
    "# preds_season.loc['EE9391', 'notes'] = 'Only a handful of observed FW19 weeks; not a realistic/useful forecast.'\n",
    "\n",
    "# preds_season.loc['EE8947', 'notes'] = 'Essentially no FW19 to work with, so our forecast is unrealistic; but better SS19, so your forecast seems reasonable.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ----- For demo ------\n",
    "\n",
    "fcsts = preds_season.copy()\n",
    "\n",
    "# fcsts = pd.read_excel('data/FW20_forecasts.xlsx') # adidas RMA2 -- 21 October\n",
    "\n",
    "\n",
    "fcsts.loc[:, 'impact'] = fcsts.apply(lambda row: np.where(row['buy_recommendation'] > row['eCom_ILS1'], \n",
    "                                                         (row['buy_recommendation'] - row['eCom_ILS1']) * row['margin'], \n",
    "                                                         (row['eCom_ILS1'] - row['buy_recommendation']) * row['cost']), axis = 1)\n",
    "\n",
    "fcsts = fcsts.sort_values('impact', ascending = False).round()\n",
    "\n",
    "fcsts = fcsts[['net_forecast', 'buy_recommendation', 'eCom_ILS1', 'impact', 'brand', 'description', 'type', 'BU', 'RMH', 'price', 'cost', 'margin',  'FW19_total_market_FC', 'notes']]\n",
    "\n",
    "\n",
    "fcsts.head()\n",
    "\n",
    "preds[preds.article_number == 'CG6708'].head()\n",
    "dat0[dat0.article_number == 'CG6708'].head()\n",
    "\n",
    "# EDA Plots\n",
    "\n",
    "# ---- Plot -----\n",
    "aoi = 'F34314'\n",
    "a = aoi\n",
    "\n",
    "# -------\n",
    "\n",
    "dat_a = preds[preds.article_number == a][['week', 'net_qty', 'GAS_est', 'y_hat']]\n",
    "dat_a['year'] = '2019'\n",
    "dat_a.week = [str(x) for x in dat_a.week]\n",
    "dat_a['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_a.year, dat_a.week)]\n",
    "\n",
    "# -------\n",
    "\n",
    "print('This season:')\n",
    "print(dat_a[['net_qty', 'GAS_est', 'y_hat']].apply(np.sum).round())\n",
    "\n",
    "dat_aoi = dat0[dat0.article_number == aoi].copy()\n",
    "\n",
    "dat_aoi = pd.merge(pd.DataFrame(dat_aoi.groupby(['year', 'week'])['net_qty'].sum()).reset_index(),dat_aoi[['year', 'week']].drop_duplicates())\n",
    "dat_aoi.year = [str(x) for x in dat_aoi.year]\n",
    "dat_aoi.week = [str(x) for x in dat_aoi.week]\n",
    "dat_aoi['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aoi.year, dat_aoi.week)]\n",
    "\n",
    "# --------\n",
    "\n",
    "full = pd.merge(dat_a, dat_aoi, how = 'outer').sort_values('date').set_index('date')\n",
    "\n",
    "full = full[~((full.index > dt.datetime(2019, 5, 27))  & full.y_hat.isna())]\n",
    "# full = full.drop_duplicates(subset = ['week'], keep = 'last')\n",
    "\n",
    "full = full.rename(columns = {'y_hat': 'model net_qty'})\n",
    "\n",
    "# --------\n",
    "\n",
    "full.loc[dt.datetime(2019, 11, 4), 'net_qty'] = np.nan # manually change entry\n",
    "\n",
    "full # ************\n",
    "\n",
    "# ----\n",
    "plt.rcParams[\"figure.figsize\"] = [20,8]\n",
    "full.drop('GAS_est', axis = 1).plot(linewidth = 3)\n",
    "plt.ylabel('net_qty')\n",
    "plt.title('Article net_qty: ' + aoi)\n",
    "\n",
    "# -------\n",
    "\n",
    "fcsts[fcsts.index == aoi]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# dat_GAS_a = np.random.choice(dat_GAS.article_number.unique(), size = 100, replace = False)\n",
    "# dat_GAS = dat_GAS[dat_GAS.article_number.isin(dat_GAS_a)]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
