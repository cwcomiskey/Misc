{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from scipy import optimize\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal density plot\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "\n",
    "mu = 38000\n",
    "# variance = 1\n",
    "sigma = 7000 # math.sqrt(variance)\n",
    "x = np.linspace(mu - 3.5*sigma, mu + 3.5*sigma, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma), linewidth = 4)\n",
    "\n",
    "plt.title('CONTIENTAL 80 Prediction Distribution', size = 18)\n",
    "plt.xlabel('Net Demand Quantity', size = 18)\n",
    "plt.ylabel('Probability Density', size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.g. Stan Smith: Normal prediction distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# e.g. Stan Smith Prediction Distribution \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 3]\n",
    "\n",
    "mu = 38000\n",
    "sigma = 5000 # math.sqrt(variance)\n",
    "x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma), linewidth = 4)\n",
    "\n",
    "plt.title('Stan Smith Prediction Distribution', size = 12)\n",
    "plt.xlabel('Net Demand Quantity', size = 12)\n",
    "plt.ylabel('Probability Density', size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss --- demand, buy, margin, cost\n",
    "def L(d, b, margin, cost):\n",
    "    if d > b:\n",
    "        return (d - b)*margin\n",
    "    elif d < b:\n",
    "        return (b - d)*cost\n",
    "    elif d == b:\n",
    "        return 0\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "# E[L | buy, article_mean, article_sd, article_margin, article_cost]\n",
    "def EL(b_0, mu_0, sigma_0, margin_0, cost_0):\n",
    "    I = lambda x: L(x, b_0, margin_0, cost_0) * stats.norm.pdf(x, mu_0, sigma_0) # I for integrand\n",
    "    Exp_loss = integrate.quad(I, mu_0 - 4*sigma_0, mu_0 + 4*sigma_0)\n",
    "    return round(Exp_loss[0], 2) \n",
    "\n",
    "\n",
    "# return buy_qty that minimizes expected loss\n",
    "def min_expected_loss(mu, sigma, margin, cost):\n",
    "    \n",
    "    buys = list(range(mu - 1*sigma, mu + 3*sigma, 50)) # buy qtys for which calculate E[L]\n",
    "    ELs = [EL(b, mu, sigma, margin, cost) for b in buys] # E[L|b] for b in buys  \n",
    "    min_loss_index = ELs.index(min(ELs)) # index of buy qty that minimizes E[L]\n",
    "    \n",
    "    return buys[min_loss_index]          # buy qty that minimized E[l]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stan Smith: Prediction, E[L] minimizing buy, comparison \n",
    "\n",
    "mu = 38000\n",
    "sigma = 7000\n",
    "margin = 65.80\n",
    "cost = 4.20\n",
    "\n",
    "buys = list(range(mu - 1*sigma, mu + 3*sigma, 50)) # buy qtys for which calculate E[L]\n",
    "ELs = [EL(b, mu, sigma, margin, cost) for b in buys]\n",
    "\n",
    "print('Buying', buys[ELs.index(min(ELs))], 'units minimizes expected loss') # buy qty that minimizes expected loss\n",
    "print()\n",
    "print('Minimized expected loss:', min(ELs)) # minimized Expected loss\n",
    "print()\n",
    "print('Expected loss with 20% buffer approach:', EL(38000*1.2, 38000, 7000, 65.80, 4.2))\n",
    "print()\n",
    "print('Expected profit increase:', round(EL(buys[ELs.index(min(ELs))], 38000, 7000, 65.80, 4.20) - EL(38000*1.2, 38000, 7000, 65.80, 4.2), 2))\n",
    "\n",
    "# Expected loss against function of buy qty\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "plt.plot(buys, ELs, linewidth = 3)\n",
    "\n",
    "plt.title('Expected Loss vs. Buy Quantity', size = 18)\n",
    "plt.xlabel('Buy Quantity', size = 18)\n",
    "plt.ylabel('Expected Loss', size = 18)\n",
    "\n",
    "min_expected_loss(mu, sigma, margin, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(loc=0.0, scale=1.0, size=1000) # NORMAL\n",
    "plt.hist(X, density = True)\n",
    "\n",
    "x = np.linspace(-4, 4, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1), linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov test\n",
    "stats.kstest(X, 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from matplotlib import pyplot\n",
    "\n",
    "qqplot(X, line='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric prediction distribution; e.g. Continental 80s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = np.random.gamma(shape = 2, scale = 200, size = 1000) + 600 # GAMMA\n",
    "# plt.hist(c, density = True, bins = 50)\n",
    "\n",
    "shape = 5\n",
    "scale = 2000\n",
    "shift = 38000 - shape*scale\n",
    "\n",
    "x = np.linspace(0, shape*scale + 5*math.sqrt(shape*scale**2), 100) + shift\n",
    "plt.plot(x, stats.gamma.pdf(x - shift, a = shape, scale = scale), linewidth = 4)\n",
    "\n",
    "plt.title('Skewed Distribution (shifted Gamma(shape = 5, scale = 2000))', size = 18)\n",
    "plt.xlabel('Net Demand Quantity', size = 18)\n",
    "plt.ylabel('Density', size = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ GAMMA EXPECTED LOSS -------\n",
    "# We think demand will be Gamma(shape, scale), with mean shape*scale, variance shape*scale^2\n",
    "# For article with cost = cost_0, margin = margin_0\n",
    " \n",
    "import math\n",
    "\n",
    "# Expected value of Loss function, given: buy, shape, scale, margin, cost, non-centrality\n",
    "def EL_gamma(b_0, shape_0, scale_0, margin_0, cost_0, shift_0):\n",
    "    \n",
    "    I = lambda x: L(x, b_0, margin_0, cost_0) * stats.gamma.pdf(x - shift_0, a = shape_0, scale = scale_0) # I for integrand\n",
    "    \n",
    "    Exp_loss = integrate.quad(I, 0 + shift_0, shift_0 + shape_0*scale_0 + 5*math.sqrt(shape_0*scale_0**2))\n",
    "    \n",
    "    return round(Exp_loss[0], 2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hypothetical Expected loss, function of buy qty\n",
    "shape = 5\n",
    "scale = 2000\n",
    "shift = 38000 - shape*scale\n",
    "\n",
    "margin = 65.80\n",
    "cost = 4.20\n",
    "\n",
    "buys = list(range(0 + shift, shift + shape*scale + 4*np.int(math.sqrt(shape*scale**2)), 50)) # buy qtys to for which it calculates E[L]\n",
    "ELs = [EL_gamma(b, shape, scale, margin, cost, shift) for b in buys]\n",
    "\n",
    "\n",
    "print('buying', buys[ELs.index(min(ELs))], 'units minimizes expected loss')\n",
    "print()\n",
    "print('The minimized expected loss is:', min(ELs)) # minimized Expected loss\n",
    "print()\n",
    "print('Expected loss with 20% buffer approach:', EL_gamma(38000*1.2, shape, scale, margin, cost, shift))\n",
    "print()\n",
    "print('Expected profit increase:', EL_gamma(buys[ELs.index(min(ELs))], shape, scale, margin, cost, shift) - EL_gamma(38000*1.2, shape, scale, margin, cost, shift))\n",
    "\n",
    "# print('Loss of', L(38000, buys[ELs.index(min(ELs))], 65.80, 4.20), 'if we buy optimally and predict perfectly') # loss assoc. with that buy if prediction perfect\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "plt.plot(buys, ELs, linewidth = 3)\n",
    "\n",
    "plt.title('Expected Loss vs. Buy Quantity', size = 18)\n",
    "plt.xlabel('Buy Quantity', size = 18)\n",
    "plt.ylabel('Expected Loss', size = 18)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Fish fishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B28128\n",
    "\n",
    "mu = 800\n",
    "sigma = 200\n",
    "margin = 55 - 11\n",
    "cost = 11\n",
    "\n",
    "buys = list(range(mu - 1*sigma, mu + 3*sigma, 50)) # buy qtys for which calculate E[L]\n",
    "ELs = [EL(b, mu, sigma, margin, cost) for b in buys]\n",
    "\n",
    "print('Buying', buys[ELs.index(min(ELs))], 'units minimizes expected loss') # buy qty that minimizes expected loss\n",
    "print()\n",
    "print('Minimized expected loss:', min(ELs)) # minimized Expected loss\n",
    "print()\n",
    "print('Expected loss with 20% buffer approach:', EL(mu*1.2, mu, sigma, margin, cost))\n",
    "print()\n",
    "print('Expected profit increase:', \n",
    "      round(np.abs(EL(buys[ELs.index(min(ELs))], mu, sigma, margin, cost) - EL(mu*1.2, mu, sigma, margin, cost)), 2))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "plt.plot(buys, ELs, linewidth = 3)\n",
    "\n",
    "plt.title('Expected Loss vs. Buy Quantity', size = 18)\n",
    "plt.xlabel('Buy Quantity', size = 18)\n",
    "plt.ylabel('Expected Loss', size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework Extension EDA: combine information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(loc=1000, scale=100, size=1000) # NORMAL\n",
    "c = np.random.gamma(shape = 2, scale = 200, size = 1000) + 600 # GAMMA\n",
    "\n",
    "plt.hist((x, c), bins = 20, density = True)\n",
    "plt.title('Combining Information', size = 18)\n",
    "plt.xlabel('Hypothetical Buy Quantity Data Points', size = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = np.concatenate((x, c)) # NORMAL + GAMMA\n",
    "pd.DataFrame(xc).quantile((0.05, 0.95))\n",
    "plt.hist(xc, bins = 40, density = True, color = 'skyblue')\n",
    "plt.axvline(700, linestyle='dashed', linewidth=3)\n",
    "plt.axvline(1324, linestyle='dashed', linewidth=3)\n",
    "plt.axvline(xc.mean(), color = 'orange', linestyle='dashed', linewidth=3)\n",
    "\n",
    "plt.title('Combining Information: Empirical Mean and Confidence Intervals', size = 18)\n",
    "plt.xlabel('Hypothetical Buy Quantity Data Points', size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: SD Estimation Impact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize Expected loss over buy quantity b_0\n",
    "\n",
    "# Change: b_0 must come last for use with partial() function\n",
    "def EL_b0(mu_0, sigma_0, margin_0, cost_0, b_0):\n",
    "    I = lambda x: L(x, b_0, margin_0, cost_0) * stats.norm.pdf(x, mu_0, sigma_0) # I for integrand\n",
    "    Exp_loss = integrate.quad(I, mu_0 - 4*sigma_0, mu_0 + 4*sigma_0)\n",
    "    return round(Exp_loss[0], 2) \n",
    "\n",
    "def minimize_EL(mu, sigma, margin, cost):\n",
    "    \n",
    "    p = partial(EL_b0, mu, sigma, margin, cost) # Make EL function of only one var: b_0\n",
    "    buy_opt = optimize.minimize_scalar(p, bounds = (mu - sigma, mu + 3*sigma))\n",
    "    \n",
    "    return int(buy_opt['x']), int(buy_opt['fun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: to calculate the EL under an incorrect sigma0 estimate:\n",
    "    # (1) Calculate the optimal buy under true sigma, and assoc loss\n",
    "    # (2) Calculate the optimal buy under falst sigma0\n",
    "    # (3) Calculate the expected losses under true mu, sigma --- but with the b_0 from optimizing w/ sigma0 in (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider again Continental 80s\n",
    "* margin = 65.80\n",
    "* cost = 4.20\n",
    "* Suppose true d ~ N(38000, 7000)\n",
    "* Suppose incorrectly think d ~ N(38000, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_0: 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((45772, 41661), 7211.5)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma: 7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48880, 58325)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_0: 9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((51990, 74990), 4570)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = 38000\n",
    "sigma = 7000\n",
    "margin = 65.80\n",
    "cost = 4.20\n",
    "\n",
    "a = minimize_EL(mu, sigma, margin, cost)\n",
    "\n",
    "# Believed optimal and true EL\n",
    "print('sigma_0:', 5000)\n",
    "b = minimize_EL(mu, 5000, margin, cost)\n",
    "b, EL(mu, 7000, margin, cost, b[0]) - EL(mu, 7000, margin, cost, a[0])\n",
    "\n",
    "# True optimal and EL:\n",
    "print('sigma:', sigma)\n",
    "a\n",
    "\n",
    "# Believed optimal and believed EL\n",
    "print('sigma_0:', 9000)\n",
    "b = minimize_EL(mu, 9000, margin, cost)\n",
    "b, round(EL(mu, 7000, margin, cost, b[0]) - EL(mu, 7000, margin, cost, a[0])) \n",
    "\n",
    "# Takeaway: more expensive to underestimate SD than overestimate SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, (48880, 58325))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig_0 = 1000 -- increase in expected losses: 94028\n",
      "sig_0 = 1500 -- increase in expected losses: 76040\n",
      "sig_0 = 2000 -- increase in expected losses: 60310\n",
      "sig_0 = 2500 -- increase in expected losses: 46782\n",
      "sig_0 = 3000 -- increase in expected losses: 35330\n",
      "sig_0 = 3500 -- increase in expected losses: 25780\n",
      "sig_0 = 4000 -- increase in expected losses: 18032\n",
      "sig_0 = 4500 -- increase in expected losses: 11903\n",
      "sig_0 = 5000 -- increase in expected losses: 7212\n",
      "sig_0 = 5500 -- increase in expected losses: 3847\n",
      "sig_0 = 6000 -- increase in expected losses: 1617\n",
      "sig_0 = 6500 -- increase in expected losses: 385\n",
      "sig_0 = 7000 -- increase in expected losses: 1\n",
      "sig_0 = 7500 -- increase in expected losses: 340\n",
      "sig_0 = 8000 -- increase in expected losses: 1282\n",
      "sig_0 = 8500 -- increase in expected losses: 2722\n",
      "sig_0 = 9000 -- increase in expected losses: 4571\n",
      "sig_0 = 9500 -- increase in expected losses: 6743\n",
      "sig_0 = 10000 -- increase in expected losses: 9186\n"
     ]
    }
   ],
   "source": [
    "mu = 38000\n",
    "sigma = 7000\n",
    "margin = 65.80\n",
    "cost = 4.20\n",
    "\n",
    "a = minimize_EL(mu, sigma, margin, cost) # minimize with real sigma\n",
    "sigma, a\n",
    "\n",
    "for sig0 in range(1000, 10001, 500):\n",
    "    b = minimize_EL(mu, sig0, margin, cost)  # buy qty to minimize with fake sigma\n",
    "    EL_0 = EL(mu, sigma, margin, cost, b[0]) # EL with that buy quantity, but real sigma\n",
    "    c = round(EL_0 - a[1])\n",
    "    print('sig_0 =', sig0, '-- increase in expected losses:', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 7000 -- (optimal buy, EL): (48880, 58325)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = minimize_EL(mu, sigma, margin, cost)\n",
    "print('sigma =', sigma, '-- (optimal buy, EL):', a)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48880, 58325)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma0, EL increase\n"
     ]
    }
   ],
   "source": [
    "print('sigma0, EL increase')\n",
    "\n",
    "results = [(sig0, a[1] - EL(minimize_EL(mu, sig0, margin, cost)[0]) for sig0 in range(1000, 15001, 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: \n",
    "# x-axis: sigma_0\n",
    "# y-axis: losses over optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate prediction SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('weekly_sales_all.csv', low_memory=False, index_col = 0) # *** DATA ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.head()\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2 = pd.DataFrame(dat.groupby(['article_number', 'season'])['net_qty'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat3 = dat2.groupby('article_number').aggregate(['mean', 'std', 'max', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat3.head()\n",
    "dat3.dropna(inplace=True)\n",
    "\n",
    "dat3 = dat3[dat3['net_qty']['max'] < 50000]\n",
    "\n",
    "dat3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dat3['net_qty']['max']).reshape(-1, 1)\n",
    "y = np.array(dat3['net_qty']['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X, y)\n",
    "reg.coef_\n",
    "\n",
    "reg.intercept_ \n",
    "reg.predict([[38000]])\n",
    "\n",
    "reg.coef_*38000 + reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot SD against mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_max = dat3['net_qty']['max']\n",
    "dat_std = dat3['net_qty']['std']\n",
    "\n",
    "dat_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "plt.scatter(dat3['net_qty']['max'], dat3['net_qty']['std'])\n",
    "\n",
    "plt.title('Standard Deviation vs. Max Season Net Demand Qty', size = 18)\n",
    "plt.xlabel('Max Season Net Demand Qty', size = 18)\n",
    "plt.ylabel('Standard Deviation', size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal buy for SDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigs = np.linspace(100, 700, 100)\n",
    "opt_buys = [min_expected_loss(1000, np.int(s), 85, 15) for s in sigs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sigs, opt_buys, linewidth = 3)\n",
    "\n",
    "plt.title('Optimal Buy vs. Prediction Standard Dev.', size = 18)\n",
    "plt.xlabel('Standard Deviation', size = 18)\n",
    "plt.ylabel('Optimal Buy', size = 18)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [18, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Article EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('weekly_sales_all.csv', low_memory=False, index_col = 0) # *** DATA ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2 = pd.DataFrame(dat.groupby(['article_number', 'year', 'season'])['net_sales'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_ref = pd.read_csv('dat_ref.csv')\n",
    "dat_ref = dat_ref[['group_article', 'rmh_product_division_descr', 'rmh_category_descr', 'rmh_retail_section_descr', 'rmh_product_type_descr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat3 = pd.merge(dat2, dat_ref, left_on='article_number', right_on='group_article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(dat2.article_number).intersection(set(dat_ref.group_article)))\n",
    "\n",
    "dat2.shape\n",
    "dat3.shape\n",
    "\n",
    "len(dat3.article_number.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve some SS19 only articles, to then build empirical-integrated priors for\n",
    "dat00 = dat2.reset_index()\n",
    "\n",
    "dat000 = pd.crosstab(index=dat00[\"article_number\"],  # Make a crosstab\n",
    "                              columns=dat00['season']).sum(axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
