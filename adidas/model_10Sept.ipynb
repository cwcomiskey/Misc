{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Modules, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T10:39:00.058662Z",
     "start_time": "2019-12-13T10:39:00.023898Z"
    },
    "code_folding": [
     0
    ],
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modules, functions -- \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "# import dask.dataframe as ddf\n",
    "# import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 1700)\n",
    "\n",
    "import scipy\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.tsatools import detrend\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T10:39:00.283944Z",
     "start_time": "2019-12-13T10:39:00.251806Z"
    },
    "code_folding": [
     0,
     2,
     14,
     18,
     29,
     46,
     65,
     76,
     93,
     112,
     124,
     140,
     150
    ],
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CHRIS and ARTEM code \n",
    "\n",
    "def initialize_parameters(par = np.array([0.5, 0.9, 0, 1, 0])):\n",
    "    # np.random.seed(3)\n",
    "    parameters = {}\n",
    "\n",
    "    parameters['alpha'] = par[0]\n",
    "    parameters['beta'] = par[1]\n",
    "    parameters['omega'] = par[2]* (1-par[1])    # one way to choose that is omega/(1-beta) = unconditional mean \n",
    "    parameters['sigma'] = par[3]\n",
    "    parameters['f0'] = par[4]                   # one way to choose is unconditional mean \n",
    "\n",
    "    return parameters\n",
    "\n",
    "def loglik(y, f, x, sigma):\n",
    "    ll = -1/2*np.log(2*np.pi ) - 1/2*np.log(sigma) - 1/(2*sigma)*(y - x*f)**2 \n",
    "    return ll\n",
    "\n",
    "def score_compute(y, f, x, parameters, epsilon = 1e-7 ):\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    \n",
    "    score = (y - x*f)/sigma\n",
    "    \n",
    "    return score\n",
    "\n",
    "def filterGAS(y, x, parameters):\n",
    "    \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    score0 = score_compute(y[0,:],  f0, x[0,:], parameters, epsilon = 1e-7) \n",
    "    f = np.zeros((len(y),1))\n",
    "    \n",
    "    f[0,:] = f0\n",
    "    for t in range(1,len(y)):\n",
    "        scoret = score_compute(y[t-1,:], f[t-1,:], x[t-1,:], parameters, epsilon = 1e-7) \n",
    "        f[t,:] = omega + alpha*scoret + beta*f[t-1,:] \n",
    "\n",
    "    return f\n",
    "\n",
    "def loglikest(par, y, x):\n",
    "    parameters = initialize_parameters(par)\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    # f0 = parameters[\"f0\"]\n",
    "    \n",
    "    f = filterGAS(y, x, parameters) \n",
    "    ll = np.zeros((len(y), 1))\n",
    "    m = len(y)\n",
    "\n",
    "    for t in range(0, len(y)):\n",
    "         ll[t,:] = loglik(y[t,:], f[t,:], x[t,:], sigma)\n",
    "    loglik_res = -(np.sum(ll))/m\n",
    "\n",
    "    return loglik_res\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def score_compute_2(y, f, x, parameters, epsilon = 1e-7 ):\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    \n",
    "    score = (y - x*f) \n",
    "    \n",
    "    return score\n",
    "\n",
    "def filterGAS_2(y, x, parameters):\n",
    "    \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    score0 = score_compute_2(y[0,:],  f0, x[0,:], parameters, epsilon = 1e-7) \n",
    "    f = np.zeros((len(y),1))\n",
    "    \n",
    "    f[0,:] = f0\n",
    "    for t in range(1,len(y)):\n",
    "        scoret = score_compute_2(y[t-1,:], f[t-1,:], x[t-1,:], parameters, epsilon = 1e-7) \n",
    "        f[t,:] = omega + alpha*scoret + beta*f[t-1,:] \n",
    "\n",
    "    return f\n",
    "\n",
    "def loglikest_2(par, y, x):\n",
    "    parameters = initialize_parameters(par)\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    # f0 = parameters[\"f0\"]\n",
    "    \n",
    "    f = filterGAS_2(y, x, parameters) \n",
    "    ll = np.zeros((len(y), 1))\n",
    "    m = len(y)\n",
    "\n",
    "    for t in range(0, len(y)):\n",
    "         ll[t,:] = loglik(y[t,:], f[t,:], x[t,:], sigma)\n",
    "    loglik_res = -(np.sum(ll))/m\n",
    "\n",
    "    return loglik_res\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def GAS_est(df):\n",
    "    \n",
    "    y = df.net_qty.values          # observed demand (response)\n",
    "    x = df.buy_availability.values # buy_availability (explanatory)\n",
    "\n",
    "    y = y.reshape((len(y),1)) \n",
    "    x = x.reshape((len(y),1))\n",
    "    \n",
    "    ret = pd.DataFrame()\n",
    "    ret['year'] = df['year']\n",
    "    ret['week'] = df['week']\n",
    "        \n",
    "    abc = scipy.optimize.minimize(\n",
    "        loglikest,                                       # function to minimize (log likelihood y|x,theta)\n",
    "        np.array([0.8, 0.9, np.mean(y), 1, np.mean(y)]), # initial parameter values (starting)\n",
    "        args=(y, x), \n",
    "        options ={'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12},\n",
    "        method='L-BFGS-B', \n",
    "        bounds=((0,  None),             # alpha\n",
    "                (-1, 1),                # beta\n",
    "                (0.001, np.mean(y)*2),  # omega \n",
    "                (0.001, None),          # sigma\n",
    "                (0.001, np.mean(y)*2)   # f\n",
    "               )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # --- CONVERGENCE control flow ---\n",
    "    if abc.success == True:\n",
    "        \n",
    "        x1par = initialize_parameters(abc.x) \n",
    "        GAS = filterGAS(y, x, x1par)\n",
    "        \n",
    "        ret['GAS_est'] = GAS\n",
    "        ret['Convergence'] = [abc.success] * len(y)\n",
    "        ret['Convg type'] = ['One'] * len(y)\n",
    "        \n",
    "    # **Modification if first algorithm fails\n",
    "    elif abc.success == False:\n",
    "        \n",
    "        abc = scipy.optimize.minimize(\n",
    "            loglikest_2,                                       # function to minimize (log likelihood y|x,theta)\n",
    "            np.array([0.8, 0.9, np.mean(y), 1, np.mean(y)]),   # initial parameter values (starting)\n",
    "            args=(y, x), \n",
    "            options ={'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12},\n",
    "            method='L-BFGS-B', \n",
    "            bounds=((0,  None),             # alpha\n",
    "                    (-1, 1),                # beta\n",
    "                    (0.001, np.mean(y)*2),  # omega \n",
    "                    (0.001, None),          # sigma\n",
    "                    (0.001, np.mean(y)*2)   # f\n",
    "                   )\n",
    "                )\n",
    "\n",
    "        x1par = initialize_parameters(abc.x) \n",
    "        GAS = filterGAS_2(y, x, x1par)\n",
    "        \n",
    "        ret['GAS_est'] = GAS\n",
    "        ret['Convergence'] = [abc.success] * len(y)\n",
    "        ret['Convg type'] = ['Two'] * len(y)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T10:59:47.604043Z",
     "start_time": "2019-12-13T10:59:47.601671Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Season zero articles\n",
    "\n",
    "SSYY = 'FW19' # Season zero: SeasonSeasonYearYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buyer_table = pd.read_csv('data/EU_seasons.csv', low_memory = False, error_bad_lines = False, sep = \",\").sort_values(['article_number', 'brand', 'season']) # 26 Aug\n",
    "buyer_table = buyer_table[(buyer_table.season == SSYY)]\n",
    "# S0 = buyer_table[buyer_table.season == SSYY].article_number # Season zero articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Forecasts -- \n",
    "SS20 = (pd.read_excel('data/DTC_Range_SS20.xlsx', sheet_name='Range', skiprows=5, \n",
    "    usecols=['Article Number (6 digits)', 'Carry Forward', 'eCom Market FC', 'Ecom Ranged RMA (PIM)'])\n",
    "        .rename(columns = {'Article Number (6 digits)': 'article_number'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# adi + reebok + both -- \n",
    "\n",
    "# ----- adidas ------ \n",
    "\n",
    "# rma1_adi = pd.read_csv('data/article_range_rma1_adidas_fw20.csv', low_memory = False, error_bad_lines = False, sep = \",\", usecols = ['Article Number', 'Market Retail Price', 'WE eCom', 'eCom Range']) # (13618, 4)\n",
    "\n",
    "rma2_adi = pd.read_excel('data/RMA-02_ Market Range Plan_24102019.xlsx', sheet_name='BSO', skiprows=2) # adidas RMA2 -- 21 October\n",
    "\n",
    "rma2_adi = rma2_adi[['Article Number', 'WE eCom 06.11.2019', 'Market Retail Price']]\n",
    "\n",
    "rma2_adi = rma2_adi[rma2_adi['WE eCom 06.11.2019'] != 0]\n",
    "\n",
    "\n",
    "# ----- rbk ------ \n",
    "\n",
    "# rma1_rbk = pd.read_csv('data/article_range_rma1_reebok_fw20.csv', low_memory = False, error_bad_lines = False, sep = \",\", usecols = ['Article_number', 'Total Marketing Forecast', '  Total Net Sales  ', '  Total Inline Forecast  ']) # rma1_rbk.columns[40:]\n",
    "\n",
    "rma2_rbk = pd.read_excel('data/eCom Range Download_Reebok_FW20.xlsx') # Reebok RMA2 - 21 October\n",
    "rma2_rbk = rma2_rbk[['Article Number', 'RBK WE eCom', 'Hub Retail Price']]\n",
    "\n",
    "clssc_rbk = pd.read_csv('data/reebok_classics.csv') # Reebok RMA2 - 21 October\n",
    "\n",
    "addtl_classics = (\n",
    "    set(clssc_rbk.article_number).\n",
    "    symmetric_difference(rma2_rbk['Article Number']).\n",
    "    intersection(clssc_rbk.article_number)\n",
    "                 )\n",
    "\n",
    "# both -- \n",
    "\n",
    "carryovers = (set(buyer_table.article_number).\n",
    "              intersection(set(rma2_adi['Article Number'])).\n",
    "              union(set(buyer_table.article_number).\n",
    "                    intersection(set(rma2_rbk['Article Number'])))\n",
    "             )\n",
    "print('Carryovers: ', len(carryovers))\n",
    "print()\n",
    "\n",
    "print('adi carryovers: ', len(set(buyer_table.article_number).intersection(set(rma2_adi['Article Number']))))\n",
    "print('rbk carryovers: ', len(set(buyer_table.article_number).intersection(set(rma2_rbk['Article Number']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T10:40:52.182591Z",
     "start_time": "2019-12-13T10:40:29.460742Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat0 = pd.read_csv('data/ch4k.csv')\n",
    "ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, error_bad_lines = False, \n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "                                  'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc',\n",
    "                                  'brand_desc', 'bus_unit_desc', 'rmh_cat_desc'])\n",
    "\n",
    "# dat0 = dat0[dat0.season.isin(['SS17', 'SS18', 'SS19'])]\n",
    "dat0 = dat0[dat0.season.isin(['FW17', 'FW18', 'FW19', 'FW20'])]\n",
    "\n",
    "# Remove clearance transactions\n",
    "# dat0['clearance'] = dat0.clearance.fillna(0) \n",
    "# dat0['net_qty'] = (1 - dat0.clearance)*dat0.net_qty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# --- INTEGRATE THIS --- to replace duplicated shenanigans below\n",
    "\n",
    "# stealths -- \n",
    "\n",
    "# stealth = pd.read_csv('data/stealth_carryovers_FW2020.csv', low_memory = False, error_bad_lines = False, sep = \";\")\n",
    "\n",
    "# # stealth2 = pd.read_csv('data/stealth_carryovers_eu_RMA2.csv').rename(columns = {'article1': 'article', 'article2': 'stealth_article'})\n",
    "# # stealth3 = pd.concat([stealth, stealth2])\n",
    "\n",
    "# FW19_range = buyer_table.article_number.unique()\n",
    "# FW20_range = set(rma2_adi['Article Number']).union(set(rma2_rbk['Article Number']))\n",
    "\n",
    "# # New forecasts: IN FW20 --- NOT IN FW19\n",
    "# additions = stealth[(stealth.article.isin(FW20_range)) & (~(stealth.article.isin(carryovers)))] \n",
    "\n",
    "# find_me = additions.stealth_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T10:41:48.316136Z",
     "start_time": "2019-12-13T10:41:48.090903Z"
    }
   },
   "outputs": [],
   "source": [
    "rbk = pd.read_excel('data/rbk_1920.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T10:42:03.058865Z",
     "start_time": "2019-12-13T10:42:03.055765Z"
    }
   },
   "outputs": [],
   "source": [
    "articles_of_interest = rbk.article_number_FW19.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# GAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T10:59:51.054316Z",
     "start_time": "2019-12-13T10:59:50.638175Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_GAS = dat0.copy()[['article_number', 'year', 'week', 'country', 'season', 'net_qty', 'buy_availability', 'brand']] \n",
    "\n",
    "# dat_GAS = dat_GAS[(dat_GAS.season == SSYY) & (dat_GAS.article_number.isin(carryovers))].sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# Only SSS19 articles **ALSO IN** SS20 range\n",
    "dat_GAS = (dat_GAS[(dat_GAS.season == SSYY) & (dat_GAS.article_number.isin(articles_of_interest))].\n",
    "           sort_values(['article_number', 'brand', 'country', 'year', 'week']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T11:01:29.099746Z",
     "start_time": "2019-12-13T10:59:52.428458Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in square\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.7/site-packages/scipy/optimize/optimize.py:697: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  grad[k] = (f(*((xk + d,) + args)) - f0) / d[k]\n"
     ]
    }
   ],
   "source": [
    "dat_GAS = dat_GAS.groupby(['article_number', 'brand', 'country']).apply(GAS_est).reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T11:01:29.109932Z",
     "start_time": "2019-12-13T11:01:29.101277Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dat_GAS = pd.DataFrame(dat_GAS.groupby(['article_number', 'year', 'week'])['GAS_est'].sum()).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# With/out clearance -- \n",
    "\n",
    "# With clearance --\n",
    "# dat_GAS0 = pd.DataFrame(dat_GAS.groupby('article_number')['GAS_est'].sum().round())\n",
    "# dat_GAS0.head()\n",
    "\n",
    "# Without clearance --\n",
    "# dat_GAS1 = pd.DataFrame(dat_GAS.groupby('article_number')['GAS_est'].sum().round())\n",
    "# dat_GAS1.head()\n",
    "\n",
    "# dat_GAS_both = pd.merge(\n",
    "#     dat_GAS0,\n",
    "#     dat_GAS1,\n",
    "#     left_index = True, right_index = True\n",
    "# )\n",
    "\n",
    "# dat_GAS_both['diff'] = dat_GAS0['GAS_est'] - dat_GAS1['GAS_est']\n",
    "\n",
    "# dat_GAS_both['diff'].hist()\n",
    "# dat_GAS_both['diff'].describe().round()\n",
    "\n",
    "\n",
    "# dat_GAS_both['lt0'] = (dat_GAS_both['diff'] <= 0)*1\n",
    "\n",
    "# dat_GAS_both['lt0'].mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T11:08:47.944677Z",
     "start_time": "2019-12-13T11:08:47.941212Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_cat_level_means(df, cat, new_col_name):\n",
    "    return pd.DataFrame(df.groupby([cat, 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': new_col_name})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T11:08:56.147555Z",
     "start_time": "2019-12-13T11:08:53.174068Z"
    },
    "code_folding": [],
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Create df, wrangle, calculate category means -- \n",
    "\n",
    "seasonality_dat = (\n",
    "    dat0[['article_number', 'brand', 'year', 'week', 'country', \n",
    "          'season', 'net_qty', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "          'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].\n",
    "    dropna().\n",
    "    sort_values(['article_number', 'year', 'week']).\n",
    "    copy())\n",
    "\n",
    "seasonality_dat['net_qty2'] = seasonality_dat.groupby(['article_number', 'brand', 'season', 'year', 'week'])['net_qty'].transform(sum)\n",
    "seasonality_dat = seasonality_dat.drop(['brand', 'country', 'net_qty'], axis = 1).drop_duplicates().rename(columns = {'net_qty2': 'net_qty'})\n",
    "\n",
    "# -- Reliable, mirror seasons --\n",
    "seasonality_dat = seasonality_dat[~seasonality_dat.season.isin(['FW14', 'FW15', 'FW16', 'SS14','SS15', 'SS16'])] # Exclude these seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T11:10:35.756587Z",
     "start_time": "2019-12-13T11:10:35.591063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = calc_cat_level_means(df = seasonality_dat, cat = 'sports_cat_desc', new_col_name = 'sport_weekly_mean')\n",
    "b = pd.DataFrame(seasonality_dat.groupby(['sports_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'sport_weekly_mean'})\n",
    "\n",
    "a.equals(b)\n",
    "\n",
    "\n",
    "# HUZZAH!! IMPLEMENT!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- Calculate cat-level weekly means across *ALL SEASONS* ---- \n",
    "seasonality_sport   = pd.DataFrame(seasonality_dat.groupby(['sports_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'sport_weekly_mean'})\n",
    "seasonality_rmh     = pd.DataFrame(seasonality_dat.groupby(['rmh_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'rmh_weekly_mean'})\n",
    "seasonality_gndr    = pd.DataFrame(seasonality_dat.groupby(['gender_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'gender_weekly_mean'})\n",
    "seasonality_agegrp  = pd.DataFrame(seasonality_dat.groupby(['age_group_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'age_weekly_mean'})\n",
    "seasonality_frnchse = pd.DataFrame(seasonality_dat.groupby(['franchise', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'franchise_weekly_mean'})\n",
    "seasonality_prdgrp  = pd.DataFrame(seasonality_dat.groupby(['prod_grp_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'prd_grp_weekly_mean'})\n",
    "\n",
    "seasonality_dfs = [seasonality_sport, seasonality_rmh, seasonality_gndr, seasonality_agegrp, seasonality_frnchse, seasonality_prdgrp]\n",
    "\n",
    "# seasonality_dat = seasonality_dat[(seasonality_dat.article_number.isin(carryovers))].sort_values(['article_number', 'year', 'week'])\n",
    "seasonality_dat = seasonality_dat[(seasonality_dat.article_number.isin(articles_of_interest))].sort_values(['article_number', 'year', 'week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function: regress one against many -- \n",
    "\n",
    "def regress(df):\n",
    "    # function for regressing article net_qty on seasonalities\n",
    "    # for article a's level of each category, retreive weekly means, then regress \n",
    "    \n",
    "    df = df.sort_values(['article_number', 'year', 'week'])\n",
    "        \n",
    "    # article net_demand_qty\n",
    "    y = df[['net_qty', 'week']].set_index('week')\n",
    "\n",
    "    # article category-level weekly means; set_index() for joining\n",
    "    x_sport   = seasonality_sport[seasonality_sport.sports_cat_desc == df.sports_cat_desc.unique()[0]].set_index('week')\n",
    "    x_rmh     = seasonality_rmh[seasonality_rmh.rmh_cat_desc == df.rmh_cat_desc.unique()[0]].set_index('week')\n",
    "    x_gndr    = seasonality_gndr[seasonality_gndr.gender_desc == df.gender_desc.unique()[0]].set_index('week')\n",
    "    x_agegrp  = seasonality_agegrp[seasonality_agegrp.age_group_desc == df.age_group_desc.unique()[0]].set_index('week')\n",
    "    x_frnchse = seasonality_frnchse[seasonality_frnchse.franchise == df.franchise.unique()[0]].set_index('week')\n",
    "    x_prdgrp  = seasonality_prdgrp[seasonality_prdgrp.prod_grp_desc == df.prod_grp_desc.unique()[0]].set_index('week')\n",
    "\n",
    "    # design matrix (ensure 'week' alignment)\n",
    "    yX = (pd.merge(y, x_sport, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_rmh, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_gndr, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_agegrp, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_frnchse, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_prdgrp, left_index=True, right_index=True, how = 'outer').\n",
    "          drop(['sports_cat_desc', 'rmh_cat_desc', 'gender_desc',\n",
    "               'age_group_desc', 'franchise', 'prod_grp_desc'], axis = 1))\n",
    "\n",
    "    # predict article 'a' net_demand_qty with 5 article 'a' category-level seasonalities\n",
    "\n",
    "    y = yX.net_qty\n",
    "    \n",
    "    X = yX.drop('net_qty', axis = 1)\n",
    "    # X = sm.add_constant(X) # ****** ******* *******\n",
    "    \n",
    "    mod = sm.OLS(y, X, missing='drop').fit()\n",
    "    # print(df.article_number.unique(), round(mod.rsquared, 2))\n",
    "\n",
    "    ret = pd.DataFrame(index = X.index)\n",
    "    ret['seas_preds'] = mod.predict(X).round()\n",
    "    \n",
    "    ret = ret.reset_index()\n",
    "    ret = pd.DataFrame(ret.groupby('week')['seas_preds'].mean())\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# regress articles of interest on seasonality\n",
    "preds = seasonality_dat.groupby(['article_number']).apply(regress).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Merge back with net_qty\n",
    "preds = (pd.merge(preds, seasonality_dat[seasonality_dat.season == SSYY], how = 'left').\n",
    "         sort_values(['article_number', 'year', 'week'])[['article_number', 'year', 'week', 'net_qty', 'seas_preds']]\n",
    "        )\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Zero out negative preds\n",
    "preds['seas_preds'] = np.where(preds.seas_preds > 0, preds.seas_preds, 0) \n",
    "\n",
    "# Combine: preds = All weeks --AND-- dat_GAS = observed weeks\n",
    "preds = pd.merge(preds, dat_GAS.round(), how = 'left', on =['article_number', 'year', 'week'])\n",
    "\n",
    "# preds = preds.astype({'year': 'int', 'net_qty': 'int', 'seas_preds': 'int', 'GAS_est': 'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Aggregate to Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eCom forecasts -- \n",
    "\n",
    "# rma2_rbk = rma2_rbk.rename(columns = {'RBK WE eCom': 'WE eCom'})\n",
    "# rma2_adi = rma2_adi.rename(columns = {'WE eCom 06.11.2019': 'WE eCom'})\n",
    "# rma2 = pd.concat([rma2_adi[['Article Number', 'WE eCom']], rma2_rbk[['Article Number', 'WE eCom']]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# weekly assignment of y_hat = f(observed, GAS, seasonality)\n",
    "\n",
    "preds['y_hat'] = np.where(np.isnan(preds.GAS_est), preds.seas_preds, (preds.GAS_est + preds.seas_preds)/2) # Evgeniy step\n",
    "\n",
    "preds['y_hat'] = np.where(preds.y_hat > preds.net_qty, preds.y_hat, preds.net_qty) # Risk management step (judgment call)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rma2_rbk = rma2_rbk.rename(columns = {'Article Number': 'article_number', 'RBK WE eCom': 'eCom Market FC'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum over season\n",
    "preds_season = pd.DataFrame(preds.groupby('article_number')['y_hat', 'net_qty'].apply(sum).round())\n",
    "\n",
    "# Growth\n",
    "preds_season['y_hat'] = preds_season.y_hat * 1.1 # default growth rate\n",
    "\n",
    "# Combine DAA + eCom\n",
    "# preds_season = pd.merge(preds_season, SS20[['article_number', 'eCom Market FC']], how = 'left', on = 'article_number').round()\n",
    "preds_season = pd.merge(preds_season, rma2_rbk[['article_number', 'eCom Market FC']], how = 'left', on = 'article_number').round()\n",
    "\n",
    "\n",
    "# preds_season = pd.merge(preds_season.drop('WE eCom', axis = 1), rma2, how = 'left', left_on='article_number', right_on='Article Number').drop('Article Number', axis = 1).round()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Add column with # of observed weeks\n",
    "# preds_season = (\n",
    "#     pd.merge(\n",
    "#         preds_season, \n",
    "#         pd.DataFrame(preds[(~preds.net_qty.isna()) & (preds.net_qty > 0)].article_number.value_counts()).rename(columns = {'article_number': 'week_count'}), \n",
    "#         how = 'left', left_on='article_number', right_index = True\n",
    "#     )\n",
    "\n",
    "# # Retain articles with at least 4 observed weeks\n",
    "# preds_season = preds_season[(preds_season.week_count > 3) & (preds_season.net_qty > 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Magnifying glass -- \n",
    "aoi = 'G27706'\n",
    "a = aoi\n",
    "\n",
    "#dat0[dat0.article_number == aoi].sort_values(['country', 'year', 'week'])#.iloc[1,]\n",
    "\n",
    "\n",
    "preds_season[preds_season.article_number == a]\n",
    "\n",
    "dat_a = preds[preds.article_number == a]\n",
    "dat_a.sort_values(['year', 'week']).set_index(['year', 'week']).drop('article_number', axis = 1).apply(np.sum)\n",
    "\n",
    "dat_a[~dat_a.net_qty.isna()]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_a.sort_values(['year', 'week']).set_index(['year', 'week']).plot(linewidth = 3)\n",
    "\n",
    "dat_a.sort_values(['year', 'week']).set_index(['year', 'week'])\n",
    "\n",
    "# dat0[dat0.article_number == aoi].sort_values(['country', 'year', 'week'])#.iloc[1,]\n",
    "\n",
    "# plot -- \n",
    "dat_aoi = dat0[dat0.article_number == aoi].copy()\n",
    "dat_aoi = pd.merge(pd.DataFrame(dat_aoi.groupby(['year', 'week'])['net_qty'].sum()).reset_index(), dat_aoi[['year', 'week']].drop_duplicates())\n",
    "dat_aoi.year = [str(x) for x in dat_aoi.year]\n",
    "dat_aoi.week = [str(x) for x in dat_aoi.week]\n",
    "dat_aoi['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aoi.year, dat_aoi.week)]\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_aoi[['date', 'net_qty']].set_index('date').plot(linewidth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Overbuy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# functions -- \n",
    "\n",
    "from functools import partial\n",
    "from scipy import optimize\n",
    "from scipy import integrate\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Loss --- demand, buy, margin, cost\n",
    "def L(d, b, margin, cost):\n",
    "    if d > b:\n",
    "        return (d - b)*margin\n",
    "    elif d < b:\n",
    "        return (b - d)*cost\n",
    "    elif d == b:\n",
    "        return 0\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "# E[L | buy, article_mean, article_sd, article_margin, article_cost]\n",
    "def EL(mu, sigma, margin, cost, b):\n",
    "    I = lambda x: L(x, b, margin, cost) * stats.norm.pdf(x, mu, sigma) # I for integrand\n",
    "    Exp_loss = integrate.quad(I, 0, mu + 3*sigma)/(1 - stats.norm.cdf(0, loc = mu, scale = sigma)) # Expected value of Loss function\n",
    "    return round(Exp_loss[0], 2) \n",
    "\n",
    "def minimize_EL(mu, sigma, margin, cost):\n",
    "    if pd.isna([margin, cost]).sum() > 0:\n",
    "        print(mu, sigma, margin, cost)\n",
    "        return 'Error'\n",
    "    if(mu < 1100): # Judgment call; does not work as designed for low demand forecasts\n",
    "        return 1.2*mu \n",
    "    p = partial(EL, mu, sigma, margin, cost) # Make EL function of only one var: b_0\n",
    "    buy_opt = optimize.minimize_scalar(p, bounds = (mu, mu + 2*sigma))\n",
    "    return int(buy_opt['x']) # optimal buy quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Load cost/margin data\n",
    "cost_margin = dat0.copy()\n",
    "# cost_margin = cost_margin[cost_margin.season.isin(['SS18', 'FW18', 'SS19', 'FW19'])]\n",
    "\n",
    "cost_margin = cost_margin[cost_margin.article_number.isin(articles_of_interest)]\n",
    "\n",
    "cost_margin = pd.DataFrame(cost_margin.groupby('article_number')['price', 'cost', 'margin'].mean().round(2)).dropna() # All but one NA is season_net_qty < 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add cost and margin for optimal overbuy estimation                                                                           \n",
    "preds_season = pd.merge(preds_season, cost_margin, how = 'left', left_on = 'article_number', right_index=True).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season.shape\n",
    "preds_season.article_number.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Opt_Ovb = preds_season.apply(lambda row: minimize_EL(row['y_hat'], 550 + 0.2*row['y_hat'], row['margin'], row['cost']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_season['Opt_Ovb'] = Opt_Ovb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbk_stealth[:2]\n",
    "rbk_stealth.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = pd.merge(preds_season.drop('eCom Market FC', axis = 1), rbk_stealth[['article_number_FW20', 'article_number_FW19']], \n",
    "                        left_on = 'article_number', right_on = 'article_number_FW19')\n",
    "\n",
    "preds_season = (pd.merge(preds_season.drop('article_number_FW19', axis = 1), rma2_rbk, \n",
    "                         left_on='article_number_FW20', right_on='article_number').\n",
    "                drop('article_number_y', axis = 1)\n",
    "               )\n",
    "\n",
    "preds_season = preds_season.replace('Error', 'NA')\n",
    "\n",
    "preds_season = preds_season.drop_duplicates(subset = 'article_number_FW20', keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = pd.merge(\n",
    "    pd.DataFrame(preds_season.groupby('article_number_FW20')['y_hat'].mean().round(0)),\n",
    "    preds_season.drop('y_hat', axis = 1), \n",
    "    left_index = True, right_on = 'article_number_FW20'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = preds_season.drop('article_number_x', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = preds_season.rename(columns = {'article_number_FW20': 'article_number'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(preds_season['eCom Market FC']*preds_season['Hub Retail Price']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Stealth carryovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "carryovers.intersection(['EE6147', 'B22716', 'EE6145', 'EE6146'])\n",
    "carryovers.intersection(['FW5947', 'FV5946', 'FV5943', 'FV5943'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# all -- \n",
    "\n",
    "stealth = pd.read_csv('data/stealth_carryovers_FW2020.csv', low_memory = False, error_bad_lines = False, sep = \";\")\n",
    "\n",
    "# stealth2 = pd.read_csv('data/stealth_carryovers_eu_RMA2.csv').rename(columns = {'article1': 'article', 'article2': 'stealth_article'})\n",
    "# stealth3 = pd.concat([stealth, stealth2])\n",
    "\n",
    "FW19_range = buyer_table.article_number.unique()\n",
    "FW20_range = set(rma2_adi['Article Number']).union(set(rma2_rbk['Article Number']))\n",
    "\n",
    "# New forecasts: IN FW20 --- NOT IN FW19\n",
    "additions = stealth[(stealth.article.isin(FW20_range)) & (~(stealth.article.isin(carryovers)))] \n",
    "\n",
    "find_me = additions.stealth_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "hideCode": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_stealth = dat0[(dat0.article_number.isin(['BB9103', 'BB9104'])) & (dat0.season.isin(['FW18', 'FW17', 'FW16', 'FW15']))].copy()\n",
    "\n",
    "dat_stealth = dat_stealth[['article_number', 'year', 'week', 'country', 'season', 'net_qty', 'buy_availability']].sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# GAS step\n",
    "\n",
    "dat_GAS_stealth = dat_stealth.groupby(['article_number', 'country']).apply(GAS_est).reset_index()\n",
    "dat_GAS_stealth = pd.DataFrame(dat_GAS_stealth.groupby(['article_number', 'year', 'week'])['GAS_est'].sum()).reset_index()\n",
    "\n",
    "# SEASONALITY step\n",
    "seasonality_dat_stealth = (dat0[dat0.article_number.isin(['BB9103', 'BB9104'])].\n",
    "    copy()[['article_number', 'year', 'week', 'country', 'season', 'net_qty', \n",
    "            'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', \n",
    "            'age_group_desc', 'franchise', 'prod_grp_desc']].\n",
    "    dropna().sort_values(['article_number', 'year', 'week'])\n",
    "                  )\n",
    "\n",
    "# -- Sum over UK/EU, ADD article reference data --\n",
    "seasonality_dat_stealth = pd.merge(\n",
    "    pd.DataFrame(seasonality_dat_stealth.groupby(['article_number', 'season', 'year', 'week'])['net_qty'].sum()).reset_index(), # sum over UK & EU\n",
    "    seasonality_dat_stealth[['article_number', 'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].drop_duplicates() # add reference information\n",
    "    ).dropna().sort_values(['article_number', 'year', 'week'])\n",
    "\n",
    "seasonality_dat_stealth = seasonality_dat_stealth[seasonality_dat_stealth.season == 'FW18']\n",
    "\n",
    "preds_stealth = seasonality_dat_stealth.groupby(['article_number']).apply(regress).reset_index()\n",
    "\n",
    "# Merge back with reference data\n",
    "preds_stealth = pd.merge(\n",
    "    preds_stealth,\n",
    "    seasonality_dat_stealth[seasonality_dat_stealth.season == 'FW18'],\n",
    "    how = 'left').sort_values(['article_number', 'year', 'week'])[['article_number', 'year', 'week', 'net_qty', 'seas_preds']] # .fillna(method='ffill')\n",
    "\n",
    "\n",
    "# Zero out negative preds\n",
    "preds_stealth['seas_preds'] = np.where(preds_stealth.seas_preds > 0, preds_stealth.seas_preds, 0) \n",
    "\n",
    "\n",
    "\n",
    "# Combined observed weeks (partial season)  --- AND --- regression predicted (all) weeks\n",
    "preds_stealth = pd.merge(\n",
    "    preds_stealth,   # all weeks\n",
    "    dat_GAS_stealth, # observed weeks\n",
    "    how = 'left')\n",
    "\n",
    "\n",
    "\n",
    "# weekly assignment of GAS, seasonality, or combination\n",
    "preds_stealth['y_hat'] = np.where(np.isnan(preds_stealth.GAS_est), preds_stealth.seas_preds, (preds_stealth.GAS_est + preds_stealth.seas_preds)/2)\n",
    "\n",
    "\n",
    "\n",
    "# Sum over season\n",
    "preds_season_stealth = pd.DataFrame(preds_stealth.groupby('article_number')['y_hat'].apply(sum).round())\n",
    "\n",
    "\n",
    "\n",
    "# Growth\n",
    "preds_season_stealth['y_hat'] = preds_season_stealth.y_hat # * 1.1 # default growth rate\n",
    "\n",
    "# Match stealth to its carryover\n",
    "preds_season_stealth = pd.merge(\n",
    "    preds_season_stealth,\n",
    "    additions,\n",
    "    how = 'left', left_index = True, right_on = 'stealth_article'\n",
    ").drop('stealth_article', axis = 1).rename(columns = {'article': 'article_number'}).set_index('article_number')\n",
    "\n",
    "# Combine DAA + eCom\n",
    "preds_season_stealth = pd.merge(\n",
    "    preds_season_stealth.reset_index(),  # DAA forecasts\n",
    "    rma2,                                # eCom RMA2 forecast\n",
    "    how = 'left', left_on='article_number', right_on='Article Number'\n",
    ").drop('Article Number', axis = 1).round()\n",
    "\n",
    "# add price/cost for optimal overbuy\n",
    "preds_season_stealth = pd.merge(preds_season_stealth, cost_margin, how = 'left', left_on = 'article_number', right_index=True).round()\n",
    "\n",
    "\n",
    "\n",
    "# see evaluation.ipynb for sd estimation \n",
    "\n",
    "opt_ovb_stealth = pd.DataFrame(preds_season_stealth.\n",
    "                           apply(lambda row: minimize_EL(row['y_hat'], 550 + 0.2*row['y_hat'], row['margin'], row['cost']), axis=1)\n",
    "                          )\n",
    "\n",
    "opt_ovb_stealth = opt_ovb_stealth.rename(columns = {opt_ovb_stealth.columns[0]: 'Opt_Ovb'})\n",
    "\n",
    "# Combine with data\n",
    "preds_season_stealth = pd.merge(\n",
    "    preds_season_stealth, # everything\n",
    "    opt_ovb_stealth,      # optimal overbuy\n",
    "    right_index= True, left_index= True\n",
    ")\n",
    "\n",
    "# IMPACT\n",
    "# preds_season_stealth['impact'] = preds_season_stealth.apply(lambda row: np.abs(row['y_hat'] - row['WE eCom'])*(row['cost'] + row['margin']), axis = 1).round()                                                \n",
    "# preds_season_stealth = preds_season_stealth.sort_values('impact', ascending = False).round()\n",
    "             \n",
    "# Reorder for concatenating\n",
    "preds_season_stealth = preds_season_stealth[['article_number', 'y_hat', 'WE eCom', 'price', 'cost', 'margin', 'Opt_Ovb']]\n",
    "                                            \n",
    "                                            \n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine stealth with the rest\n",
    "\n",
    "preds_season = pd.concat([preds_season, preds_season_stealth], sort=True)[['article_number', 'y_hat', 'WE eCom', 'price', 'cost', 'margin', 'Opt_Ovb']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_season.to_excel('data/SS20_forecasts_inclusive.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_excel('data/SS20_forecasts_all.xlsx')\n",
    "\n",
    "dat = dat[dat.net_forecast > dat.eCom_FC]\n",
    "\n",
    "dat = dat.set_index('article_number')\n",
    "\n",
    "dat.to_excel('data/SS20_forecasts_6Dec2019.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CLEARANCE y/n -- \n",
    "\n",
    "# INCLUDED --  \n",
    "\n",
    "# preds.to_excel('data/preds_all.xlsx')\n",
    "# preds_season.to_excel('data/preds_season_all.xlsx')\n",
    "\n",
    "\n",
    "# NOT included --\n",
    "\n",
    "# preds.to_excel('data/preds.xlsx')\n",
    "preds.head()\n",
    "\n",
    "# preds_season.to_excel('data/preds_season.xlsx')\n",
    "preds_season.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# --- READ IN non/all DATA --- \n",
    "\n",
    "# preds_all = pd.read_excel('data/preds_all.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "# preds_season_all = pd.read_excel('data/preds_season_all.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# preds = pd.read_excel('data/preds.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "# preds_season = pd.read_excel('data/preds_season.xlsx').drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Columns (add, rename, AA), Rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_season['pct_difference'] = ((preds_season.y_hat - preds_season['eCom Market FC'])/preds_season['eCom Market FC'] * 100).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# April Allen requested columns \n",
    "\n",
    "# aa_cols = pd.read_csv('data/columns_22october.csv', ...)\n",
    "# aa_cols = pd.read_csv('data/columns_11November.csv', ...)\n",
    "\n",
    "aa_cols = pd.read_csv('data/columns_21November.csv', low_memory = False, error_bad_lines = False, sep = \";\")\n",
    "aa_cols.at[0, 'FW19_total_market_FC'] = 400\n",
    "\n",
    "# Add leading zero to short article numbers\n",
    "for i in aa_cols.index:\n",
    "    if len(aa_cols.iloc[i]['article_number']) == 5:\n",
    "        aa_cols.at[i, 'article_number'] = '0' + aa_cols.iloc[i]['article_number']\n",
    "        \n",
    "aa_cols = aa_cols.drop_duplicates(subset = 'article_number') # One duplicate\n",
    "\n",
    "preds_season = pd.merge(preds_season, aa_cols, how = 'left', left_on = 'article_number', right_on = 'article_number')\n",
    "\n",
    "preds_season['FW19_total_ecom_SO'] = [np.float(str(x).replace('.', '')) for x in preds_season.FW19_total_ecom_SO] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dat0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = pd.merge(preds_season, ref_dat0[['article_no', 'model_no', 'art_desc', 'brand_desc',\n",
    "       'bus_unit_desc', 'rmh_cat_desc', 'prod_grp_desc', 'prod_type_desc',\n",
    "       'gender_desc', 'age_group_desc']], left_on='article_number', right_on='article_no', how = 'left').drop('article_no', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "preds_season = preds_season.rename(columns = {'y_hat': 'net_forecast', 'Opt_Ovb': 'buy_recommendation', 'art_desc': 'description', 'eCom Market FC': 'eCom_FC', \n",
    "                                              'prod_type_desc': 'type', 'brand_desc': 'brand', 'bus_unit_desc': 'BU', 'rmh_cat_desc': 'RMH'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_season = preds_season[['article_number',  'brand', 'model_no', 'description', 'type', 'BU', 'RMH', \n",
    "                             'price', 'cost', 'margin', 'net_forecast', 'buy_recommendation', 'eCom_FC']] # 'eCom_ILS1', \n",
    "                             #'FW19_total_market_FC', 'FW19_total_ecom_SO', 'FW19_total_ecom_RDP']] # space after RDP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = preds_season.set_index('article_number')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to integers to remove '.0' endings\n",
    "for c in preds_season.columns:\n",
    "    if type(preds_season[c][1]) == np.float64:\n",
    "        preds_season.loc[:, c] = preds_season[c].fillna(0).replace(np.inf, 0).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = preds_season.replace('Error', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season = preds_season[['brand', 'model_no', 'description', 'price',\n",
    "       'cost', 'margin', 'net_forecast', 'buy_recommendation', 'eCom_FC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Managing 'inclusive' irregularities -- \n",
    "\n",
    "# preds_season.loc[:, 'impact'] = preds_season.apply(lambda row: np.abs(row['net_forecast'] - row['eCom_ILS1'])*(row['cost'] + row['margin']), axis = 1).round()                                                \n",
    "             \n",
    "# preds_season_all = pd.read_excel('data/preds_season_all.xlsx').drop('Unnamed: 0', axis = 1).rename(columns = {'y_hat': 'net_forecast_incl_clearance', 'Opt_Ovb': 'buy_rec_incl_clearance'})\n",
    "\n",
    "\n",
    "# preds_season.shape\n",
    "\n",
    "# preds_season_both = pd.merge(preds_season, preds_season_all[['article_number', 'net_forecast_incl_clearance', 'buy_rec_incl_clearance']], how = 'left')\n",
    "\n",
    "# preds_season_both = preds_season_both[['article_number', 'brand', 'model_no', 'description', 'type', 'BU', 'RMH', 'price', 'cost', 'margin', \n",
    "#                                        'net_forecast', 'buy_recommendation', 'net_forecast_incl_clearance', 'buy_rec_incl_clearance', 'eCom_ILS1', # 'pct_difference',\n",
    "#                                        'FW19_total_market_FC', 'FW19_total_ecom_SO', 'FW19_total_ecom_RDP']].set_index('article_number')\n",
    "\n",
    "# preds_season_both['buy_rec_incl_clearance'] = np.where(preds_season_both['buy_rec_incl_clearance'] > preds_season_both['buy_recommendation'], preds_season_both['buy_rec_incl_clearance'], preds_season_both['buy_recommendation'])\n",
    "# preds_season_both['net_forecast_incl_clearance'] = np.where(preds_season_both['net_forecast_incl_clearance'] > preds_season_both['net_forecast'], preds_season_both['net_forecast_incl_clearance'], preds_season_both['net_forecast'])\n",
    "\n",
    "# # Convert to integers to remove '.0' endings\n",
    "# for c in preds_season_both.columns:\n",
    "#     if type(preds_season_both[c][1]) == np.float64:\n",
    "#         preds_season_both.loc[:, c] = preds_season_both[c].fillna(0).replace(np.inf, 0).astype(int)\n",
    "\n",
    "\n",
    "# preds_season_both = preds_season_both[~preds_season_both.index.isin(unreliable)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load including-clearance forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_season.to_excel('FW20_rbk_model_stealths_incl_cl.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_all = pd.read_excel('data/preds_all.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "# preds_season_all = pd.read_excel('data/preds_season_all.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# preds = pd.read_excel('data/preds.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "# preds_season = pd.read_excel('data/preds_season.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "preds_season_both = pd.read_excel('FW20_forecasts_incl_cl.xlsx')#.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season_both = preds_season_both.rename(columns = {'eCom_ILS1': 'eCom_21Nov'}).set_index('article_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_season_both['net_forecast_diff'] = preds_season_both['net_forecast_incl_clearance'] - preds_season_both['net_forecast']\n",
    "# preds_season_both.sort_values('net_forecast_diff', ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adidas stealth eda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_season_both = pd.read_excel('FW20_forecasts_incl_cl.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dat0 = pd.read_csv('data/article_reference_data.csv', low_memory = False, error_bad_lines = False,\n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'brand_desc', 'colorway_long_descr',\n",
    "                                  'primary_color', 'secondary_color', 'tertiary_color', 'quarternary_color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dat0.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Magnifying Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unreliable forecasts -- \n",
    "\n",
    "unreliable = list(['G26523', 'EJ9682', 'EE9391', 'EE8947', 'EE8943', 'BS0980', 'EE4727']) \n",
    "\n",
    "# preds_season = preds_season[~preds_season.article_number.isin(unreliable)]\n",
    "# preds_season = preds_season[~preds_season.index.isin(unreliable)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compare DAA & eCom -- \n",
    "preds_season.loc[:, 'diff'] = preds_season['net_forecast'] - preds_season['eCom_FC']\n",
    "\n",
    "preds_season.sort_values('diff', ascending = False)[['net_forecast', 'buy_recommendation', 'eCom_FC',\n",
    "       'diff', 'brand', 'model_no', 'description', 'type', 'BU', 'RMH']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Magnifying glass - article \n",
    "\n",
    "aoi = 'S21489'\n",
    "a = aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magnifying class -- \n",
    "\n",
    "preds_season[preds_season.article_number == a][['article_number', 'net_forecast', 'buy_recommendation', \n",
    "        'eCom_FC', 'diff', 'brand', 'model_no', 'description', 'type', 'BU', 'RMH']]\n",
    "\n",
    "dat_a = preds[preds.article_number == a][['week', 'net_qty', 'corrected', 'y_hat']].round()\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_a.sort_values('week').set_index('week').plot(linewidth = 3)\n",
    "\n",
    "dat_a.set_index('week').apply(np.sum).round()\n",
    "dat_a.sort_values('week').set_index('week').round()\n",
    "\n",
    "# dat0[dat0.article_number == aoi].sort_values(['country', 'year', 'week'])#.iloc[1,]\n",
    "\n",
    "# plot -- \n",
    "dat_aoi = dat0[dat0.article_number == aoi].copy()\n",
    "dat_aoi = pd.merge(pd.DataFrame(dat_aoi.groupby(['year', 'week'])['net_qty'].sum()).reset_index(), dat_aoi[['year', 'week']].drop_duplicates())\n",
    "dat_aoi.year = [str(x) for x in dat_aoi.year]\n",
    "dat_aoi.week = [str(x) for x in dat_aoi.week]\n",
    "dat_aoi['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aoi.year, dat_aoi.week)]\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_aoi[['date', 'net_qty']].set_index('date').plot(linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# ---- Comments ---- \n",
    "\n",
    "if preds_season.index.name != 'article_number':\n",
    "    preds_season = preds_season.set_index('article_number')\n",
    "\n",
    "preds_season['notes'] = '-'\n",
    "preds_season.loc['EF0371', 'notes'] = 'Short FW19; our forecast assumes full FW20, with performance on par with observed FW19'\n",
    "preds_season.loc['EE7570', 'notes'] = 'Short FW19, so not a lot of data to work with. Our forecast assumes full FW20, with performance on par with observed FW19'\n",
    "preds_season.loc['F36641', 'notes'] = 'Forecast assumes FULL availability throughout FULL FW20'\n",
    "preds_season.loc['EE6999', 'notes'] = 'late drop, our forecast assumes a full FW20'\n",
    "preds_season.loc['CG6193', 'notes'] = 'Seasonality component of model is predicting strong end to FW19'\n",
    "preds_season.loc['EE6464', 'notes'] = 'Late drop in FW19, but our forecast assumes full FW20.'\n",
    "preds_season.loc['EE7773', 'notes'] = 'Shortened FW19, but forecast assumes full FW20.'\n",
    "preds_season.loc['EE7775', 'notes'] = 'Short FW19, but our forecast assumes full FW20'\n",
    "\n",
    "# preds_season.loc['DV0152', 'notes'] = 'All FW19 transactions are clearance.'\n",
    "# preds_season.loc['DV0169', 'notes'] = 'All FW19 transactions are clearance.'\n",
    "# preds_season.loc['DV2848', 'notes'] = 'Lots of clearance transactions (out of scope) pushed our numbers down'\n",
    "\n",
    "# preds_season.loc['x', 'notes'] =\n",
    "# preds_season.loc['x', 'notes'] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ---- Comments ---- \n",
    "\n",
    "if preds_season_both.index.name != 'article_number':\n",
    "    preds_season_both = preds_season_both.set_index('article_number')\n",
    "\n",
    "preds_season_both['notes'] = '-'\n",
    "preds_season_both.loc['EF0371', 'notes'] = 'Short FW19; our forecast assumes full FW20, with performance on par with observed FW19'\n",
    "preds_season_both.loc['EE7570', 'notes'] = 'Short FW19, so not a lot of data to work with. Our forecast assumes full FW20, with performance on par with observed FW19'\n",
    "preds_season_both.loc['F36641', 'notes'] = 'Forecast assumes FULL availability throughout FULL FW20'\n",
    "preds_season_both.loc['EE6999', 'notes'] = 'late drop, our forecast assumes a full FW20'\n",
    "preds_season_both.loc['CG6193', 'notes'] = 'Seasonality component of model is predicting strong end to FW19'\n",
    "preds_season_both.loc['EE6464', 'notes'] = 'Late drop in FW19, but our forecast assumes full FW20.'\n",
    "preds_season_both.loc['EE7773', 'notes'] = 'Shortened FW19, but forecast assumes full FW20.'\n",
    "preds_season_both.loc['EE7775', 'notes'] = 'Short FW19, but our forecast assumes full FW20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save -- \n",
    "\n",
    "# preds_season.to_excel(\"FW20_forecasts.xlsx\")\n",
    "# preds_season_both.to_excel('FW20_forecasts_incl_cl.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-clearance vs. All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate w/ one article focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "dat0 = pd.read_csv('data/ch4k.csv')\n",
    "ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, error_bad_lines = False, \n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "                                  'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc',\n",
    "                                  'brand_desc', 'bus_unit_desc', 'rmh_cat_desc'])\n",
    "\n",
    "# All transactions\n",
    "dat_all = dat0.copy()\n",
    "\n",
    "# Non-clearance transactions\n",
    "\n",
    "# dat = dat0.copy()\n",
    "# dat['clearance'] = dat.clearance.fillna(0) \n",
    "# dat['net_qty'] = (1 - dat.clearance)*dat.net_qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = 'EE7570'\n",
    "\n",
    "#dat_eda = dat[(dat.article_number.isin([aoi])) & (dat.season.isin(['FW19', 'FW18', 'FW17', 'FW16', 'FW15']))].copy()\n",
    "dat_eda = dat_all[(dat_all.article_number.isin([aoi])) & (dat_all.season.isin(['FW19', 'FW18', 'FW17', 'FW16', 'FW15']))].copy()\n",
    "\n",
    "dat_eda = dat_eda[['article_number', 'year', 'week', 'country', 'season', 'net_qty', 'buy_availability']].sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# GAS step\n",
    "\n",
    "dat_GAS_eda = dat_eda.groupby(['article_number', 'country']).apply(GAS_est).reset_index()\n",
    "dat_GAS_eda = pd.DataFrame(dat_GAS_eda.groupby(['article_number', 'year', 'week'])['GAS_est'].sum()).reset_index()\n",
    "\n",
    "# both = pd.read_excel('data/both.xlsx')\n",
    "# both['diff'] = both.apply(lambda row: row['net_forecast_y'] - row['net_forecast_x'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# SEASONALITY step\n",
    "\n",
    "seasonality_dat_eda = (dat_all[['article_number', 'year', 'week', 'country', 'season', 'net_qty', \n",
    "                                'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', \n",
    "                                'age_group_desc', 'franchise', 'prod_grp_desc']].\n",
    "                       dropna().\n",
    "                       sort_values(['article_number', 'year', 'week']).\n",
    "                       copy()\n",
    "                  )\n",
    "\n",
    "# -- Sum over UK/EU, ADD article reference data --\n",
    "seasonality_dat_eda = pd.merge(\n",
    "    pd.DataFrame(seasonality_dat_eda.groupby(['article_number', 'season', 'year', 'week'])['net_qty'].sum()).reset_index(), # sum over UK & EU\n",
    "    seasonality_dat_eda[['article_number', 'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].drop_duplicates() # add reference information\n",
    "    ).dropna().sort_values(['article_number', 'year', 'week'])\n",
    "\n",
    "\n",
    "\n",
    "# -- Reliable, mirror seasons --\n",
    "seasonality_dat_eda = seasonality_dat_eda[seasonality_dat_eda.season.isin(['FW15', 'FW16', 'FW17', 'FW18', 'FW19'])]\n",
    "\n",
    "# ---- Calculate cat-level weekly means across *ALL SEASONS* ---- \n",
    "\n",
    "seasonality_sport   = pd.DataFrame(seasonality_dat_eda.groupby(['sports_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'sport_weekly_mean'})\n",
    "seasonality_rmh     = pd.DataFrame(seasonality_dat_eda.groupby(['rmh_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'rmh_weekly_mean'})\n",
    "seasonality_gndr    = pd.DataFrame(seasonality_dat_eda.groupby(['gender_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'gender_weekly_mean'})\n",
    "seasonality_agegrp  = pd.DataFrame(seasonality_dat_eda.groupby(['age_group_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'age_weekly_mean'})\n",
    "seasonality_frnchse = pd.DataFrame(seasonality_dat_eda.groupby(['franchise', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'franchise_weekly_mean'})\n",
    "seasonality_prdgrp  = pd.DataFrame(seasonality_dat_eda.groupby(['prod_grp_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'prd_grp_weekly_mean'})\n",
    "\n",
    "seasonality_dfs = [seasonality_sport, seasonality_rmh, seasonality_gndr, seasonality_agegrp, seasonality_frnchse, seasonality_prdgrp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "seasonality_dat_eda = seasonality_dat_eda[(seasonality_dat_eda.season == 'FW19') & (seasonality_dat_eda.article_number == aoi)]\n",
    "\n",
    "\n",
    "\n",
    "preds_eda = seasonality_dat_eda.groupby(['article_number']).apply(regress).reset_index()\n",
    "\n",
    "# Merge back with reference data\n",
    "preds_eda = pd.merge(\n",
    "    preds_eda,\n",
    "    seasonality_dat_eda,\n",
    "    how = 'left').sort_values(['article_number', 'week'])[['article_number', 'year', 'week', 'net_qty', 'seas_preds']] # .fillna(method='ffill')\n",
    "\n",
    "# Zero out negative preds\n",
    "preds_eda['seas_preds'] = np.where(preds_eda.seas_preds > 0, preds_eda.seas_preds, 0) \n",
    "\n",
    "# Combined observed weeks (partial season)  --- AND --- regression predicted (all) weeks\n",
    "preds_eda = pd.merge(\n",
    "    preds_eda,   # all weeks\n",
    "    dat_GAS_eda, # observed weeks\n",
    "    how = 'left')\n",
    "\n",
    "# weekly assignment of GAS, seasonality, or combination\n",
    "preds_eda['y_hat'] = np.where(np.isnan(preds_eda.GAS_est), preds_eda.seas_preds, (preds_eda.GAS_est + preds_eda.seas_preds)/2).round()\n",
    "\n",
    "# Sum over season\n",
    "# preds_season_eda = pd.DataFrame(preds_eda.groupby('article_number')['y_hat'].apply(sum).round())\n",
    "\n",
    "# Growth\n",
    "# preds_season_eda['y_hat'] = preds_season_eda.y_hat # * 1.1 # default growth rate\n",
    "                                           \n",
    "                                            \n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EE7570_0 = preds_eda.copy()\n",
    "EE7570_0.head()\n",
    "\n",
    "# EE7570_1 = preds_eda.copy()\n",
    "\n",
    "EE7570_0.head()\n",
    "EE7570_1.head()\n",
    "\n",
    "both[both.article_number == aoi].head()\n",
    "\n",
    "both.shape\n",
    "# both[['net_forecast_x', 'net_forecast_y', 'diff']]\n",
    "\n",
    "both['diff'].describe().round()\n",
    "both['diff'].hist(bins = [-2500, 0, 1000, 12000])\n",
    "# without clearance ---- with clearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both.to_excel('data/both.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare, Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both = pd.read_excel('data/SS20_forecasts_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_season = pd.read_excel('data/SS20_forecasts_5Dec2019.xlsx')#.drop('Unnamed: 0', axis = 1)\n",
    "# preds_season_all = pd.read_excel('data/preds_season_all.xlsx').drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_season_all0 = preds_season.copy()\n",
    "# preds_season_all = preds_season.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "preds_season_all = preds_season_all.rename(columns = {'net_forecast': 'net_forecast_w_clearance', 'buy_recommendation': 'buy_recommendation_w_clearance',})\n",
    "\n",
    "both = pd.merge(preds_season, preds_season_all[['article_number', 'net_forecast_w_clearance', 'buy_recommendation_w_clearance']],\n",
    "                left_on='article_number', right_on = 'article_number')\n",
    "\n",
    "both['buy_recommendation_w_clearance'] = np.where(both['buy_recommendation_w_clearance'] > both['buy_recommendation'], both['buy_recommendation_w_clearance'], both['buy_recommendation'])\n",
    "both['net_forecast_w_clearance'] = np.where(both['net_forecast_w_clearance'] > both['net_forecast'], both['net_forecast_w_clearance'], both['net_forecast'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both['net_forecast_diff'] = both['net_forecast_w_clearance'] - both['net_forecast']\n",
    "both['buy_rec_diff'] = both['buy_recommendation_w_clearance'] - both['buy_recommendation']\n",
    "\n",
    "both = both.sort_values('buy_rec_diff', ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove clearance transactions\n",
    "dat3 = dat0.copy()\n",
    "dat3['clearance'] = dat3.clearance.fillna(0) \n",
    "dat3['net_qty0'] = (1 - dat3.clearance)*dat3.net_qty\n",
    "\n",
    "both = pd.merge(both, dat3[dat3.article_number.isin(both.article_number) & (dat3.season == 'SS19')].groupby('article_number')['net_qty0', 'net_qty'].sum().astype('int'), \n",
    "         left_on = 'article_number', right_on = 'article_number').rename(columns = {'net_qty0': 'SS19_net_qty0', 'net_qty': 'SS19_net_qty'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both[['article_number', 'description', \n",
    "       'RMH', 'SS19_net_qty0', 'SS19_net_qty', 'net_forecast', 'buy_recommendation',\n",
    "       'net_forecast_w_clearance', 'buy_recommendation_w_clearance', 'eCom_FC',\n",
    "       'buy_rec_diff']]#.to_excel('data/forecast_eda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both[['article_number',  'brand', 'model_no', 'description', 'type', 'BU', 'RMH', \n",
    "        'price', 'cost', 'margin', 'net_forecast', 'buy_recommendation', 'net_forecast_w_clearance', 'buy_recommendation_w_clearance', 'eCom_FC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both = both#[['article_number',  'brand', 'model_no', 'description', 'type', 'BU', 'RMH', \n",
    "        #'price', 'cost', 'margin', 'net_forecast', 'buy_recommendation', 'net_forecast_w_clearance', 'buy_recommendation_w_clearance', 'eCom_FC']].set_index('article_number')\n",
    "\n",
    "# both.to_excel('data/SS20_forecasts_all.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # adidas v rbk\n",
    "# from pandas import DataFrame, Series\n",
    "# fw20['brand'].value_counts()\n",
    "\n",
    "# # Carryover coverage\n",
    "# fw20['rev'] = fw20.price * fw20.eCom_RMA1\n",
    "# fw20.groupby('brand')['rev'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # DAVID code\n",
    "\n",
    "# # Function Definitions\n",
    "\n",
    "# class parameters():\n",
    "#     def __init__(self, param_array):\n",
    "#         self.alpha = param_array[0]\n",
    "#         self.beta = param_array[1]\n",
    "#         self.omega = param_array[2] / (1-self.beta) # one way to choose that is omega/(1-beta) = uncoMnditional mean\n",
    "#         self.sigma = param_array[3]\n",
    "#         self.f0 = param_array[4] # one way to choose is unconditional mean\n",
    "\n",
    "# def loglik(x, y, f, sigma):\n",
    "#     ll = -1/2*np.log(2*np.pi ) - 1/2*np.log(sigma) - 1/(2*sigma)*(y - x*f)**2\n",
    "#     return ll\n",
    "\n",
    "# def score_compute(x, y, f, sigma):\n",
    "#     return (y - x*f)/sigma\n",
    "\n",
    "# def score_compute_2(x, y, f, sigma=None):\n",
    "#     return(y - x*f) # ** The 'type = 2' modification **\n",
    "\n",
    "# def filterGAS(p, x, y, score_fun):\n",
    "#     score0 = score_fun(x[0,:], y[0,:],p.f0, p.sigma)\n",
    "#     f = np.zeros((len(y),1))\n",
    "#     f[0,:] = p.f0\n",
    "#     for t in range(1,len(y)):\n",
    "#         scoret = score_fun(x[t-1,:], y[t-1,:], f[t-1,:], p.sigma)\n",
    "#         f[t,:] = p.omega + p.alpha*scoret + p.beta*f[t-1,:]\n",
    "#     return f\n",
    "\n",
    "\n",
    "# def loglikest(params, x, y, score_fun):\n",
    "#     p = parameters(params)\n",
    "#     f = filterGAS(p, x, y, score_fun)\n",
    "#     ll = np.zeros((len(y), 1))\n",
    "#     m = len(y)\n",
    "#     for t in range(0, len(y)):\n",
    "#         ll[t,:] = loglik(x[t,:], y[t,:], f[t,:], p.sigma)\n",
    "#     loglik_res = -(np.sum(ll))/m\n",
    "#     return loglik_res\n",
    "\n",
    "\n",
    "# def GAS_optimize(x, y, score_fun, marker_str):\n",
    "#     return scipy.optimize.minimize(\n",
    "#        loglikest,                              # function to minimize (log likelihood y|x,theta)\n",
    "#        np.array([0.8, 0.9, np.mean(y), 1, np.mean(y)]), # initial parameter values (starting)\n",
    "#        args=(x, y, score_fun),\n",
    "#        options ={'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12}, # TODO pass as parameter or create config file\n",
    "#        method='L-BFGS-B',\n",
    "#        bounds=((0,  None),             # alpha\n",
    "#                (-1, 1),                # beta\n",
    "#                (0.001, np.mean(y)*2),  # omega\n",
    "#                (0.001, None),          # sigma\n",
    "#                (0.001, np.mean(y)*2)   # f\n",
    "#               )\n",
    "#        )\n",
    "\n",
    "\n",
    "\n",
    "# def GAS_est(df):\n",
    "#     \"\"\" <High level description of function>\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df : pandas DataFrame\n",
    "#        <Description>\n",
    "#     Returns\n",
    "#     -------\n",
    "#     ret: pandas DataFrame\n",
    "#        <Description>\n",
    "#     Raises\n",
    "#     ------\n",
    "#     (List and description of specific errors generated and thrown based on intenal function requirements)\n",
    "#     OtherError when an other error\n",
    "#     \"\"\"\n",
    "#     y = df.net_qty.values.reshape(-1,1)          # observed demand (response)\n",
    "#     x = df.buy_availability.values.reshape(-1,1)   # buy_availability (explanatory)\n",
    "    \n",
    "#     ret = pd.DataFrame()\n",
    "#     ret[['year','week']] = df[['year','week']]\n",
    "    \n",
    "#     score_fun = score_compute\n",
    "#     marker_str = 'One'\n",
    "    \n",
    "#     opt_result = GAS_optimize(x, y, score_fun, marker_str)\n",
    "    \n",
    "#     if opt_result.success == False:\n",
    "#         score_fun= score_compute_2\n",
    "#         marker_str = 'Two'\n",
    "#         opt_result = GAS_optimize(x, y, score_fun, marker_str)\n",
    "        \n",
    "#     x1par = parameters(opt_result.x)\n",
    "#     GAS = filterGAS(x1par, x, y, score_fun)\n",
    "    \n",
    "#     ret['GAS_est'] = GAS\n",
    "#     ret['Convergence'] = [opt_result.success] * len(y)\n",
    "#     ret['Convg type'] = [marker_str] * len(y)\n",
    "    \n",
    "#     return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# April articles\n",
    "\n",
    "aas = ['DV1549', 'EE1152', 'DV1508', 'ED6024', 'CY4574', 'ED9384', 'BK7345', 'DV2400', 'DH5798']\n",
    "\n",
    "# aoi = 'DV1549'\n",
    "# a = aoi\n",
    "\n",
    "preds_aa = (preds[preds.article_number.isin(aas)][['article_number', 'week', 'net_qty', 'GAS_est', 'seas_preds', 'y_hat']].\n",
    "            sort_values(['article_number', 'week']).\n",
    "            set_index('week')).round()\n",
    "        \n",
    "\n",
    "preds_aa[preds_aa.article_number == 'DV1549']\n",
    "\n",
    "pred_aggs_aa = preds_aa.groupby('article_number')[['net_qty', 'GAS_est', 'seas_preds', 'y_hat']].apply(sum).round()\n",
    "\n",
    "\n",
    "\n",
    "for c in pred_aggs_aa.columns:\n",
    "    if type(pred_aggs_aa[c][1]) == np.float64:\n",
    "        pred_aggs_aa[c] = pred_aggs_aa[c].fillna(0).astype(int)\n",
    "\n",
    "pred_aggs_aa[~np.isnan(pred_aggs_aa.GAS_est)].loc[aas[i-1], 'y_hat'].sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25,32)); # width, height\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.2); # vertical spacing, horizontal spacing\n",
    "for i in range(1, 10):\n",
    "    ax = fig.add_subplot(5, 2, i, )\n",
    "    preds_ax = preds_aa[preds_aa.article_number == aas[i-1]]\n",
    "    ax.plot(preds_ax.index, preds_ax['net_qty'], linewidth=4.5, label = 'Observed net_qty')\n",
    "    ax.plot(preds_ax.index, preds_ax['y_hat'], linewidth=4.5, label = 'Model net_qty estimate')\n",
    "    ax.set_title('Article: ' + aas[i-1] + \n",
    "                 ' \\n net_qty STD: ' + str(pred_aggs_aa.loc[aas[i-1], 'net_qty']) + \n",
    "                 ' \\n Full availability estimate STD: ' + str(preds_aa[(~np.isnan(preds_aa.GAS_est)) & (preds_aa.article_number == aas[i-1])].y_hat.sum().astype(int)) +\n",
    "                 ' \\n FW19 full season estimate: ' + str(pred_aggs_aa.loc[aas[i-1], 'y_hat']), \n",
    "                 fontsize=16)\n",
    "    ax.legend()\n",
    "\n",
    "fig.savefig('aa_fig.png')    \n",
    "\n",
    "pass;\n",
    "\n",
    "\n",
    "\n",
    "preds_aa[['GAS_est', 'net_qty', 'seas_preds']].apply(np.sum).round()\n",
    "\n",
    "preds_season.reset_index()[preds_season.index.isin(aas)]\n",
    "\n",
    "dat_aa = dat0[dat0.article_number.isin(aas)].copy()\n",
    "\n",
    "dat_aa = pd.merge(\n",
    "    pd.DataFrame(dat_aa.groupby(['year', 'week'])['net_qty'].sum()).reset_index(),\n",
    "    dat_aa[['year', 'week']].drop_duplicates()\n",
    ")\n",
    "\n",
    "dat_aa.year = [str(x) for x in dat_aa.year]\n",
    "dat_aa.week = [str(x) for x in dat_aa.week]\n",
    "dat_aa['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aa.year, dat_aa.week)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,7]\n",
    "dat_aa[['date', 'net_qty']].set_index('date').plot(linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ----- For demo ------\n",
    "\n",
    "fcsts = preds_season.copy()\n",
    "\n",
    "# fcsts = pd.read_excel('data/FW20_forecasts.xlsx') # adidas RMA2 -- 21 October\n",
    "\n",
    "\n",
    "fcsts.loc[:, 'impact'] = fcsts.apply(lambda row: np.where(row['buy_recommendation'] > row['eCom_ILS1'], \n",
    "                                                         (row['buy_recommendation'] - row['eCom_ILS1']) * row['margin'], \n",
    "                                                         (row['eCom_ILS1'] - row['buy_recommendation']) * row['cost']), axis = 1)\n",
    "\n",
    "fcsts = fcsts.sort_values('impact', ascending = False).round()\n",
    "\n",
    "fcsts = fcsts[['net_forecast', 'buy_recommendation', 'eCom_ILS1', 'impact', 'brand', 'description', 'type', 'BU', 'RMH', 'price', 'cost', 'margin',  'FW19_total_market_FC', 'notes']]\n",
    "\n",
    "\n",
    "fcsts.head()\n",
    "\n",
    "preds[preds.article_number == 'CG6708'].head()\n",
    "dat0[dat0.article_number == 'CG6708'].head()\n",
    "\n",
    "# EDA Plots\n",
    "\n",
    "# ---- Plot -----\n",
    "aoi = 'F34314'\n",
    "a = aoi\n",
    "\n",
    "# -------\n",
    "\n",
    "dat_a = preds[preds.article_number == a][['week', 'net_qty', 'GAS_est', 'y_hat']]\n",
    "dat_a['year'] = '2019'\n",
    "dat_a.week = [str(x) for x in dat_a.week]\n",
    "dat_a['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_a.year, dat_a.week)]\n",
    "\n",
    "# -------\n",
    "\n",
    "print('This season:')\n",
    "print(dat_a[['net_qty', 'GAS_est', 'y_hat']].apply(np.sum).round())\n",
    "\n",
    "dat_aoi = dat0[dat0.article_number == aoi].copy()\n",
    "\n",
    "dat_aoi = pd.merge(pd.DataFrame(dat_aoi.groupby(['year', 'week'])['net_qty'].sum()).reset_index(),dat_aoi[['year', 'week']].drop_duplicates())\n",
    "dat_aoi.year = [str(x) for x in dat_aoi.year]\n",
    "dat_aoi.week = [str(x) for x in dat_aoi.week]\n",
    "dat_aoi['date'] = [dt.datetime.strptime(x[0] + '-' + x[1] + '-1', \"%Y-%W-%w\") for x in zip(dat_aoi.year, dat_aoi.week)]\n",
    "\n",
    "# --------\n",
    "\n",
    "full = pd.merge(dat_a, dat_aoi, how = 'outer').sort_values('date').set_index('date')\n",
    "\n",
    "full = full[~((full.index > dt.datetime(2019, 5, 27))  & full.y_hat.isna())]\n",
    "# full = full.drop_duplicates(subset = ['week'], keep = 'last')\n",
    "\n",
    "full = full.rename(columns = {'y_hat': 'model net_qty'})\n",
    "\n",
    "# --------\n",
    "\n",
    "full.loc[dt.datetime(2019, 11, 4), 'net_qty'] = np.nan # manually change entry\n",
    "\n",
    "full # ************\n",
    "\n",
    "# ----\n",
    "plt.rcParams[\"figure.figsize\"] = [20,8]\n",
    "full.drop('GAS_est', axis = 1).plot(linewidth = 3)\n",
    "plt.ylabel('net_qty')\n",
    "plt.title('Article net_qty: ' + aoi)\n",
    "\n",
    "# -------\n",
    "\n",
    "fcsts[fcsts.index == aoi]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# dat_GAS_a = np.random.choice(dat_GAS.article_number.unique(), size = 100, replace = False)\n",
    "# dat_GAS = dat_GAS[dat_GAS.article_number.isin(dat_GAS_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# impact evaluations -- \n",
    "\n",
    "rma2_adi['rev'] = rma2_adi['Sum of WE eCom'] * rma2_adi['Market Retail Price']\n",
    "\n",
    "rma2_adi['carryover'] = rma2_adi['Article Number'].isin(carryovers)\n",
    "\n",
    "print('Total range: ', len(rma2_adi['Article Number'].unique()))\n",
    "rma2_adi.head()\n",
    "rma2_adi.carryover.value_counts() # non/carryover\n",
    "\n",
    "# Revenue total\n",
    "rma2_adi.rev.sum()\n",
    "rma2_adi.groupby('carryover')['rev'].sum().round()\n",
    "\n",
    "rbk_carryovers = carryovers.union(addtl_classics)\n",
    "\n",
    "rma2_rbk['rev'] = rma2_rbk['RBK WE eCom'] * rma2_rbk['Hub Retail Price']\n",
    "\n",
    "rma2_rbk['carryover'] = rma2_rbk['Article Number'].isin(rbk_carryovers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Overbuy statistics ---\n",
    "# preds_season['pct_overbuy'] = (preds_season.Opt_Ovb - preds_season.y_hat)/preds_season.y_hat*100\n",
    "# b = np.array([0, 1000, 2000, 5000, 10000, 50000])\n",
    "# preds_season['bins'] = pd.cut(preds_season.y_hat, bins = b)\n",
    "# preds_season.groupby('bins')['pct_overbuy'].describe().round()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
