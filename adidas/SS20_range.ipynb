{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Overstock-SupplyChain-Understock (OSU) Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# # Wall time: 3min 4s\n",
    "# %%time\n",
    "\n",
    "# dat = pd.read_csv('Transaction SS19.csv', low_memory=False) # *** DATA ***\n",
    "# dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])\n",
    "\n",
    "# # Transaction subset: SS19\n",
    "# dat = dat[['consumer_order_date', 'article_number', 'gross_demand_quantity', 'Sale', 'season']]\n",
    "# dat = dat[dat['season'] == 'SS19']\n",
    "\n",
    "# # Subset to non-clearance transactions only --- change here for David/Mike's request\n",
    "# dat = dat[dat['Sale'] == 0]\n",
    "\n",
    "# %%time\n",
    "# # Wall time: 4min 35s\n",
    "\n",
    "# dat_SS20_range = pd.read_csv('dat_SS20_range.csv') # *** DATA ***\n",
    "# SS20_range = dat_SS20_range['Article Number'].unique()\n",
    "# dat = dat[[(a in SS20_range) for a in dat['article_number']]]\n",
    "\n",
    "# # 'aggregate' to weekly sums by article for buy_availability merge and adjustment\n",
    "# dat.set_index('consumer_order_date', inplace = True)\n",
    "\n",
    "# dat = dat[['article_number', 'gross_demand_quantity']].groupby(['article_number']).resample('W').sum()\n",
    "# dat.reset_index(inplace=True)\n",
    "\n",
    "# # Add 'week' and 'year' for merging with stock (buy_availability) data (b/c min_date_of_week)\n",
    "# dat['week'] = [t.week for t in dat['consumer_order_date']]\n",
    "# dat['year'] = [t.year for t in dat['consumer_order_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat.to_csv('dat_SS20range.csv')\n",
    "\n",
    "dat = pd.read_csv('dat_SS20range.csv', low_memory=False, index_col = 0) # *** DATA ***\n",
    "dat['consumer_order_date'] = pd.to_datetime(dat['consumer_order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_number</th>\n",
       "      <th>consumer_order_date</th>\n",
       "      <th>gross_demand_quantity</th>\n",
       "      <th>week</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>011040</td>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>011040</td>\n",
       "      <td>2018-12-09</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>011040</td>\n",
       "      <td>2018-12-16</td>\n",
       "      <td>12.0</td>\n",
       "      <td>50</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>011040</td>\n",
       "      <td>2018-12-23</td>\n",
       "      <td>6.0</td>\n",
       "      <td>51</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>011040</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>4.0</td>\n",
       "      <td>52</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_number consumer_order_date  gross_demand_quantity  week  year\n",
       "0         011040          2018-12-02                    7.0    48  2018\n",
       "1         011040          2018-12-09                   15.0    49  2018\n",
       "2         011040          2018-12-16                   12.0    50  2018\n",
       "3         011040          2018-12-23                    6.0    51  2018\n",
       "4         011040          2018-12-30                    4.0    52  2018"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock data\n",
    "stock = pd.read_csv('Stock.csv', low_memory=False, index_col = 0) # *** DATA ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['year'] = [str(x)[0:4] for x in stock['week_id']]\n",
    "stock['week'] = [str(x)[4:6] for x in stock['week_id']]\n",
    "stock.drop('week_id', axis = 1, inplace = True)\n",
    "\n",
    "# Tidy\n",
    "stock.reset_index(inplace = True)\n",
    "stock.drop(['avg(ecom_available_stock)', 'avg(size_availability)'], axis = 1, inplace = True)\n",
    "stock.rename(columns = {'avg(buy_availability)': 'buy_availability'}, inplace = True)\n",
    "\n",
    "stock = stock[(stock['year'] != '2016') & (stock['year'] != '2017')]\n",
    "\n",
    "stock[['year', 'week']] = stock[['year', 'week']].astype('int64', copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge weekly demand df and buy_availability df -- for understock correction\n",
    "dat = pd.merge(dat, stock, \n",
    "               left_on = ['article_number', 'year', 'week'], \n",
    "               right_on = ['article_number', 'year', 'week'], \n",
    "               how = 'left')\n",
    "\n",
    "dat.fillna(1, inplace=True) # Assume buy_availability = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat[[x in ['DQ2136', 'DQ3089', 'DU0367', 'DU0369', 'DU0382', 'EE9806', 'EE9809', 'G28417'] for x in dat['article_number']]] \n",
    "# ------- Joerian: buy_availability DNE for these articles, for some reason -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----- Calculate *observed* full season gross_demand_quantity per article -----\n",
    "dat_season = pd.DataFrame(dat.groupby(['article_number'])['gross_demand_quantity'].sum())\n",
    "dat_season.rename(columns = {'gross_demand_quantity':'season_gross_demand_quantity'}, inplace = True)\n",
    "\n",
    "# WEEKLY averages for articles when buy_availability > 0.35\n",
    "dat_stocked = pd.DataFrame(\n",
    "    dat[dat['buy_availability'] > 0.35].\n",
    "    groupby(['article_number'])['gross_demand_quantity'].\n",
    "    mean())\n",
    "\n",
    "# {'604433', '620635', 'BQ1935', 'BQ2001', 'BS0980', 'CV4000', 'CY8772', 'G27026'}\n",
    "# These articles have ZERO weeks with: (1) buy_availability > 0.35   ***AND***   (2) gross_demand_quantity > 0\n",
    "\n",
    "dat_stocked.rename(columns = {'gross_demand_quantity':'corrected_weekly_avg_gross_demand_quantity'}, inplace= True)\n",
    "\n",
    "# Extend to full season (26 weeks) to estimate full season demand\n",
    "dat_stocked['corrected_gross_demand_quantity'] = 26*dat_stocked['corrected_weekly_avg_gross_demand_quantity'] \n",
    "\n",
    "dat_season = pd.merge(dat_season, dat_stocked, left_index=True, right_index=True, how = 'outer')\n",
    "dat_season.drop('corrected_weekly_avg_gross_demand_quantity', inplace=True, axis = 1)\n",
    "\n",
    "preds = (pd.DataFrame(pd.read_csv('Buyers predictions.csv', low_memory=False, index_col = 0))\n",
    "         [['season', 'ecom_marketing_forecast']].\n",
    "         reset_index().\n",
    "         dropna()\n",
    "        )\n",
    "\n",
    "preds = preds[preds['season'] == 'SS19']\n",
    "\n",
    "dat_season['corrected_gross_demand_quantity'] = np.where(\n",
    "    dat_season['corrected_gross_demand_quantity'].isna(),\n",
    "    dat_season['season_gross_demand_quantity'],\n",
    "    dat_season['corrected_gross_demand_quantity']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dat_season = pd.merge(\n",
    "    dat_season, preds, \n",
    "    left_index = True,\n",
    "    right_on = 'article', \n",
    "    how = 'left').round()\n",
    "\n",
    "dat_season = dat_season[['article', 'season_gross_demand_quantity', \n",
    "                         'corrected_gross_demand_quantity', 'ecom_marketing_forecast']]\n",
    "\n",
    "dat_season = dat_season[dat_season['season_gross_demand_quantity'] > 83]\n",
    "dat_season['DAA_SS20_prediction'] = dat_season['corrected_gross_demand_quantity']*1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename, reorder\n",
    "dat_season.rename(columns = {\n",
    "    'article': 'Article Number',\n",
    "    'season_gross_demand_quantity': 'SS19 Demand',\n",
    "    'corrected_gross_demand_quantity': 'SS19 Corrected Demand',\n",
    "    'ecom_marketing_forecast': 'SS19 eCom Forecast',\n",
    "    'DAA_SS20_prediction': 'Analytics SS20 Forecast',\n",
    "                            }, inplace = True)\n",
    "\n",
    "dat_season = dat_season[['Article Number', 'SS19 eCom Forecast', 'SS19 Demand', \n",
    "                         'SS19 Corrected Demand', 'Analytics SS20 Forecast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_season.to_csv('dat_season.csv')\n",
    "dat_season = pd.read_csv('dat_season.csv', low_memory=False, index_col = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- David code ---\n",
    "\n",
    "# buyers = pd.read_csv('article_managers.csv', low_memory=False, index_col = 0)\n",
    "\n",
    "# raw_ILS = pd.read_excel('SS20_RMA2_13May.xlsx',\n",
    "#                         sheet_name='RMA-02 Market Range Plan', header=4)\n",
    "\n",
    "# cols_ILS = {\n",
    "#     'Article Number': 'article_no', \n",
    "#     'CM': 'article_manager',\n",
    "#     'Product Division': 'product_div',\n",
    "#     'Article Business Segment': 'bus_seg',\n",
    "#     'GTM Target Retail Price EUR':'retail_price',\n",
    "#     'Product Group':'product_group',\n",
    "#     'Product Type':'product_type',\n",
    "#     'Sports Category':'category',\n",
    "#     'WE eCom ILS 1 BUY SIGN OFF final': 'quantity'\n",
    "# }\n",
    "\n",
    "# # reduce to fields of interest and add a max_revenue = price*quantity field\n",
    "\n",
    "# ils1 = raw_ILS[list(cols_ILS.keys())].rename(columns = cols_ILS).fillna(0).set_index('article_no')\n",
    "\n",
    "# ils1['max_revenue'] = ils1['quantity'] * ils1.retail_price\n",
    "\n",
    "# ils1.to_csv('ils1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ils1 = pd.read_csv('ils1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = (pd.merge(dat_season, ils1[['article_no', 'article_manager', 'quantity', 'retail_price']], \n",
    "                      left_on = 'Article Number', right_on = 'article_no', how = 'left').\n",
    "             set_index('Article Number').\n",
    "             drop('article_no', axis = 1).\n",
    "             rename(columns = {'quantity': 'eCom SS20 Forecast'})\n",
    "            )\n",
    "\n",
    "forecasts = forecasts[['SS19 eCom Forecast', 'SS19 Demand', 'SS19 Corrected Demand', 'eCom SS20 Forecast', \n",
    "                       'Analytics SS20 Forecast', 'article_manager', 'retail_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecasts[forecasts.article_manager == '0'] # missing managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Add cost to DF ----\n",
    "cost_price = pd.read_csv('Cost Price.csv', low_memory=False, index_col = 0)\n",
    "cost_price.rename(columns = {'avg(cost_of_sales)': 'cost'}, inplace = True)\n",
    "cost_price = pd.DataFrame(cost_price['cost'].groupby(cost_price.index).mean()).round()\n",
    "\n",
    "forecasts = pd.merge(forecasts, cost_price, left_index=True, right_index = True, how = 'left')\n",
    "\n",
    "diff = forecasts['eCom SS20 Forecast'] - forecasts['Analytics SS20 Forecast']\n",
    "forecasts['Difference-Cost'] = np.where(\n",
    "        diff > 0, \n",
    "        diff*forecasts['cost'],\n",
    "        diff*(-1)*(forecasts['retail_price'] - forecasts['cost'])\n",
    "    )\n",
    "del diff\n",
    "\n",
    "forecasts.sort_values('Difference-Cost', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecasts['article_manager'].value_counts()\n",
    "forecasts[forecasts['article_manager'] == '0'].index # ['218977', 'CF6925', 'CF6926', 'X35859', 'X53042']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.DataFrame({'comments': ['test']})\n",
    "\n",
    "# ----- post-apocalypse -----\n",
    "# comments.loc['B42200'] = 'This is strange article history data. In SS19 the sales were paltry, and the buy availability was below 0.35 most weeks. In the few weeks with sufficient buy availability, demand was 0. Is there reason to believe demand will be 3000?'\n",
    "\n",
    "# Based on gross_demand_quantity\n",
    "comments.loc['DB3258'] = 'This article had a very short SS19, but sold quite well during that time. Extrapolated out to a full season at that weekly rate the sales would have been 18000. Growth on this numbers yields our SS20 forecast.'\n",
    "comments.loc['G27706'] = 'The estimated full season demand in SS19 is about 50000. Do you have reason to expect a decline?'\n",
    "comments.loc['B28128'] = 'The eCom prediction this season was spot on; is there reason to believe demand will drop by 50%?'\n",
    "comments.loc['S75104'] = 'Demand in SS19 was about twice what was predicted; if demand for this article grows---or stays the same---the current eCom forecast would again be half of demand. Is this intentional? '\n",
    "comments.loc['EE8836'] = 'This article missed the first six weeks of the season, but correcting for that gives it an estimated SS19 demand of about 7500. Growth on top of that yields our forecast of around 8300.'\n",
    "comments.loc['G27637'] = 'In a familiar pattern, this article was not introduced until well into the season (March), and so SS20 full season sales should be much higher than the partial SS19, hence our prediction.'\n",
    "comments.loc['B37616'] = 'This article sold well, then abruptly stopped selling in the beginning of February; I do not have information on why this is. Assuming sufficient stock throughout SS20, full season sales should be higher than its abbreviated 10 week SS19.'\n",
    "comments.loc['G27639'] = 'This article debuted in March, but in a full season sales should be much higher; unless for some reason it will not be available for the full SS20 season.'\n",
    "comments.loc['CG5675'] = 'Not sure why, but we are showing an eCom forecast of 0, which is obviously quite different than our forecast. :)'\n",
    "comments.loc['DP2398'] = 'Demand in SS19 was about twice what was predicted; if demand for this article grows---or stays the same---the current eCom forecast would again be half of demand. Perhaps there is information we do not have access to that justifies this.'\n",
    "comments.loc['B28129'] = 'Demand in SS19 should be about 7800, with growth SS20 would see demand of around 8600. Is there reason to belive demand will decrease?'\n",
    "comments.loc['F36215'] = 'There were some supply chain fluctuations in SS19 where low buy availability affected demand. Correcting for this we estimate SS19 demand would have been about 5700. Growth on this account for our SS20 forecast of about 6300.'\n",
    "comments.loc['EE8925'] = 'This article missed all of December and half of January in SS19. Correcting for this we estimate an SS19 full season demand of about 4000; adding growth gives our SS20 prediction.'\n",
    "comments.loc['M20325'] = 'This is also a unique article. Sales for all of SS19 should be around 29500, so typical growth on that puts our estimate a bit above yours. The percentage difference between our prediction and yours is small, but underprediction is expensive, and therefore this discrepancy climed quite high on the list.'\n",
    "comments.loc['BB5478'] = 'SS19 demand should be around 8800; do you have reason to expect a decline?'\n",
    "comments.loc['G26880'] = 'Similar story; SS19 demand will end up at about 3700; our prediction is therefore about 4000 for SS20.'\n",
    "comments.loc['BD7633'] = 'I see SS19 demand at about 2700, and thus SS20 at about 3000; is reason to expect a decline?'\n",
    "comments.loc['B22705'] = 'By season\\'s end demand should be about 5500; is there reason to expect a drop in SS20'\n",
    "comments.loc['AQ1134'] = 'Demand for SS19 should be about 2100, but eCom forecast is 565. Reason to expect decline?'\n",
    "comments.loc['M20605'] = 'Demand should be between 6500 and 7000 by end of SS19; is there reason to expect a decline in SS20?'\n",
    "comments.loc['B96578'] = 'There were supply chain issues in the December which limited availability, and again in late January/early February. Without these problems we think demand would have been about 7000, and growth on that gives our SS20 prediction.'\n",
    "comments.loc['F36485'] = 'Why a drop from ~2000 in SS19 to ~500 in SS20?'\n",
    "comments.loc['G28109'] = 'This article did not start selling until February, but selling over a full season at the same rate would have yielded an estimated demand of ~2300. Growth on this yields our SS20 forecast.'\n",
    "comments.loc['B75806'] = 'eCom\\'s number for SS19 was spot on: by season\\'s end demand should be right around 5000. Should we expect a drop in SS20?'\n",
    "comments.loc['F34314'] = 'There were two weeks where buy availability affected demand. SS19 demand should be between 3500 and 4000, plus growth gives our SS20.'\n",
    "\n",
    "# ------- big net-diffs ---------\n",
    "\n",
    "comments.loc['280647'] = 'SS19 fell far short of expectations; and eCom SS20 forecast is almost 3X SS19 net. Is there sufficient justification for this big predicted increase?'\n",
    "comments.loc['280648'] = 'Fell far short of expectations in SS19; is there reason to believe net demand will quadruple from SS19 to SS20?'\n",
    "comments.loc['288022'] = 'Fell far short of expectations in SS19; is there reason to believe net demand will almost triple from SS19 to SS20?'\n",
    "comments.loc['CW1275'] = 'The net demand in SS19 will be about 7000; is there reason to believe this will jump to 18000 in SS20?'\n",
    "comments.loc['D95958'] = 'The net demand in SS19 will be around 4000; is there reason to believe this will jump to ~14000 in SS20?'\n",
    "comments.loc['DT7964'] = 'The net demand in SS19 will be around 4000; is there reason to believe this will jump to ~13000 in SS20?'\n",
    "comments.loc['F99787'] = 'Is there reason to expect demand to double from SS19 to SS20?'\n",
    "comments.loc['G28109'] = 'Very short SS19, but with a full season net quantity would have been an estimated ~1300; this puts our forecast at 3X yours.'\n",
    "comments.loc['M20324'] = 'This is a unique article. Sales for all of SS19 should be around 12500, so typical growth on that puts our estimate quite a bit below yours; do you have reason to believe demand will increase so much?'\n",
    "comments.loc['S82137'] = 'SS19 net should be about 10400, but eCom SS20 forecast is twice that. Why such a large increase?'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = (comments.\n",
    "            reset_index().\n",
    "            rename(columns = {'index': 'article'})[1:]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB3258</td>\n",
       "      <td>This article had a very short SS19, but sold q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G27706</td>\n",
       "      <td>The estimated full season demand in SS19 is ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B28128</td>\n",
       "      <td>The eCom prediction this season was spot on; i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S75104</td>\n",
       "      <td>Demand in SS19 was about twice what was predic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EE8836</td>\n",
       "      <td>This article missed the first six weeks of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G27637</td>\n",
       "      <td>In a familiar pattern, this article was not in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B37616</td>\n",
       "      <td>This article sold well, then abruptly stopped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>G27639</td>\n",
       "      <td>This article debuted in March, but in a full s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CG5675</td>\n",
       "      <td>Not sure why, but we are showing an eCom forec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DP2398</td>\n",
       "      <td>Demand in SS19 was about twice what was predic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B28129</td>\n",
       "      <td>Demand in SS19 should be about 7800, with grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>F36215</td>\n",
       "      <td>There were some supply chain fluctuations in S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EE8925</td>\n",
       "      <td>This article missed all of December and half o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M20325</td>\n",
       "      <td>This is also a unique article. Sales for all o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BB5478</td>\n",
       "      <td>SS19 demand should be around 8800; do you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>G26880</td>\n",
       "      <td>Similar story; SS19 demand will end up at abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BD7633</td>\n",
       "      <td>I see SS19 demand at about 2700, and thus SS20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article                                           comments\n",
       "1   DB3258  This article had a very short SS19, but sold q...\n",
       "2   G27706  The estimated full season demand in SS19 is ab...\n",
       "3   B28128  The eCom prediction this season was spot on; i...\n",
       "4   S75104  Demand in SS19 was about twice what was predic...\n",
       "5   EE8836  This article missed the first six weeks of the...\n",
       "6   G27637  In a familiar pattern, this article was not in...\n",
       "7   B37616  This article sold well, then abruptly stopped ...\n",
       "8   G27639  This article debuted in March, but in a full s...\n",
       "9   CG5675  Not sure why, but we are showing an eCom forec...\n",
       "10  DP2398  Demand in SS19 was about twice what was predic...\n",
       "11  B28129  Demand in SS19 should be about 7800, with grow...\n",
       "12  F36215  There were some supply chain fluctuations in S...\n",
       "13  EE8925  This article missed all of December and half o...\n",
       "14  M20325  This is also a unique article. Sales for all o...\n",
       "15  BB5478  SS19 demand should be around 8800; do you have...\n",
       "16  G26880  Similar story; SS19 demand will end up at abou...\n",
       "17  BD7633  I see SS19 demand at about 2700, and thus SS20..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts17th = pd.merge(forecasts, comments, left_index=True, right_on='article').set_index('article')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for_mike.to_excel('mike.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = 'DB3258'\n",
    "d = dat[dat['article_number'] == article]\n",
    "\n",
    "pivoted = d.pivot(index = 'consumer_order_date', columns = 'article_number', values = 'buy_availability')\n",
    "pivoted2 = d.pivot(index = 'consumer_order_date', columns = 'article_number', values = 'gross_demand_quantity')    \n",
    "\n",
    "forecasts[forecasts.index == article]\n",
    "\n",
    "d\n",
    "d.gross_demand_quantity.sum()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [8,2.5]\n",
    "\n",
    "pivoted.plot(linewidth = 4)\n",
    "plt.title('Buy Availability Over Time')\n",
    "plt.ylabel('Buy Availability')\n",
    "\n",
    "pivoted2.plot(linewidth = 4)\n",
    "plt.title('Weekly Gross Demand Quantity')\n",
    "plt.ylabel('Gross Demand Quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts.to_csv('forecasts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
