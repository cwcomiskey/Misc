{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T09:34:32.880803Z",
     "start_time": "2019-11-01T09:34:32.878544Z"
    }
   },
   "source": [
    "# Backtest: Forecast SS19, DAA vs. eCom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:48:54.362264Z",
     "start_time": "2019-11-07T12:48:52.179870Z"
    },
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import zipfile\n",
    "import sys, getopt\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from io import StringIO\n",
    "\n",
    "import dask.dataframe as dd\n",
    "#from chest import Chest\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# Magic function to make matplotlib inline\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 800)\n",
    "\n",
    "import scipy\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.tsatools import detrend\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:49:06.271755Z",
     "start_time": "2019-11-07T12:49:06.193952Z"
    },
    "code_folding": [],
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CHRIS and ARTEM code\n",
    "\n",
    "def initialize_parameters(par = np.array([0.5, 0.9, 0, 1, 0])):\n",
    "    # np.random.seed(3)\n",
    "    parameters = {}\n",
    "\n",
    "    parameters['alpha'] = par[0]\n",
    "    parameters['beta'] = par[1]\n",
    "    parameters['omega'] = par[2]* (1-par[1])    # one way to choose that is omega/(1-beta) = unconditional mean \n",
    "    parameters['sigma'] = par[3]\n",
    "    parameters['f0'] = par[4]                   # one way to choose is unconditional mean \n",
    "\n",
    "    return parameters\n",
    "\n",
    "def loglik(y, f, x, sigma):\n",
    "    ll = -1/2*np.log(2*np.pi ) - 1/2*np.log(sigma) - 1/(2*sigma)*(y - x*f)**2 \n",
    "    return ll\n",
    "\n",
    "\n",
    "def score_compute(y, f, x, parameters, epsilon = 1e-7 ):\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    \n",
    "    score = (y - x*f)/sigma\n",
    "    \n",
    "    return score\n",
    "\n",
    "def filterGAS(y, x, parameters):\n",
    "    \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    score0 = score_compute(y[0,:],  f0, x[0,:], parameters, epsilon = 1e-7) \n",
    "    f = np.zeros((len(y),1))\n",
    "    \n",
    "    f[0,:] = f0\n",
    "    for t in range(1,len(y)):\n",
    "        scoret = score_compute(y[t-1,:], f[t-1,:], x[t-1,:], parameters, epsilon = 1e-7) \n",
    "        f[t,:] = omega + alpha*scoret + beta*f[t-1,:] \n",
    "\n",
    "    return f\n",
    "\n",
    "def loglikest(par, y, x):\n",
    "    parameters = initialize_parameters(par)\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    # f0 = parameters[\"f0\"]\n",
    "    \n",
    "    f = filterGAS(y, x, parameters) \n",
    "    ll = np.zeros((len(y), 1))\n",
    "    m = len(y)\n",
    "\n",
    "    for t in range(0, len(y)):\n",
    "         ll[t,:] = loglik(y[t,:], f[t,:], x[t,:], sigma)\n",
    "    loglik_res = -(np.sum(ll))/m\n",
    "\n",
    "    return loglik_res\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def score_compute_2(y, f, x, parameters, epsilon = 1e-7 ):\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    \n",
    "    score = (y - x*f) \n",
    "    \n",
    "    return score\n",
    "\n",
    "def filterGAS_2(y, x, parameters):\n",
    "    \n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    omega = parameters['omega']\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    f0 = parameters[\"f0\"]\n",
    "    score0 = score_compute_2(y[0,:],  f0, x[0,:], parameters, epsilon = 1e-7) \n",
    "    f = np.zeros((len(y),1))\n",
    "    \n",
    "    f[0,:] = f0\n",
    "    for t in range(1,len(y)):\n",
    "        scoret = score_compute_2(y[t-1,:], f[t-1,:], x[t-1,:], parameters, epsilon = 1e-7) \n",
    "        f[t,:] = omega + alpha*scoret + beta*f[t-1,:] \n",
    "\n",
    "    return f\n",
    "\n",
    "def loglikest_2(par, y, x):\n",
    "    parameters = initialize_parameters(par)\n",
    "    alpha = parameters[\"alpha\"]\n",
    "    beta = parameters[\"beta\"]\n",
    "    sigma = parameters[\"sigma\"]\n",
    "    # f0 = parameters[\"f0\"]\n",
    "    \n",
    "    f = filterGAS_2(y, x, parameters) \n",
    "    ll = np.zeros((len(y), 1))\n",
    "    m = len(y)\n",
    "\n",
    "    for t in range(0, len(y)):\n",
    "         ll[t,:] = loglik(y[t,:], f[t,:], x[t,:], sigma)\n",
    "    loglik_res = -(np.sum(ll))/m\n",
    "\n",
    "    return loglik_res\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def GAS_est(df):\n",
    "    \n",
    "    y = df.net_qty.values          # observed demand (response)\n",
    "    x = df.buy_availability.values # buy_availability (explanatory)\n",
    "\n",
    "    y = y.reshape((len(y),1)) \n",
    "    x = x.reshape((len(y),1))\n",
    "    \n",
    "    ret = pd.DataFrame()\n",
    "    ret['year'] = df['year']\n",
    "    ret['week'] = df['week']\n",
    "        \n",
    "    abc = scipy.optimize.minimize(\n",
    "        loglikest,                                       # function to minimize (log likelihood y|x,theta)\n",
    "        np.array([0.8, 0.9, np.mean(y), 1, np.mean(y)]), # initial parameter values (starting)\n",
    "        args=(y, x), \n",
    "        options ={'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12},\n",
    "        method='L-BFGS-B', \n",
    "        bounds=((0,  None),             # alpha\n",
    "                (-1, 1),                # beta\n",
    "                (0.001, np.mean(y)*2),  # omega \n",
    "                (0.001, None),          # sigma\n",
    "                (0.001, np.mean(y)*2)   # f\n",
    "               )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # --- CONVERGENCE control flow ---\n",
    "    if abc.success == True:\n",
    "        \n",
    "        x1par = initialize_parameters(abc.x) \n",
    "        GAS = filterGAS(y, x, x1par)\n",
    "        \n",
    "        ret['GAS_est'] = GAS\n",
    "        ret['Convergence'] = [abc.success] * len(y)\n",
    "        ret['Convg type'] = ['One'] * len(y)\n",
    "        \n",
    "    # **Modification if first algorithm fails\n",
    "    elif abc.success == False:\n",
    "        \n",
    "        abc = scipy.optimize.minimize(\n",
    "            loglikest_2,                                       # function to minimize (log likelihood y|x,theta)\n",
    "            np.array([0.8, 0.9, np.mean(y), 1, np.mean(y)]),   # initial parameter values (starting)\n",
    "            args=(y, x), \n",
    "            options ={'eps':1e-09, 'maxiter': 600, 'ftol': 1e-12},\n",
    "            method='L-BFGS-B', \n",
    "            bounds=((0,  None),             # alpha\n",
    "                    (-1, 1),                # beta\n",
    "                    (0.001, np.mean(y)*2),  # omega \n",
    "                    (0.001, None),          # sigma\n",
    "                    (0.001, np.mean(y)*2)   # f\n",
    "                   )\n",
    "                )\n",
    "\n",
    "        x1par = initialize_parameters(abc.x) \n",
    "        GAS = filterGAS_2(y, x, x1par)\n",
    "        \n",
    "        ret['GAS_est'] = GAS\n",
    "        ret['Convergence'] = [abc.success] * len(y)\n",
    "        ret['Convg type'] = ['Two'] * len(y)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T11:59:25.179884Z",
     "start_time": "2019-11-04T11:59:25.166620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5724, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5722"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EU_seasons = pd.read_csv('data/EU_seasons.csv', low_memory = False, error_bad_lines = False, sep = \",\") # 26 Aug\n",
    "# SS18 = EU_seasons[(EU_seasons.season == 'SS18')]\n",
    "# SS19 = EU_seasons[(EU_seasons.season == 'SS19')]\n",
    "\n",
    "# CO = SS18[SS18.article_number.isin(SS19.article_number)]\n",
    "# CO.shape\n",
    "# len(CO.article_number.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:49:25.518188Z",
     "start_time": "2019-11-07T12:49:25.167948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1177"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS19 = pd.read_excel('data/ecom_SS19.xlsx').dropna()\n",
    "SS19_carryovers = SS19[SS19.carryover_FW18 == 'YES'] # carryovers only\n",
    "len(SS19_carryovers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:50:05.593784Z",
     "start_time": "2019-11-07T12:49:31.701888Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dat0 = pd.read_csv('data/ch4k_df.csv')\n",
    "ref_dat0 = pd.read_csv('data/Article reference data.csv', low_memory = False, error_bad_lines = False, \n",
    "                       usecols = ['article_no', 'model_no', 'art_desc', 'sports_cat_desc', 'rmh_cat_desc', \n",
    "                                  'franchise', 'gender_desc', 'age_group_desc', 'prod_grp_desc', 'prod_type_desc',\n",
    "                                  'brand_desc', 'bus_unit_desc', 'rmh_cat_desc'])\n",
    "\n",
    "# Remove clearance transactions!!\n",
    "dat0['clearance'] = dat0.clearance.fillna(0) \n",
    "dat0['net_qty'] = (1 - dat0.clearance)*dat0.net_qty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:08:03.642008Z",
     "start_time": "2019-11-07T13:08:01.957311Z"
    }
   },
   "outputs": [],
   "source": [
    "dat = dat0.copy()\n",
    "dat = dat[(dat.net_qty > 0) & (dat.season.isin(['SS16', 'SS17', 'SS18']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:08:46.046983Z",
     "start_time": "2019-11-07T13:08:03.644022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: Mean of empty slice\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Within article price, cost, margin averages\n",
    "dat[['price', 'cost', 'margin']] = (\n",
    "    dat.\n",
    "    groupby('article_number', group_keys=False)[['price', 'cost', 'margin']].\n",
    "    transform(lambda x: np.nanmean(x).round(2))\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buy Availability Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:10:59.964694Z",
     "start_time": "2019-11-07T13:08:46.049058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instead of GAS (quicker)\n",
    "\n",
    "dat = dat[~dat.price.isna()]\n",
    "dat = dat.sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# ---- Replace NAs and zeros (w/ no impact replacements) ----\n",
    "dat['buy_availability'] = dat.buy_availability.fillna(1) # assume full availability \n",
    "dat['buy_availability'] = np.where(dat.buy_availability == 0, 1, dat.buy_availability) # replace 0\n",
    "\n",
    "\n",
    "# ---- Smooth buy_availability ----\n",
    "def roll(df):\n",
    "    return df.rolling(window = 5, min_periods = 1, center = True).mean()\n",
    "\n",
    "dat['buy_availability'] = dat.groupby(['article_number', 'country'])['buy_availability'].apply(roll)\n",
    "\n",
    "\n",
    "# ---- Corrected net_qty ----\n",
    "dat['corr_net_qty'] = (dat.net_qty / dat.buy_availability).round()\n",
    "dat['corr_net_qty'] = dat.groupby(['article_number', 'year', 'week'])['corr_net_qty'].transform(lambda x: np.sum(x)) # over country\n",
    "\n",
    "\n",
    "# ---- Aggregate by season, across country ----\n",
    "# dat['corr_season_net_qty'] = dat.groupby(['article_number', 'season'])['corr_net_qty'].transform(lambda x: np.sum(x))\n",
    "# dat['corr_season_net_qty'] = np.where(dat.corr_season_net_qty > dat.season_net_qty, dat.corr_season_net_qty, dat.season_net_qty) # only if >\n",
    "\n",
    "dat = dat[['article_number', 'season', 'year', 'week', 'corr_net_qty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# --- GAS ----\n",
    "\n",
    "# dat_GAS = dat0.copy()[['article_number', 'year', 'week', 'country', 'season', 'net_qty', 'buy_availability']]\n",
    "# dat_GAS = dat_GAS[(dat_GAS.season == 'SS18') & (dat_GAS.article_number.isin(SS19_carryovers.article_number))].sort_values(['article_number', 'country', 'year', 'week'])\n",
    "# dat_GAS = dat_GAS.groupby(['article_number', 'country']).apply(GAS_est).reset_index()\n",
    "# dat_GAS = pd.DataFrame(dat_GAS.groupby(['article_number', 'year', 'week'])['GAS_est'].sum()).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:55:38.983140Z",
     "start_time": "2019-11-07T12:55:28.252462Z"
    },
    "code_folding": [],
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seasonality_dat = (dat0.copy()[[\n",
    "    'article_number', 'year', 'week', 'country', 'season', 'net_qty', 'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].\n",
    "                   dropna().sort_values(['article_number', 'year', 'week'])\n",
    "                  )\n",
    "\n",
    "# -- Sum over UK/EU, ADD article reference data --\n",
    "# seasonality_dat = pd.merge(\n",
    "#     pd.DataFrame(seasonality_dat.groupby(['article_number', 'season', 'year', 'week'])['net_qty'].sum()).reset_index(), # sum over UK & EU\n",
    "#     seasonality_dat[['article_number', 'sports_cat_desc', 'rmh_cat_desc', 'gender_desc', 'age_group_desc', 'franchise', 'prod_grp_desc']].drop_duplicates() # add reference information\n",
    "#     ).dropna().sort_values(['article_number', 'year', 'week'])\n",
    "\n",
    "seasonality_dat['net_qty2'] = seasonality_dat.groupby(['article_number', 'season', 'year', 'week'])['net_qty'].transform(sum)\n",
    "seasonality_dat = seasonality_dat.drop(['country', 'net_qty'], axis = 1).drop_duplicates()\n",
    "seasonality_dat = seasonality_dat.rename(columns = {'net_qty2': 'net_qty'})\n",
    "\n",
    "# -- Mirror seasons only --\n",
    "seasonality_dat = seasonality_dat[seasonality_dat.season.isin(['SS16', 'SS17', 'SS18'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:55:43.767391Z",
     "start_time": "2019-11-07T12:55:43.386821Z"
    },
    "code_folding": [],
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- Calculate cat-level weekly means across *ALL SEASONS* ---- \n",
    "\n",
    "seasonality_sport   = pd.DataFrame(seasonality_dat.groupby(['sports_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'sport_weekly_mean'})\n",
    "seasonality_rmh     = pd.DataFrame(seasonality_dat.groupby(['rmh_cat_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'rmh_weekly_mean'})\n",
    "seasonality_gndr    = pd.DataFrame(seasonality_dat.groupby(['gender_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'gender_weekly_mean'})\n",
    "seasonality_agegrp  = pd.DataFrame(seasonality_dat.groupby(['age_group_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'age_weekly_mean'})\n",
    "seasonality_frnchse = pd.DataFrame(seasonality_dat.groupby(['franchise', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'franchise_weekly_mean'})\n",
    "seasonality_prdgrp  = pd.DataFrame(seasonality_dat.groupby(['prod_grp_desc', 'week'])['net_qty'].mean()).reset_index().rename(columns = {'net_qty': 'prd_grp_weekly_mean'})\n",
    "\n",
    "seasonality_dfs = [seasonality_sport, seasonality_rmh, seasonality_gndr, seasonality_agegrp, seasonality_frnchse, seasonality_prdgrp]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:55:46.779343Z",
     "start_time": "2019-11-07T12:55:46.766988Z"
    },
    "code_folding": [],
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def regress(df):\n",
    "    # function for regressing article net_qty on seasonalities\n",
    "    # for article a's level of each category, retreive weekly means, then regress \n",
    "    \n",
    "    df = df.sort_values(['article_number', 'year', 'week'])\n",
    "        \n",
    "    # article net_demand_qty\n",
    "    y = df[['net_qty', 'week']].set_index('week')\n",
    "\n",
    "    # article category-level weekly means; set_index() for joining\n",
    "    x_sport   = seasonality_sport[seasonality_sport.sports_cat_desc == df.sports_cat_desc.unique()[0]].set_index('week')\n",
    "    x_rmh     = seasonality_rmh[seasonality_rmh.rmh_cat_desc == df.rmh_cat_desc.unique()[0]].set_index('week')\n",
    "    x_gndr    = seasonality_gndr[seasonality_gndr.gender_desc == df.gender_desc.unique()[0]].set_index('week')\n",
    "    x_agegrp  = seasonality_agegrp[seasonality_agegrp.age_group_desc == df.age_group_desc.unique()[0]].set_index('week')\n",
    "    x_frnchse = seasonality_frnchse[seasonality_frnchse.franchise == df.franchise.unique()[0]].set_index('week')\n",
    "    x_prdgrp  = seasonality_prdgrp[seasonality_prdgrp.prod_grp_desc == df.prod_grp_desc.unique()[0]].set_index('week')\n",
    "\n",
    "    # design matrix (ensure 'week' alignment)\n",
    "    yX = (pd.merge(y, x_sport, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_rmh, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_gndr, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_agegrp, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_frnchse, left_index=True, right_index=True, how = 'outer').\n",
    "          merge(x_prdgrp, left_index=True, right_index=True, how = 'outer').\n",
    "          drop(['sports_cat_desc', 'rmh_cat_desc', 'gender_desc',\n",
    "               'age_group_desc', 'franchise', 'prod_grp_desc'], axis = 1))\n",
    "\n",
    "    # predict article 'a' net_demand_qty with 5 article 'a' category-level seasonalities\n",
    "\n",
    "    y = yX.net_qty\n",
    "    \n",
    "    X = yX.drop('net_qty', axis = 1)\n",
    "    X = sm.add_constant(X) # ****** ******* *******\n",
    "    \n",
    "    mod = sm.OLS(y, X, missing='drop').fit()\n",
    "    # print(df.article_number.unique(), round(mod.rsquared, 2))\n",
    "\n",
    "    ret = pd.DataFrame(index = X.index)\n",
    "    ret['seas_preds'] = mod.predict(X).round()\n",
    "    \n",
    "    ret = ret.reset_index()\n",
    "    ret = pd.DataFrame(ret.groupby('week')['seas_preds'].mean())\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:55:53.253234Z",
     "start_time": "2019-11-07T12:55:53.177563Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just SS18-SS19 carryovers\n",
    "seasonality_dat = seasonality_dat[(seasonality_dat.article_number.isin(SS19_carryovers.article_number))].sort_values(['article_number', 'year', 'week']) \n",
    "\n",
    "# Some confusion as to which articles are SS18 to SS19 carryovers; using EU_seasons and ecom_SS19.xlsx gives different\n",
    "# Using ecom_SS19.xlsx, from Demand Planning SS Mike pointed me to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:13:43.735708Z",
     "start_time": "2019-11-07T13:13:33.957817Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "preds = seasonality_dat.groupby(['article_number']).apply(regress).reset_index() # regress articles of interest on seasonality\n",
    "preds['seas_preds'] = np.where(preds.seas_preds > 0, preds.seas_preds, 0) # Zero out negative preds\n",
    "\n",
    "\n",
    "# Merge back with reference data\n",
    "preds = (pd.merge(preds, seasonality_dat[seasonality_dat.season == 'SS18'], how = 'left', left_on = ['article_number', 'week'], right_on=['article_number', 'week']).\n",
    "         sort_values(['article_number', 'year', 'week']))[['article_number', 'year', 'week', 'net_qty', 'seas_preds']]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:13:45.359216Z",
     "start_time": "2019-11-07T13:13:45.169381Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combined observed weeks (partial season)  --- AND --- regression predicted (all) weeks\n",
    "preds = pd.merge(\n",
    "    preds,   # all weeks\n",
    "    dat,     # observed weeks\n",
    "    how = 'left')\n",
    "\n",
    "# corr_net_qty NAs where buy_availability == NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Aggregate to Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:13:50.324294Z",
     "start_time": "2019-11-07T13:13:50.320232Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# weekly assignment of GAS, seasonality, or combination\n",
    "preds['y_hat'] = np.where(np.isnan(preds.corr_net_qty), preds.seas_preds, (preds.corr_net_qty + preds.seas_preds)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:13:58.142645Z",
     "start_time": "2019-11-07T13:13:58.041658Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "preds['corrected'] = (preds.corr_net_qty + preds.seas_preds)/2\n",
    "\n",
    "# Sum over season\n",
    "preds_season = pd.DataFrame(preds.groupby('article_number')['y_hat'].apply(sum).round())\n",
    "\n",
    "preds_season['y_hat'] = preds_season.y_hat * 1.1 # default growth rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:14:15.590680Z",
     "start_time": "2019-11-07T13:14:15.579660Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(608, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine DAA + eCom\n",
    "\n",
    "preds_season.shape\n",
    "preds_season = pd.merge(\n",
    "    preds_season.reset_index(),             # DAA forecasts\n",
    "    SS19_carryovers[['article_number', 'Ecom_FC_RMA']], # eCom forecast\n",
    "    how = 'left', left_on = 'article_number', right_on = 'article_number'\n",
    ")\n",
    "preds_season2.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T12:57:41.084348Z",
     "start_time": "2019-09-11T12:57:40.941795Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Overbuy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:00:41.005749Z",
     "start_time": "2019-11-07T13:00:40.998460Z"
    },
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from scipy import optimize\n",
    "from scipy import integrate\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Loss --- demand, buy, margin, cost\n",
    "def L(d, b, margin, cost):\n",
    "    if d > b:\n",
    "        return (d - b)*margin\n",
    "    elif d < b:\n",
    "        return (b - d)*cost\n",
    "    elif d == b:\n",
    "        return 0\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "# E[L | buy, article_mean, article_sd, article_margin, article_cost]\n",
    "def EL(mu, sigma, margin, cost, b):\n",
    "    I = lambda x: L(x, b, margin, cost) * stats.norm.pdf(x, mu, sigma) # I for integrand\n",
    "    Exp_loss = integrate.quad(I, 0, mu + 3*sigma)/(1 - stats.norm.cdf(0, loc = mu, scale = sigma)) # Expected value of Loss function\n",
    "    return round(Exp_loss[0], 2) \n",
    "\n",
    "def minimize_EL(mu, sigma, margin, cost):\n",
    "    if(mu < 1000):\n",
    "        return 1.2*mu \n",
    "    p = partial(EL, mu, sigma, margin, cost) # Make EL function of only one var: b_0\n",
    "    buy_opt = optimize.minimize_scalar(p, bounds = (mu, mu + 2*sigma))\n",
    "    return int(buy_opt['x']) # optimal buy quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:14:34.966889Z",
     "start_time": "2019-11-07T13:14:32.858237Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load cost/margin data\n",
    "cost_margin = dat0.copy()\n",
    "\n",
    "cost_margin = pd.DataFrame(\n",
    "    cost_margin[cost_margin.season.isin(['SS18', 'FW18', 'SS19', 'FW19'])].\n",
    "                groupby('article_number')[['price', 'cost', 'margin']].\n",
    "                mean()\n",
    ")\n",
    "\n",
    "# Add cost and margin for optimal overbuy estimation                                                                           \n",
    "preds_season = pd.merge(preds_season, cost_margin, how = 'left', left_on = 'article_number', right_index=True).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-07T13:01:36.998Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# see evaluation.ipynb for sd estimation \n",
    "\n",
    "# ---- CURRENT ----\n",
    "# opt_ovb_all = pd.DataFrame(preds_season.\n",
    "#     apply(lambda row: minimize_EL(row['y_hat'], 550 + 0.2*row['y_hat'], row['margin'], row['cost']), axis=1)\n",
    "#                           )\n",
    "\n",
    "# opt_ovb_all = opt_ovb_all.rename(columns = {opt_ovb_all.columns[0]: 'Opt_Ovb'})\n",
    "\n",
    "# preds_season = pd.merge(\n",
    "#     preds_season, # everything\n",
    "#     opt_ovb_all,  # optimal overbuy\n",
    "#     right_index= True, left_index= True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:05:35.709734Z",
     "start_time": "2019-11-07T13:01:37.000339Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# ------- Try -------\n",
    "Opt_Ovb = preds_season.apply(lambda row: minimize_EL(row['y_hat'], 550 + 0.2*row['y_hat'], row['margin'], row['cost']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:19:22.134155Z",
     "start_time": "2019-11-07T13:19:22.130206Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_season = preds_season.set_index('article_number')\n",
    "preds_season['Opt_Ovb'] = Opt_Ovb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:12:43.021483Z",
     "start_time": "2019-11-07T13:12:43.017971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Profit\n",
    "def P(d, margin, cost, b):\n",
    "    if d > b:    # CANNOT satisfy demand\n",
    "        return b*margin\n",
    "    \n",
    "    elif d <= b: # CAN satisfy demand\n",
    "        return d*margin - (b - d)*cost\n",
    "    \n",
    "    else:\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:27:00.512221Z",
     "start_time": "2019-11-07T13:26:57.572664Z"
    }
   },
   "outputs": [],
   "source": [
    "# SS19 only, corrected\n",
    "\n",
    "# Instead of GAS (quicker)\n",
    "dat19 = dat0.copy()\n",
    "dat19 = dat19[(dat19.net_qty > 0) & (dat19.season == 'SS19') & (dat19.article_number.isin(SS19_carryovers.article_number))]\n",
    "\n",
    "dat19 = dat19[~dat19.price.isna()]\n",
    "dat19 = dat19.sort_values(['article_number', 'country', 'year', 'week'])\n",
    "\n",
    "# ---- Replace NAs and zeros (w/ no impact replacements) ----\n",
    "dat19['buy_availability'] = dat19.buy_availability.fillna(1) # assume full availability \n",
    "dat19['buy_availability'] = np.where(dat19.buy_availability == 0, 1, dat19.buy_availability) # replace 0\n",
    "\n",
    "\n",
    "# ---- Smooth buy_availability ----\n",
    "def roll(df):\n",
    "    return df.rolling(window = 5, min_periods = 1, center = True).mean()\n",
    "\n",
    "dat19['buy_availability'] = dat19.groupby(['article_number', 'country'])['buy_availability'].apply(roll)\n",
    "\n",
    "# ---- Corrected net_qty ----\n",
    "dat19['corr_net_qty'] = (dat19.net_qty / dat19.buy_availability).round()\n",
    "\n",
    "dat19['corr_net_qty'] = dat19.groupby(['article_number', 'year', 'week'])['corr_net_qty'].transform(lambda x: np.sum(x)) # over country\n",
    "\n",
    "dat19 = dat19[['article_number', 'season', 'year', 'week', 'corr_net_qty']]\n",
    "\n",
    "\n",
    "dat19['corr_season_net_qty'] = dat19.groupby(['article_number', 'season'])['corr_net_qty'].transform(lambda x: np.sum(x))\n",
    "\n",
    "dat19 = dat19[['article_number', 'season', 'corr_season_net_qty']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T13:32:31.494938Z",
     "start_time": "2019-11-07T13:32:31.481190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>Ecom_FC_RMA</th>\n",
       "      <th>Opt_Ovb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>011040</th>\n",
       "      <td>207.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>248.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>015110</th>\n",
       "      <td>1107.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>019000</th>\n",
       "      <td>2878.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3663.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>019228</th>\n",
       "      <td>891.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1069.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>019310</th>\n",
       "      <td>319.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>382.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 y_hat  Ecom_FC_RMA  Opt_Ovb\n",
       "article_number                              \n",
       "011040           207.0        600.0    248.4\n",
       "015110          1107.0        800.0   1723.0\n",
       "019000          2878.0       2000.0   3663.0\n",
       "019228           891.0        500.0   1069.2\n",
       "019310           319.0        500.0    382.8"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_number</th>\n",
       "      <th>season</th>\n",
       "      <th>corr_season_net_qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2843310</th>\n",
       "      <td>011040</td>\n",
       "      <td>SS19</td>\n",
       "      <td>209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773754</th>\n",
       "      <td>015110</td>\n",
       "      <td>SS19</td>\n",
       "      <td>1590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21785</th>\n",
       "      <td>019000</td>\n",
       "      <td>SS19</td>\n",
       "      <td>2507.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411561</th>\n",
       "      <td>019228</td>\n",
       "      <td>SS19</td>\n",
       "      <td>1027.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532499</th>\n",
       "      <td>019310</td>\n",
       "      <td>SS19</td>\n",
       "      <td>486.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_number season  corr_season_net_qty\n",
       "2843310         011040   SS19                209.0\n",
       "3773754         015110   SS19               1590.0\n",
       "21785           019000   SS19               2507.0\n",
       "1411561         019228   SS19               1027.0\n",
       "2532499         019310   SS19                486.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_season[['y_hat', 'Ecom_FC_RMA', 'Opt_Ovb']].head()\n",
    "dat19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some merging...\n",
    "# ...calculate profits -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit example\n",
    "eCom_profit = dat.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['Ecom_FC_RMA']), axis=1)\n",
    "DAA_profit = dat.apply(lambda row: P(row['corr_season_net_qty'], row['margin'], row['cost'], row['Opt_Ovb']), axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
